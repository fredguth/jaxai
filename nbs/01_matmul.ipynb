{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are allowed to use:\n",
    "\n",
    "- Python\n",
    "- matplotlib\n",
    "- The Python standard library\n",
    "- JAX standard library\n",
    "- Jupyter notebooks and nbdev\n",
    "\n",
    "We can also use `miniai` and`torch` for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt, lovely_tensors as lt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lovely_tensors as lt\n",
    "import torch\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea ðŸ’¡: someone should port `lovely_tensors` to `jax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an utility function to show memory allocation in the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "def get_memory_free_MiB(gpu_index):\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(int(gpu_index))\n",
    "    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    return mem_info.free // 1024 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11176"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_free_MiB(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\n",
    "path_data = Path('data')\n",
    "path_data.mkdir(exist_ok=True)\n",
    "path_gz = path_data/'mnist.pkl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "if not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((50000, 784), (50000,)), ((10000, 784), (10000,)), numpy.ndarray)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train.shape, y_train.shape), (x_valid.shape, y_valid.shape), type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11176"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_free_MiB(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random array initialization \n",
    "\n",
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.3 Âµs Â± 39.9 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "tw = torch.randn(784,10)\n",
    "%timeit -n 10 tw = torch.randn(784,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11176"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_free_MiB(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we don't explicitly move to cuda, the tensor variable is in cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that JAX allocates everything it can at once. [`JAX will preallocate 90% of the total GPU memory when the first JAX operation is run.`](https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html)\n",
    "Let's change the default to 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFAIK this is the way to check if jax is running in GPU, TPU or CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp \n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = jax.devices(\"cpu\")\n",
    "gpus = jax.devices(\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11036"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_free_MiB(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5412"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "jw = jax.random.normal(key, shape=(784,10), dtype=jax.numpy.float32)\n",
    "get_memory_free_MiB(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534 Âµs Â± 93.2 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 jw = jax.random.normal(key, shape=(784,10), dtype=jax.numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 10),\n",
       " StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0),\n",
       " (Array(-3.7302895, dtype=float32), Array(3.9276645, dtype=float32)),\n",
       " (Array(0.01403106, dtype=float32), Array(0.9955147, dtype=float32)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jw.shape, jw.device(), (jw.min(), jw.max()), (jw.mean(), jw.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.334519572953737"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "534/84.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTorch` random initialization is 6.33 times faster than `JAX`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works in pytorch, jax and numpy\n",
    "def matmul(x,y):\n",
    "    return x@y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch + Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Tensor)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = torch.randn(784,10)\n",
    "tx_train = torch.tensor(x_train)\n",
    "tx_train.shape, type(tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 s, sys: 768 ms, total: 2.31 s\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%time txc, twc = tx_train.cuda(), tw.cuda() # measure JAX device transfer time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([784, 10]),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txc.shape, twc.shape, txc.device, twc.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565 Âµs Â± 61 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 matmul(txc, twc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.57 ms, sys: 106 ms, total: 113 ms\n",
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "%time jx_train= jax.numpy.array(x_train) # measure JAX device transfer time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784),\n",
       " jaxlib.xla_extension.Array,\n",
       " StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0),\n",
       " (784, 10),\n",
       " jaxlib.xla_extension.Array,\n",
       " StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jx_train.shape, type(jx_train), jx_train.device(), jw.shape, type(jw), jw.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 571 ms, sys: 502 ms, total: 1.07 s\n",
      "Wall time: 2.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[ -0.97251654,  -6.734766  ,  -1.7148048 , ...,  -7.3052707 ,\n",
       "          4.7699347 ,  -9.978209  ],\n",
       "       [ -3.7587132 , -15.646385  ,   4.6581864 , ..., -12.779354  ,\n",
       "          8.308025  ,   1.2251611 ],\n",
       "       [ -8.072983  ,  -1.9900167 ,   4.9306164 , ...,  -1.5675635 ,\n",
       "          9.269543  ,   5.2164707 ],\n",
       "       ...,\n",
       "       [  5.3260746 ,   2.7311437 ,  12.856297  , ...,   5.9363794 ,\n",
       "         -5.454983  , -11.519852  ],\n",
       "       [  9.113716  ,  -7.591759  ,   4.178947  , ...,  10.593879  ,\n",
       "         -3.862001  ,  11.375757  ],\n",
       "       [ 11.984879  ,  -3.51389   ,  15.2327    , ...,  16.66311   ,\n",
       "         -2.9007533 ,  -5.151217  ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_jit = jax.jit(matmul)\n",
    "%time matmul_jit(jx_train, jw).block_until_ready() # measure JAX compilation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905 Âµs Â± 107 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 matmul_jit(jx_train, jw).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6017699115044248"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "905/565"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test, PyTorch was 1.6 times faster than JAX for matrix multiplication."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for self: Still trying to figure out how I achieved 44 microseconds for PyTorch in previous run.  Probably an error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b806adfb64333d0ca5c14ed2dbf613d5d551ec856d702e8a01588c05fb48e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
