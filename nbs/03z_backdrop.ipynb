{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_MEM_FRACTION=0.8\n"
     ]
    }
   ],
   "source": [
    "# Sets how much GPU memory JAX preallocate\n",
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import lovely_jax as lj\n",
    "lj.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11176"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pynvml\n",
    "def get_memory_free_MiB(gpu_index):\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(int(gpu_index))\n",
    "    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    return mem_info.free // 1024 ** 2\n",
    "\n",
    "get_memory_free_MiB(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, value_and_grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(jnp.array, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on JAX documentation\n",
    "https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Array[50, 784] n=39200 x∈[-0.043, 0.042] μ=-6.460e-05 σ=0.010 gpu:0,\n",
       "  Array[50] x∈[-0.024, 0.030] μ=-0.000 σ=0.011 gpu:0),\n",
       " (Array[50, 50] n=2500 x∈[-0.034, 0.033] μ=-0.000 σ=0.010 gpu:0,\n",
       "  Array[50] x∈[-0.020, 0.022] μ=0.001 σ=0.010 gpu:0),\n",
       " (Array[1, 50] x∈[-0.028, 0.026] μ=-0.002 σ=0.010 gpu:0,\n",
       "  Array[1] gpu:0 [-0.005])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "  w_key, b_key = random.split(key)\n",
    "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 50, 50, 1]\n",
    "step_size = 0.01\n",
    "num_epochs = 8\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, input):\n",
    "  # per-example prediction\n",
    "  z = input\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = w@z + b\n",
    "    z = relu(outputs)\n",
    "  \n",
    "  final_w, final_b = params[-1]\n",
    "  logits = final_w@z + final_b\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[1] gpu:0 [-0.006]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(params, x_train[0])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[50000, 1] x∈[-0.007, -0.005] μ=-0.006 σ=0.000 gpu:0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "preds = batched_predict(params, x_train)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array gpu:0 28.202"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(o,t): return ((o[:,0]-t) ** 2).mean()\n",
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array gpu:0 28.202"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(params, inputs, targets):\n",
    "    return mse(batched_predict(params, inputs), targets)\n",
    "\n",
    "loss(params, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[False,  True, False, False, False, False, False, False, False,\n",
       "         False],\n",
       "        [False, False,  True, False, False, False, False, False, False,\n",
       "         False]]),\n",
       " (2, 10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.array([1,2])\n",
    "y = np.array(x[:,None]==jnp.arange(10))\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size=0.01\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "  grads = grad(loss)(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Array[50, 784] n=39200 x∈[-0.043, 0.042] μ=-6.520e-05 σ=0.010 gpu:0,\n",
       "  Array[50] x∈[-0.024, 0.030] μ=-0.000 σ=0.011 gpu:0),\n",
       " (Array[50, 50] n=2500 x∈[-0.034, 0.033] μ=-0.000 σ=0.010 gpu:0,\n",
       "  Array[50] x∈[-0.020, 0.021] μ=0.001 σ=0.010 gpu:0),\n",
       " (Array[1, 50] x∈[-0.027, 0.026] μ=-0.002 σ=0.010 gpu:0,\n",
       "  Array[1] gpu:0 [0.085])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_params = update(params, x_train, y_train)\n",
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array gpu:0 28.202, Array gpu:0 27.411)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1=loss(params, x_train, y_train)\n",
    "new_params = update(params, x_train, y_train)\n",
    "e1, loss(new_params, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[50000, 1] x∈[-0.007, -0.005] μ=-0.006 σ=0.000 gpu:0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot(x, k, dtype=jnp.float32):\n",
    "#   \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "#   return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "# one_hot(preds, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array[50000] i32 x∈[0, 9] μ=4.449 σ=2.891 gpu:0, Array i32 gpu:0 0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, jnp.int32(.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array[50000, 1] i32 \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0,\n",
       " (50000,),\n",
       " Array[50000, 1] i32 x∈[0, 9] μ=4.449 σ=2.891 gpu:0,\n",
       " Array gpu:0 0.099)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.int32(preds), y_train.shape,y_train[:,None],jnp.mean(jnp.int32(preds) == y_train[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, inputs, targets):\n",
    "  return jnp.mean(jnp.int32(batched_predict(params, inputs)) == targets[:,None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array gpu:0 0.099"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(params, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit\n",
    "def update(params, x, y):  \n",
    "  error, grads = value_and_grad(loss)(params, x, y)\n",
    "  return error, [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_epochs = 10\n",
    "def train(params, num_epochs=10):\n",
    "  training_start = time.time()  \n",
    "  for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    error, new_params = update(params, x_train, y_train)\n",
    "    params = new_params\n",
    "    train_acc = accuracy(params, x_train, y_train)\n",
    "    test_acc = accuracy(params, x_valid, y_valid)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    train_time = time.time() - training_start\n",
    "    \n",
    "    print(f'{epoch}/{num_epochs}, {epoch_time:0.2f} sec, {train_time:0.2f} sec')\n",
    "    print(\"Training set loss {}\".format(error))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))\n",
    "    print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1000, 0.01 sec, 0.01 sec\n",
      "Training set loss 28.202253341674805\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "1/1000, 0.01 sec, 0.03 sec\n",
      "Training set loss 27.411394119262695\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "2/1000, 0.01 sec, 0.04 sec\n",
      "Training set loss 26.65247917175293\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "3/1000, 0.01 sec, 0.05 sec\n",
      "Training set loss 25.92398452758789\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "4/1000, 0.01 sec, 0.06 sec\n",
      "Training set loss 25.224512100219727\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "5/1000, 0.01 sec, 0.07 sec\n",
      "Training set loss 24.552783966064453\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "6/1000, 0.01 sec, 0.09 sec\n",
      "Training set loss 23.90764617919922\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "7/1000, 0.01 sec, 0.10 sec\n",
      "Training set loss 23.287960052490234\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "8/1000, 0.01 sec, 0.11 sec\n",
      "Training set loss 22.69258689880371\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "9/1000, 0.02 sec, 0.13 sec\n",
      "Training set loss 22.120441436767578\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "10/1000, 0.01 sec, 0.14 sec\n",
      "Training set loss 21.57050895690918\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "11/1000, 0.01 sec, 0.15 sec\n",
      "Training set loss 21.04183006286621\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "-------------------------------\n",
      "12/1000, 0.01 sec, 0.16 sec\n",
      "Training set loss 20.533485412597656\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "13/1000, 0.01 sec, 0.17 sec\n",
      "Training set loss 20.044584274291992\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "14/1000, 0.01 sec, 0.19 sec\n",
      "Training set loss 19.574283599853516\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "15/1000, 0.01 sec, 0.20 sec\n",
      "Training set loss 19.121761322021484\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "16/1000, 0.01 sec, 0.21 sec\n",
      "Training set loss 18.686222076416016\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "17/1000, 0.01 sec, 0.22 sec\n",
      "Training set loss 18.266897201538086\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "18/1000, 0.01 sec, 0.24 sec\n",
      "Training set loss 17.863037109375\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "19/1000, 0.02 sec, 0.26 sec\n",
      "Training set loss 17.47391128540039\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "20/1000, 0.01 sec, 0.27 sec\n",
      "Training set loss 17.098804473876953\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "21/1000, 0.02 sec, 0.29 sec\n",
      "Training set loss 16.73701286315918\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "22/1000, 0.01 sec, 0.30 sec\n",
      "Training set loss 16.387840270996094\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "23/1000, 0.01 sec, 0.32 sec\n",
      "Training set loss 16.05059242248535\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "24/1000, 0.01 sec, 0.33 sec\n",
      "Training set loss 15.724573135375977\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "25/1000, 0.01 sec, 0.34 sec\n",
      "Training set loss 15.409074783325195\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "26/1000, 0.01 sec, 0.35 sec\n",
      "Training set loss 15.103374481201172\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "27/1000, 0.01 sec, 0.37 sec\n",
      "Training set loss 14.806711196899414\n",
      "Training set accuracy 0.11355999857187271\n",
      "Test set accuracy 0.10639999806880951\n",
      "-------------------------------\n",
      "28/1000, 0.01 sec, 0.38 sec\n",
      "Training set loss 14.51828670501709\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "29/1000, 0.01 sec, 0.39 sec\n",
      "Training set loss 14.237241744995117\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "30/1000, 0.01 sec, 0.40 sec\n",
      "Training set loss 13.96263313293457\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "31/1000, 0.01 sec, 0.41 sec\n",
      "Training set loss 13.693411827087402\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "32/1000, 0.01 sec, 0.42 sec\n",
      "Training set loss 13.428386688232422\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "33/1000, 0.01 sec, 0.43 sec\n",
      "Training set loss 13.166186332702637\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "34/1000, 0.01 sec, 0.45 sec\n",
      "Training set loss 12.905200004577637\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "35/1000, 0.01 sec, 0.46 sec\n",
      "Training set loss 12.643535614013672\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "36/1000, 0.01 sec, 0.47 sec\n",
      "Training set loss 12.378936767578125\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "37/1000, 0.01 sec, 0.49 sec\n",
      "Training set loss 12.108735084533691\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "38/1000, 0.01 sec, 0.50 sec\n",
      "Training set loss 11.829791069030762\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "39/1000, 0.01 sec, 0.51 sec\n",
      "Training set loss 11.538501739501953\n",
      "Training set accuracy 0.09935999661684036\n",
      "Test set accuracy 0.0989999994635582\n",
      "-------------------------------\n",
      "40/1000, 0.01 sec, 0.52 sec\n",
      "Training set loss 11.230927467346191\n",
      "Training set accuracy 0.09953999519348145\n",
      "Test set accuracy 0.09879999607801437\n",
      "-------------------------------\n",
      "41/1000, 0.01 sec, 0.53 sec\n",
      "Training set loss 10.903127670288086\n",
      "Training set accuracy 0.10201999545097351\n",
      "Test set accuracy 0.09929999709129333\n",
      "-------------------------------\n",
      "42/1000, 0.01 sec, 0.55 sec\n",
      "Training set loss 10.551904678344727\n",
      "Training set accuracy 0.10081999748945236\n",
      "Test set accuracy 0.09929999709129333\n",
      "-------------------------------\n",
      "43/1000, 0.01 sec, 0.56 sec\n",
      "Training set loss 10.17612361907959\n",
      "Training set accuracy 0.10211999714374542\n",
      "Test set accuracy 0.10260000079870224\n",
      "-------------------------------\n",
      "44/1000, 0.01 sec, 0.57 sec\n",
      "Training set loss 9.77873420715332\n",
      "Training set accuracy 0.10204000025987625\n",
      "Test set accuracy 0.1030999943614006\n",
      "-------------------------------\n",
      "45/1000, 0.01 sec, 0.58 sec\n",
      "Training set loss 9.3692045211792\n",
      "Training set accuracy 0.10142000019550323\n",
      "Test set accuracy 0.1022999957203865\n",
      "-------------------------------\n",
      "46/1000, 0.01 sec, 0.60 sec\n",
      "Training set loss 8.96515941619873\n",
      "Training set accuracy 0.09839999675750732\n",
      "Test set accuracy 0.10080000013113022\n",
      "-------------------------------\n",
      "47/1000, 0.01 sec, 0.61 sec\n",
      "Training set loss 8.590899467468262\n",
      "Training set accuracy 0.1027199998497963\n",
      "Test set accuracy 0.10209999978542328\n",
      "-------------------------------\n",
      "48/1000, 0.01 sec, 0.62 sec\n",
      "Training set loss 8.270634651184082\n",
      "Training set accuracy 0.10737999528646469\n",
      "Test set accuracy 0.1078999936580658\n",
      "-------------------------------\n",
      "49/1000, 0.01 sec, 0.63 sec\n",
      "Training set loss 8.018061637878418\n",
      "Training set accuracy 0.10831999778747559\n",
      "Test set accuracy 0.11149999499320984\n",
      "-------------------------------\n",
      "50/1000, 0.01 sec, 0.65 sec\n",
      "Training set loss 7.829571723937988\n",
      "Training set accuracy 0.1066799983382225\n",
      "Test set accuracy 0.11009999364614487\n",
      "-------------------------------\n",
      "51/1000, 0.01 sec, 0.66 sec\n",
      "Training set loss 7.687808513641357\n",
      "Training set accuracy 0.1029599979519844\n",
      "Test set accuracy 0.10579999536275864\n",
      "-------------------------------\n",
      "52/1000, 0.01 sec, 0.67 sec\n",
      "Training set loss 7.572429656982422\n",
      "Training set accuracy 0.09829999506473541\n",
      "Test set accuracy 0.1021999940276146\n",
      "-------------------------------\n",
      "53/1000, 0.01 sec, 0.69 sec\n",
      "Training set loss 7.468307971954346\n",
      "Training set accuracy 0.09467999637126923\n",
      "Test set accuracy 0.0982000008225441\n",
      "-------------------------------\n",
      "54/1000, 0.01 sec, 0.70 sec\n",
      "Training set loss 7.36702299118042\n",
      "Training set accuracy 0.09126000106334686\n",
      "Test set accuracy 0.09609999507665634\n",
      "-------------------------------\n",
      "55/1000, 0.01 sec, 0.71 sec\n",
      "Training set loss 7.26467752456665\n",
      "Training set accuracy 0.08884000033140182\n",
      "Test set accuracy 0.09389999508857727\n",
      "-------------------------------\n",
      "56/1000, 0.01 sec, 0.72 sec\n",
      "Training set loss 7.15960168838501\n",
      "Training set accuracy 0.08727999776601791\n",
      "Test set accuracy 0.09019999951124191\n",
      "-------------------------------\n",
      "57/1000, 0.01 sec, 0.73 sec\n",
      "Training set loss 7.051047325134277\n",
      "Training set accuracy 0.08535999804735184\n",
      "Test set accuracy 0.0885000005364418\n",
      "-------------------------------\n",
      "58/1000, 0.01 sec, 0.75 sec\n",
      "Training set loss 6.9386396408081055\n",
      "Training set accuracy 0.08376000076532364\n",
      "Test set accuracy 0.08619999885559082\n",
      "-------------------------------\n",
      "59/1000, 0.01 sec, 0.76 sec\n",
      "Training set loss 6.822169780731201\n",
      "Training set accuracy 0.08286000043153763\n",
      "Test set accuracy 0.08449999988079071\n",
      "-------------------------------\n",
      "60/1000, 0.01 sec, 0.77 sec\n",
      "Training set loss 6.701658725738525\n",
      "Training set accuracy 0.08178000152111053\n",
      "Test set accuracy 0.08609999716281891\n",
      "-------------------------------\n",
      "61/1000, 0.01 sec, 0.78 sec\n",
      "Training set loss 6.577334403991699\n",
      "Training set accuracy 0.08178000152111053\n",
      "Test set accuracy 0.08509999513626099\n",
      "-------------------------------\n",
      "62/1000, 0.01 sec, 0.80 sec\n",
      "Training set loss 6.449730396270752\n",
      "Training set accuracy 0.08123999834060669\n",
      "Test set accuracy 0.08489999920129776\n",
      "-------------------------------\n",
      "63/1000, 0.01 sec, 0.81 sec\n",
      "Training set loss 6.3198723793029785\n",
      "Training set accuracy 0.08181999623775482\n",
      "Test set accuracy 0.08489999920129776\n",
      "-------------------------------\n",
      "64/1000, 0.01 sec, 0.82 sec\n",
      "Training set loss 6.189274787902832\n",
      "Training set accuracy 0.0818599984049797\n",
      "Test set accuracy 0.08469999581575394\n",
      "-------------------------------\n",
      "65/1000, 0.01 sec, 0.84 sec\n",
      "Training set loss 6.059952259063721\n",
      "Training set accuracy 0.0825599953532219\n",
      "Test set accuracy 0.08419999480247498\n",
      "-------------------------------\n",
      "66/1000, 0.01 sec, 0.85 sec\n",
      "Training set loss 5.934215068817139\n",
      "Training set accuracy 0.0828000009059906\n",
      "Test set accuracy 0.08509999513626099\n",
      "-------------------------------\n",
      "67/1000, 0.01 sec, 0.86 sec\n",
      "Training set loss 5.814455986022949\n",
      "Training set accuracy 0.08377999812364578\n",
      "Test set accuracy 0.08399999886751175\n",
      "-------------------------------\n",
      "68/1000, 0.01 sec, 0.88 sec\n",
      "Training set loss 5.702683448791504\n",
      "Training set accuracy 0.08479999750852585\n",
      "Test set accuracy 0.08379999548196793\n",
      "-------------------------------\n",
      "69/1000, 0.01 sec, 0.89 sec\n",
      "Training set loss 5.599806785583496\n",
      "Training set accuracy 0.08618000149726868\n",
      "Test set accuracy 0.08619999885559082\n",
      "-------------------------------\n",
      "70/1000, 0.01 sec, 0.90 sec\n",
      "Training set loss 5.506099224090576\n",
      "Training set accuracy 0.08793999999761581\n",
      "Test set accuracy 0.08689999580383301\n",
      "-------------------------------\n",
      "71/1000, 0.01 sec, 0.92 sec\n",
      "Training set loss 5.421186923980713\n",
      "Training set accuracy 0.09045999497175217\n",
      "Test set accuracy 0.08869999647140503\n",
      "-------------------------------\n",
      "72/1000, 0.01 sec, 0.93 sec\n",
      "Training set loss 5.344588756561279\n",
      "Training set accuracy 0.09291999787092209\n",
      "Test set accuracy 0.09109999984502792\n",
      "-------------------------------\n",
      "73/1000, 0.01 sec, 0.94 sec\n",
      "Training set loss 5.275472164154053\n",
      "Training set accuracy 0.09657999873161316\n",
      "Test set accuracy 0.09349999576807022\n",
      "-------------------------------\n",
      "74/1000, 0.01 sec, 0.95 sec\n",
      "Training set loss 5.213184356689453\n",
      "Training set accuracy 0.0997999981045723\n",
      "Test set accuracy 0.09769999980926514\n",
      "-------------------------------\n",
      "75/1000, 0.01 sec, 0.97 sec\n",
      "Training set loss 5.156991958618164\n",
      "Training set accuracy 0.10273999720811844\n",
      "Test set accuracy 0.09959999471902847\n",
      "-------------------------------\n",
      "76/1000, 0.01 sec, 0.98 sec\n",
      "Training set loss 5.106165885925293\n",
      "Training set accuracy 0.10599999874830246\n",
      "Test set accuracy 0.10189999639987946\n",
      "-------------------------------\n",
      "77/1000, 0.01 sec, 0.99 sec\n",
      "Training set loss 5.060043811798096\n",
      "Training set accuracy 0.10947999358177185\n",
      "Test set accuracy 0.10520000010728836\n",
      "-------------------------------\n",
      "78/1000, 0.01 sec, 1.00 sec\n",
      "Training set loss 5.017925262451172\n",
      "Training set accuracy 0.11187999695539474\n",
      "Test set accuracy 0.10849999636411667\n",
      "-------------------------------\n",
      "79/1000, 0.02 sec, 1.02 sec\n",
      "Training set loss 4.979216575622559\n",
      "Training set accuracy 0.11413999646902084\n",
      "Test set accuracy 0.11229999363422394\n",
      "-------------------------------\n",
      "80/1000, 0.01 sec, 1.03 sec\n",
      "Training set loss 4.943424224853516\n",
      "Training set accuracy 0.11603999882936478\n",
      "Test set accuracy 0.11429999768733978\n",
      "-------------------------------\n",
      "81/1000, 0.01 sec, 1.05 sec\n",
      "Training set loss 4.910118103027344\n",
      "Training set accuracy 0.11787999421358109\n",
      "Test set accuracy 0.11539999395608902\n",
      "-------------------------------\n",
      "82/1000, 0.01 sec, 1.06 sec\n",
      "Training set loss 4.878875732421875\n",
      "Training set accuracy 0.12035999447107315\n",
      "Test set accuracy 0.11800000071525574\n",
      "-------------------------------\n",
      "83/1000, 0.01 sec, 1.07 sec\n",
      "Training set loss 4.849386215209961\n",
      "Training set accuracy 0.12195999920368195\n",
      "Test set accuracy 0.12150000035762787\n",
      "-------------------------------\n",
      "84/1000, 0.01 sec, 1.09 sec\n",
      "Training set loss 4.821399688720703\n",
      "Training set accuracy 0.12363999336957932\n",
      "Test set accuracy 0.12399999797344208\n",
      "-------------------------------\n",
      "85/1000, 0.01 sec, 1.10 sec\n",
      "Training set loss 4.794710636138916\n",
      "Training set accuracy 0.125\n",
      "Test set accuracy 0.12610000371932983\n",
      "-------------------------------\n",
      "86/1000, 0.01 sec, 1.11 sec\n",
      "Training set loss 4.769108772277832\n",
      "Training set accuracy 0.1260399967432022\n",
      "Test set accuracy 0.12870000302791595\n",
      "-------------------------------\n",
      "87/1000, 0.01 sec, 1.12 sec\n",
      "Training set loss 4.7444353103637695\n",
      "Training set accuracy 0.12695999443531036\n",
      "Test set accuracy 0.131400004029274\n",
      "-------------------------------\n",
      "88/1000, 0.01 sec, 1.13 sec\n",
      "Training set loss 4.720581531524658\n",
      "Training set accuracy 0.12807999551296234\n",
      "Test set accuracy 0.13419999182224274\n",
      "-------------------------------\n",
      "89/1000, 0.01 sec, 1.14 sec\n",
      "Training set loss 4.697450637817383\n",
      "Training set accuracy 0.12865999341011047\n",
      "Test set accuracy 0.13589999079704285\n",
      "-------------------------------\n",
      "90/1000, 0.01 sec, 1.16 sec\n",
      "Training set loss 4.674966335296631\n",
      "Training set accuracy 0.12999999523162842\n",
      "Test set accuracy 0.1386999934911728\n",
      "-------------------------------\n",
      "91/1000, 0.01 sec, 1.17 sec\n",
      "Training set loss 4.65304708480835\n",
      "Training set accuracy 0.13142000138759613\n",
      "Test set accuracy 0.1395999938249588\n",
      "-------------------------------\n",
      "92/1000, 0.01 sec, 1.18 sec\n",
      "Training set loss 4.631645679473877\n",
      "Training set accuracy 0.13261999189853668\n",
      "Test set accuracy 0.14019998908042908\n",
      "-------------------------------\n",
      "93/1000, 0.01 sec, 1.19 sec\n",
      "Training set loss 4.610715866088867\n",
      "Training set accuracy 0.13385999202728271\n",
      "Test set accuracy 0.141199991106987\n",
      "-------------------------------\n",
      "94/1000, 0.01 sec, 1.20 sec\n",
      "Training set loss 4.590236663818359\n",
      "Training set accuracy 0.1348000019788742\n",
      "Test set accuracy 0.14229999482631683\n",
      "-------------------------------\n",
      "95/1000, 0.01 sec, 1.22 sec\n",
      "Training set loss 4.570167541503906\n",
      "Training set accuracy 0.13595999777317047\n",
      "Test set accuracy 0.14319999516010284\n",
      "-------------------------------\n",
      "96/1000, 0.01 sec, 1.23 sec\n",
      "Training set loss 4.550481796264648\n",
      "Training set accuracy 0.13721999526023865\n",
      "Test set accuracy 0.14499999582767487\n",
      "-------------------------------\n",
      "97/1000, 0.01 sec, 1.24 sec\n",
      "Training set loss 4.531158447265625\n",
      "Training set accuracy 0.1383799910545349\n",
      "Test set accuracy 0.14549998939037323\n",
      "-------------------------------\n",
      "98/1000, 0.01 sec, 1.25 sec\n",
      "Training set loss 4.51217794418335\n",
      "Training set accuracy 0.13910000026226044\n",
      "Test set accuracy 0.14649999141693115\n",
      "-------------------------------\n",
      "99/1000, 0.01 sec, 1.26 sec\n",
      "Training set loss 4.493513584136963\n",
      "Training set accuracy 0.14023999869823456\n",
      "Test set accuracy 0.1476999968290329\n",
      "-------------------------------\n",
      "100/1000, 0.01 sec, 1.27 sec\n",
      "Training set loss 4.475141525268555\n",
      "Training set accuracy 0.14107999205589294\n",
      "Test set accuracy 0.14880000054836273\n",
      "-------------------------------\n",
      "101/1000, 0.01 sec, 1.29 sec\n",
      "Training set loss 4.457054138183594\n",
      "Training set accuracy 0.14190000295639038\n",
      "Test set accuracy 0.14949999749660492\n",
      "-------------------------------\n",
      "102/1000, 0.01 sec, 1.30 sec\n",
      "Training set loss 4.439248085021973\n",
      "Training set accuracy 0.14282000064849854\n",
      "Test set accuracy 0.14999999105930328\n",
      "-------------------------------\n",
      "103/1000, 0.01 sec, 1.31 sec\n",
      "Training set loss 4.421698093414307\n",
      "Training set accuracy 0.14367999136447906\n",
      "Test set accuracy 0.15079998970031738\n",
      "-------------------------------\n",
      "104/1000, 0.01 sec, 1.32 sec\n",
      "Training set loss 4.404389381408691\n",
      "Training set accuracy 0.14417999982833862\n",
      "Test set accuracy 0.15070000290870667\n",
      "-------------------------------\n",
      "105/1000, 0.01 sec, 1.33 sec\n",
      "Training set loss 4.3873138427734375\n",
      "Training set accuracy 0.1446399986743927\n",
      "Test set accuracy 0.15139999985694885\n",
      "-------------------------------\n",
      "106/1000, 0.01 sec, 1.34 sec\n",
      "Training set loss 4.370462894439697\n",
      "Training set accuracy 0.1454000025987625\n",
      "Test set accuracy 0.15199999511241913\n",
      "-------------------------------\n",
      "107/1000, 0.01 sec, 1.36 sec\n",
      "Training set loss 4.353823184967041\n",
      "Training set accuracy 0.146479994058609\n",
      "Test set accuracy 0.1525999903678894\n",
      "-------------------------------\n",
      "108/1000, 0.01 sec, 1.37 sec\n",
      "Training set loss 4.337376594543457\n",
      "Training set accuracy 0.14688000082969666\n",
      "Test set accuracy 0.15359999239444733\n",
      "-------------------------------\n",
      "109/1000, 0.01 sec, 1.38 sec\n",
      "Training set loss 4.321109771728516\n",
      "Training set accuracy 0.14739999175071716\n",
      "Test set accuracy 0.1542000025510788\n",
      "-------------------------------\n",
      "110/1000, 0.01 sec, 1.39 sec\n",
      "Training set loss 4.305019855499268\n",
      "Training set accuracy 0.14778000116348267\n",
      "Test set accuracy 0.15549999475479126\n",
      "-------------------------------\n",
      "111/1000, 0.01 sec, 1.40 sec\n",
      "Training set loss 4.289093971252441\n",
      "Training set accuracy 0.14883999526500702\n",
      "Test set accuracy 0.15639999508857727\n",
      "-------------------------------\n",
      "112/1000, 0.01 sec, 1.41 sec\n",
      "Training set loss 4.2733235359191895\n",
      "Training set accuracy 0.149959996342659\n",
      "Test set accuracy 0.15809999406337738\n",
      "-------------------------------\n",
      "113/1000, 0.01 sec, 1.43 sec\n",
      "Training set loss 4.257702350616455\n",
      "Training set accuracy 0.15029999613761902\n",
      "Test set accuracy 0.15940000116825104\n",
      "-------------------------------\n",
      "114/1000, 0.01 sec, 1.44 sec\n",
      "Training set loss 4.242232799530029\n",
      "Training set accuracy 0.15065999329090118\n",
      "Test set accuracy 0.16040000319480896\n",
      "-------------------------------\n",
      "115/1000, 0.01 sec, 1.45 sec\n",
      "Training set loss 4.226942539215088\n",
      "Training set accuracy 0.15123999118804932\n",
      "Test set accuracy 0.1607999950647354\n",
      "-------------------------------\n",
      "116/1000, 0.01 sec, 1.46 sec\n",
      "Training set loss 4.211893558502197\n",
      "Training set accuracy 0.1519400030374527\n",
      "Test set accuracy 0.16009999811649323\n",
      "-------------------------------\n",
      "117/1000, 0.01 sec, 1.47 sec\n",
      "Training set loss 4.1971869468688965\n",
      "Training set accuracy 0.15230000019073486\n",
      "Test set accuracy 0.16120000183582306\n",
      "-------------------------------\n",
      "118/1000, 0.01 sec, 1.49 sec\n",
      "Training set loss 4.182973861694336\n",
      "Training set accuracy 0.15277999639511108\n",
      "Test set accuracy 0.16279999911785126\n",
      "-------------------------------\n",
      "119/1000, 0.01 sec, 1.50 sec\n",
      "Training set loss 4.1693854331970215\n",
      "Training set accuracy 0.1535400003194809\n",
      "Test set accuracy 0.16359999775886536\n",
      "-------------------------------\n",
      "120/1000, 0.01 sec, 1.51 sec\n",
      "Training set loss 4.156471252441406\n",
      "Training set accuracy 0.15389999747276306\n",
      "Test set accuracy 0.16439999639987946\n",
      "-------------------------------\n",
      "121/1000, 0.01 sec, 1.52 sec\n",
      "Training set loss 4.144220352172852\n",
      "Training set accuracy 0.15423999726772308\n",
      "Test set accuracy 0.16489998996257782\n",
      "-------------------------------\n",
      "122/1000, 0.01 sec, 1.53 sec\n",
      "Training set loss 4.132562160491943\n",
      "Training set accuracy 0.15481999516487122\n",
      "Test set accuracy 0.16569998860359192\n",
      "-------------------------------\n",
      "123/1000, 0.01 sec, 1.54 sec\n",
      "Training set loss 4.121441841125488\n",
      "Training set accuracy 0.15544000267982483\n",
      "Test set accuracy 0.16659998893737793\n",
      "-------------------------------\n",
      "124/1000, 0.01 sec, 1.55 sec\n",
      "Training set loss 4.110842227935791\n",
      "Training set accuracy 0.15600000321865082\n",
      "Test set accuracy 0.1673000007867813\n",
      "-------------------------------\n",
      "125/1000, 0.01 sec, 1.57 sec\n",
      "Training set loss 4.100716590881348\n",
      "Training set accuracy 0.15629999339580536\n",
      "Test set accuracy 0.16769999265670776\n",
      "-------------------------------\n",
      "126/1000, 0.01 sec, 1.58 sec\n",
      "Training set loss 4.0910234451293945\n",
      "Training set accuracy 0.15715999901294708\n",
      "Test set accuracy 0.1678999960422516\n",
      "-------------------------------\n",
      "127/1000, 0.01 sec, 1.59 sec\n",
      "Training set loss 4.0817341804504395\n",
      "Training set accuracy 0.15771999955177307\n",
      "Test set accuracy 0.16910000145435333\n",
      "-------------------------------\n",
      "128/1000, 0.01 sec, 1.60 sec\n",
      "Training set loss 4.072796821594238\n",
      "Training set accuracy 0.1586799919605255\n",
      "Test set accuracy 0.16949999332427979\n",
      "-------------------------------\n",
      "129/1000, 0.01 sec, 1.61 sec\n",
      "Training set loss 4.064161777496338\n",
      "Training set accuracy 0.15901999175548553\n",
      "Test set accuracy 0.1704999953508377\n",
      "-------------------------------\n",
      "130/1000, 0.01 sec, 1.62 sec\n",
      "Training set loss 4.055795192718506\n",
      "Training set accuracy 0.15941999852657318\n",
      "Test set accuracy 0.17099998891353607\n",
      "-------------------------------\n",
      "131/1000, 0.01 sec, 1.64 sec\n",
      "Training set loss 4.047672748565674\n",
      "Training set accuracy 0.15997999906539917\n",
      "Test set accuracy 0.17139999568462372\n",
      "-------------------------------\n",
      "132/1000, 0.01 sec, 1.65 sec\n",
      "Training set loss 4.039765357971191\n",
      "Training set accuracy 0.16067999601364136\n",
      "Test set accuracy 0.17159999907016754\n",
      "-------------------------------\n",
      "133/1000, 0.01 sec, 1.66 sec\n",
      "Training set loss 4.0320515632629395\n",
      "Training set accuracy 0.1612599939107895\n",
      "Test set accuracy 0.1720999926328659\n",
      "-------------------------------\n",
      "134/1000, 0.01 sec, 1.67 sec\n",
      "Training set loss 4.024527072906494\n",
      "Training set accuracy 0.1615999937057495\n",
      "Test set accuracy 0.1727999895811081\n",
      "-------------------------------\n",
      "135/1000, 0.01 sec, 1.68 sec\n",
      "Training set loss 4.017173767089844\n",
      "Training set accuracy 0.1621599942445755\n",
      "Test set accuracy 0.1735999882221222\n",
      "-------------------------------\n",
      "136/1000, 0.01 sec, 1.69 sec\n",
      "Training set loss 4.009984493255615\n",
      "Training set accuracy 0.1620599925518036\n",
      "Test set accuracy 0.17440000176429749\n",
      "-------------------------------\n",
      "137/1000, 0.01 sec, 1.70 sec\n",
      "Training set loss 4.00295352935791\n",
      "Training set accuracy 0.16247999668121338\n",
      "Test set accuracy 0.17479999363422394\n",
      "-------------------------------\n",
      "138/1000, 0.01 sec, 1.72 sec\n",
      "Training set loss 3.9960708618164062\n",
      "Training set accuracy 0.1629599928855896\n",
      "Test set accuracy 0.17520000040531158\n",
      "-------------------------------\n",
      "139/1000, 0.01 sec, 1.73 sec\n",
      "Training set loss 3.989330530166626\n",
      "Training set accuracy 0.1633799970149994\n",
      "Test set accuracy 0.17569999396800995\n",
      "-------------------------------\n",
      "140/1000, 0.01 sec, 1.74 sec\n",
      "Training set loss 3.982724905014038\n",
      "Training set accuracy 0.16409999132156372\n",
      "Test set accuracy 0.17569999396800995\n",
      "-------------------------------\n",
      "141/1000, 0.01 sec, 1.75 sec\n",
      "Training set loss 3.976243019104004\n",
      "Training set accuracy 0.1643799990415573\n",
      "Test set accuracy 0.1762000024318695\n",
      "-------------------------------\n",
      "142/1000, 0.01 sec, 1.76 sec\n",
      "Training set loss 3.969883441925049\n",
      "Training set accuracy 0.16487999260425568\n",
      "Test set accuracy 0.17729999125003815\n",
      "-------------------------------\n",
      "143/1000, 0.01 sec, 1.77 sec\n",
      "Training set loss 3.9636411666870117\n",
      "Training set accuracy 0.1652199923992157\n",
      "Test set accuracy 0.17759999632835388\n",
      "-------------------------------\n",
      "144/1000, 0.01 sec, 1.78 sec\n",
      "Training set loss 3.957507371902466\n",
      "Training set accuracy 0.1653199940919876\n",
      "Test set accuracy 0.17799998819828033\n",
      "-------------------------------\n",
      "145/1000, 0.01 sec, 1.79 sec\n",
      "Training set loss 3.9514801502227783\n",
      "Training set accuracy 0.16579999029636383\n",
      "Test set accuracy 0.17829999327659607\n",
      "-------------------------------\n",
      "146/1000, 0.01 sec, 1.80 sec\n",
      "Training set loss 3.945556879043579\n",
      "Training set accuracy 0.16603998839855194\n",
      "Test set accuracy 0.17899999022483826\n",
      "-------------------------------\n",
      "147/1000, 0.01 sec, 1.81 sec\n",
      "Training set loss 3.9397342205047607\n",
      "Training set accuracy 0.16641999781131744\n",
      "Test set accuracy 0.17899999022483826\n",
      "-------------------------------\n",
      "148/1000, 0.01 sec, 1.83 sec\n",
      "Training set loss 3.9340105056762695\n",
      "Training set accuracy 0.16674000024795532\n",
      "Test set accuracy 0.17949999868869781\n",
      "-------------------------------\n",
      "149/1000, 0.01 sec, 1.84 sec\n",
      "Training set loss 3.928378105163574\n",
      "Training set accuracy 0.1673000007867813\n",
      "Test set accuracy 0.18019999563694\n",
      "-------------------------------\n",
      "150/1000, 0.01 sec, 1.85 sec\n",
      "Training set loss 3.9228386878967285\n",
      "Training set accuracy 0.16752000153064728\n",
      "Test set accuracy 0.1808999925851822\n",
      "-------------------------------\n",
      "151/1000, 0.01 sec, 1.86 sec\n",
      "Training set loss 3.917386054992676\n",
      "Training set accuracy 0.1677599996328354\n",
      "Test set accuracy 0.18140000104904175\n",
      "-------------------------------\n",
      "152/1000, 0.01 sec, 1.87 sec\n",
      "Training set loss 3.9120192527770996\n",
      "Training set accuracy 0.1682799905538559\n",
      "Test set accuracy 0.18140000104904175\n",
      "-------------------------------\n",
      "153/1000, 0.01 sec, 1.88 sec\n",
      "Training set loss 3.9067347049713135\n",
      "Training set accuracy 0.1684200018644333\n",
      "Test set accuracy 0.18150000274181366\n",
      "-------------------------------\n",
      "154/1000, 0.01 sec, 1.89 sec\n",
      "Training set loss 3.901529312133789\n",
      "Training set accuracy 0.1686599999666214\n",
      "Test set accuracy 0.1818999946117401\n",
      "-------------------------------\n",
      "155/1000, 0.01 sec, 1.90 sec\n",
      "Training set loss 3.8964030742645264\n",
      "Training set accuracy 0.16915999352931976\n",
      "Test set accuracy 0.18239998817443848\n",
      "-------------------------------\n",
      "156/1000, 0.01 sec, 1.91 sec\n",
      "Training set loss 3.891350507736206\n",
      "Training set accuracy 0.16933999955654144\n",
      "Test set accuracy 0.18239998817443848\n",
      "-------------------------------\n",
      "157/1000, 0.01 sec, 1.93 sec\n",
      "Training set loss 3.8863704204559326\n",
      "Training set accuracy 0.1698399931192398\n",
      "Test set accuracy 0.18289999663829803\n",
      "-------------------------------\n",
      "158/1000, 0.01 sec, 1.94 sec\n",
      "Training set loss 3.8814618587493896\n",
      "Training set accuracy 0.16995999217033386\n",
      "Test set accuracy 0.1826999932527542\n",
      "-------------------------------\n",
      "159/1000, 0.01 sec, 1.95 sec\n",
      "Training set loss 3.876619815826416\n",
      "Training set accuracy 0.17007999122142792\n",
      "Test set accuracy 0.18279999494552612\n",
      "-------------------------------\n",
      "160/1000, 0.01 sec, 1.96 sec\n",
      "Training set loss 3.8718454837799072\n",
      "Training set accuracy 0.17023999989032745\n",
      "Test set accuracy 0.18299999833106995\n",
      "-------------------------------\n",
      "161/1000, 0.01 sec, 1.97 sec\n",
      "Training set loss 3.867133617401123\n",
      "Training set accuracy 0.17056000232696533\n",
      "Test set accuracy 0.18369999527931213\n",
      "-------------------------------\n",
      "162/1000, 0.01 sec, 1.98 sec\n",
      "Training set loss 3.862478733062744\n",
      "Training set accuracy 0.17071999609470367\n",
      "Test set accuracy 0.18369999527931213\n",
      "-------------------------------\n",
      "163/1000, 0.01 sec, 1.99 sec\n",
      "Training set loss 3.8578760623931885\n",
      "Training set accuracy 0.1710200011730194\n",
      "Test set accuracy 0.1841999888420105\n",
      "-------------------------------\n",
      "164/1000, 0.01 sec, 2.01 sec\n",
      "Training set loss 3.8533270359039307\n",
      "Training set accuracy 0.17117999494075775\n",
      "Test set accuracy 0.18410000205039978\n",
      "-------------------------------\n",
      "165/1000, 0.01 sec, 2.02 sec\n",
      "Training set loss 3.8488292694091797\n",
      "Training set accuracy 0.17139999568462372\n",
      "Test set accuracy 0.18449999392032623\n",
      "-------------------------------\n",
      "166/1000, 0.01 sec, 2.03 sec\n",
      "Training set loss 3.8443775177001953\n",
      "Training set accuracy 0.17189998924732208\n",
      "Test set accuracy 0.18469999730587006\n",
      "-------------------------------\n",
      "167/1000, 0.01 sec, 2.04 sec\n",
      "Training set loss 3.839970588684082\n",
      "Training set accuracy 0.17204000055789948\n",
      "Test set accuracy 0.1850999891757965\n",
      "-------------------------------\n",
      "168/1000, 0.01 sec, 2.05 sec\n",
      "Training set loss 3.835602045059204\n",
      "Training set accuracy 0.17233999073505402\n",
      "Test set accuracy 0.1850000023841858\n",
      "-------------------------------\n",
      "169/1000, 0.01 sec, 2.06 sec\n",
      "Training set loss 3.8312718868255615\n",
      "Training set accuracy 0.17226000130176544\n",
      "Test set accuracy 0.18519999086856842\n",
      "-------------------------------\n",
      "170/1000, 0.01 sec, 2.08 sec\n",
      "Training set loss 3.826982021331787\n",
      "Training set accuracy 0.17245998978614807\n",
      "Test set accuracy 0.18539999425411224\n",
      "-------------------------------\n",
      "171/1000, 0.01 sec, 2.09 sec\n",
      "Training set loss 3.822726249694824\n",
      "Training set accuracy 0.17260000109672546\n",
      "Test set accuracy 0.18559999763965607\n",
      "-------------------------------\n",
      "172/1000, 0.01 sec, 2.10 sec\n",
      "Training set loss 3.818502187728882\n",
      "Training set accuracy 0.1726199984550476\n",
      "Test set accuracy 0.18649999797344208\n",
      "-------------------------------\n",
      "173/1000, 0.01 sec, 2.11 sec\n",
      "Training set loss 3.814305543899536\n",
      "Training set accuracy 0.17295999825000763\n",
      "Test set accuracy 0.18709999322891235\n",
      "-------------------------------\n",
      "174/1000, 0.01 sec, 2.12 sec\n",
      "Training set loss 3.810136079788208\n",
      "Training set accuracy 0.17309999465942383\n",
      "Test set accuracy 0.18709999322891235\n",
      "-------------------------------\n",
      "175/1000, 0.01 sec, 2.13 sec\n",
      "Training set loss 3.805994987487793\n",
      "Training set accuracy 0.17339999973773956\n",
      "Test set accuracy 0.18689998984336853\n",
      "-------------------------------\n",
      "176/1000, 0.01 sec, 2.15 sec\n",
      "Training set loss 3.8018760681152344\n",
      "Training set accuracy 0.17373999953269958\n",
      "Test set accuracy 0.18699999153614044\n",
      "-------------------------------\n",
      "177/1000, 0.01 sec, 2.16 sec\n",
      "Training set loss 3.797776222229004\n",
      "Training set accuracy 0.1740799993276596\n",
      "Test set accuracy 0.1876000016927719\n",
      "-------------------------------\n",
      "178/1000, 0.01 sec, 2.17 sec\n",
      "Training set loss 3.7936911582946777\n",
      "Training set accuracy 0.1740799993276596\n",
      "Test set accuracy 0.1873999983072281\n",
      "-------------------------------\n",
      "179/1000, 0.01 sec, 2.18 sec\n",
      "Training set loss 3.789619207382202\n",
      "Training set accuracy 0.1742599904537201\n",
      "Test set accuracy 0.18799999356269836\n",
      "-------------------------------\n",
      "180/1000, 0.01 sec, 2.19 sec\n",
      "Training set loss 3.785566806793213\n",
      "Training set accuracy 0.17428000271320343\n",
      "Test set accuracy 0.188400000333786\n",
      "-------------------------------\n",
      "181/1000, 0.01 sec, 2.20 sec\n",
      "Training set loss 3.781526803970337\n",
      "Training set accuracy 0.17431999742984772\n",
      "Test set accuracy 0.18879999220371246\n",
      "-------------------------------\n",
      "182/1000, 0.01 sec, 2.22 sec\n",
      "Training set loss 3.777501106262207\n",
      "Training set accuracy 0.17443999648094177\n",
      "Test set accuracy 0.18889999389648438\n",
      "-------------------------------\n",
      "183/1000, 0.01 sec, 2.23 sec\n",
      "Training set loss 3.7734878063201904\n",
      "Training set accuracy 0.17455999553203583\n",
      "Test set accuracy 0.18879999220371246\n",
      "-------------------------------\n",
      "184/1000, 0.01 sec, 2.24 sec\n",
      "Training set loss 3.7694859504699707\n",
      "Training set accuracy 0.17462000250816345\n",
      "Test set accuracy 0.18889999389648438\n",
      "-------------------------------\n",
      "185/1000, 0.01 sec, 2.25 sec\n",
      "Training set loss 3.7654943466186523\n",
      "Training set accuracy 0.17486000061035156\n",
      "Test set accuracy 0.18889999389648438\n",
      "-------------------------------\n",
      "186/1000, 0.01 sec, 2.26 sec\n",
      "Training set loss 3.761514663696289\n",
      "Training set accuracy 0.1750199943780899\n",
      "Test set accuracy 0.18940000236034393\n",
      "-------------------------------\n",
      "187/1000, 0.01 sec, 2.28 sec\n",
      "Training set loss 3.7575392723083496\n",
      "Training set accuracy 0.17499999701976776\n",
      "Test set accuracy 0.18930000066757202\n",
      "-------------------------------\n",
      "188/1000, 0.01 sec, 2.29 sec\n",
      "Training set loss 3.75356125831604\n",
      "Training set accuracy 0.17523999512195587\n",
      "Test set accuracy 0.18940000236034393\n",
      "-------------------------------\n",
      "189/1000, 0.01 sec, 2.30 sec\n",
      "Training set loss 3.7495779991149902\n",
      "Training set accuracy 0.17525999248027802\n",
      "Test set accuracy 0.1898999959230423\n",
      "-------------------------------\n",
      "190/1000, 0.01 sec, 2.31 sec\n",
      "Training set loss 3.7455928325653076\n",
      "Training set accuracy 0.17555999755859375\n",
      "Test set accuracy 0.18969999253749847\n",
      "-------------------------------\n",
      "191/1000, 0.01 sec, 2.32 sec\n",
      "Training set loss 3.7415966987609863\n",
      "Training set accuracy 0.17576000094413757\n",
      "Test set accuracy 0.18949998915195465\n",
      "-------------------------------\n",
      "192/1000, 0.01 sec, 2.34 sec\n",
      "Training set loss 3.7376010417938232\n",
      "Training set accuracy 0.17586000263690948\n",
      "Test set accuracy 0.18959999084472656\n",
      "-------------------------------\n",
      "193/1000, 0.01 sec, 2.35 sec\n",
      "Training set loss 3.7336199283599854\n",
      "Training set accuracy 0.17617999017238617\n",
      "Test set accuracy 0.19020000100135803\n",
      "-------------------------------\n",
      "194/1000, 0.01 sec, 2.36 sec\n",
      "Training set loss 3.7296483516693115\n",
      "Training set accuracy 0.17644000053405762\n",
      "Test set accuracy 0.19020000100135803\n",
      "-------------------------------\n",
      "195/1000, 0.01 sec, 2.37 sec\n",
      "Training set loss 3.72566556930542\n",
      "Training set accuracy 0.17673999071121216\n",
      "Test set accuracy 0.19009999930858612\n",
      "-------------------------------\n",
      "196/1000, 0.01 sec, 2.38 sec\n",
      "Training set loss 3.721675157546997\n",
      "Training set accuracy 0.17723999917507172\n",
      "Test set accuracy 0.19009999930858612\n",
      "-------------------------------\n",
      "197/1000, 0.01 sec, 2.39 sec\n",
      "Training set loss 3.7176804542541504\n",
      "Training set accuracy 0.177279993891716\n",
      "Test set accuracy 0.19059999287128448\n",
      "-------------------------------\n",
      "198/1000, 0.01 sec, 2.41 sec\n",
      "Training set loss 3.713684320449829\n",
      "Training set accuracy 0.17749999463558197\n",
      "Test set accuracy 0.19099999964237213\n",
      "-------------------------------\n",
      "199/1000, 0.01 sec, 2.42 sec\n",
      "Training set loss 3.709688663482666\n",
      "Training set accuracy 0.1777999997138977\n",
      "Test set accuracy 0.19119998812675476\n",
      "-------------------------------\n",
      "200/1000, 0.01 sec, 2.43 sec\n",
      "Training set loss 3.70569109916687\n",
      "Training set accuracy 0.177839994430542\n",
      "Test set accuracy 0.1914999932050705\n",
      "-------------------------------\n",
      "201/1000, 0.01 sec, 2.44 sec\n",
      "Training set loss 3.701686382293701\n",
      "Training set accuracy 0.17803999781608582\n",
      "Test set accuracy 0.19189999997615814\n",
      "-------------------------------\n",
      "202/1000, 0.01 sec, 2.45 sec\n",
      "Training set loss 3.697683572769165\n",
      "Training set accuracy 0.17809998989105225\n",
      "Test set accuracy 0.19249999523162842\n",
      "-------------------------------\n",
      "203/1000, 0.01 sec, 2.47 sec\n",
      "Training set loss 3.6936793327331543\n",
      "Training set accuracy 0.17815999686717987\n",
      "Test set accuracy 0.19280000030994415\n",
      "-------------------------------\n",
      "204/1000, 0.01 sec, 2.48 sec\n",
      "Training set loss 3.6896777153015137\n",
      "Training set accuracy 0.17829999327659607\n",
      "Test set accuracy 0.19290000200271606\n",
      "-------------------------------\n",
      "205/1000, 0.01 sec, 2.49 sec\n",
      "Training set loss 3.6856801509857178\n",
      "Training set accuracy 0.17829999327659607\n",
      "Test set accuracy 0.19329999387264252\n",
      "-------------------------------\n",
      "206/1000, 0.01 sec, 2.50 sec\n",
      "Training set loss 3.6816792488098145\n",
      "Training set accuracy 0.1782199889421463\n",
      "Test set accuracy 0.19349999725818634\n",
      "-------------------------------\n",
      "207/1000, 0.01 sec, 2.52 sec\n",
      "Training set loss 3.6776812076568604\n",
      "Training set accuracy 0.1783600002527237\n",
      "Test set accuracy 0.19349999725818634\n",
      "-------------------------------\n",
      "208/1000, 0.01 sec, 2.53 sec\n",
      "Training set loss 3.6736834049224854\n",
      "Training set accuracy 0.1784999966621399\n",
      "Test set accuracy 0.19349999725818634\n",
      "-------------------------------\n",
      "209/1000, 0.01 sec, 2.54 sec\n",
      "Training set loss 3.6696856021881104\n",
      "Training set accuracy 0.1788799911737442\n",
      "Test set accuracy 0.19370000064373016\n",
      "-------------------------------\n",
      "210/1000, 0.01 sec, 2.55 sec\n",
      "Training set loss 3.6656882762908936\n",
      "Training set accuracy 0.17917999625205994\n",
      "Test set accuracy 0.19419999420642853\n",
      "-------------------------------\n",
      "211/1000, 0.01 sec, 2.56 sec\n",
      "Training set loss 3.6616885662078857\n",
      "Training set accuracy 0.17927999794483185\n",
      "Test set accuracy 0.19409999251365662\n",
      "-------------------------------\n",
      "212/1000, 0.01 sec, 2.58 sec\n",
      "Training set loss 3.6576948165893555\n",
      "Training set accuracy 0.17921999096870422\n",
      "Test set accuracy 0.1938999891281128\n",
      "-------------------------------\n",
      "213/1000, 0.01 sec, 2.59 sec\n",
      "Training set loss 3.6537024974823\n",
      "Training set accuracy 0.1795799881219864\n",
      "Test set accuracy 0.19419999420642853\n",
      "-------------------------------\n",
      "214/1000, 0.01 sec, 2.60 sec\n",
      "Training set loss 3.649710178375244\n",
      "Training set accuracy 0.17961999773979187\n",
      "Test set accuracy 0.19460000097751617\n",
      "-------------------------------\n",
      "215/1000, 0.01 sec, 2.61 sec\n",
      "Training set loss 3.6457149982452393\n",
      "Training set accuracy 0.17975999414920807\n",
      "Test set accuracy 0.19519999623298645\n",
      "-------------------------------\n",
      "216/1000, 0.01 sec, 2.62 sec\n",
      "Training set loss 3.641721248626709\n",
      "Training set accuracy 0.17999999225139618\n",
      "Test set accuracy 0.19509999454021454\n",
      "-------------------------------\n",
      "217/1000, 0.01 sec, 2.63 sec\n",
      "Training set loss 3.6377291679382324\n",
      "Training set accuracy 0.18017999827861786\n",
      "Test set accuracy 0.19519999623298645\n",
      "-------------------------------\n",
      "218/1000, 0.01 sec, 2.64 sec\n",
      "Training set loss 3.6337411403656006\n",
      "Training set accuracy 0.18026000261306763\n",
      "Test set accuracy 0.19519999623298645\n",
      "-------------------------------\n",
      "219/1000, 0.01 sec, 2.66 sec\n",
      "Training set loss 3.6297507286071777\n",
      "Training set accuracy 0.18060000240802765\n",
      "Test set accuracy 0.19529999792575836\n",
      "-------------------------------\n",
      "220/1000, 0.01 sec, 2.67 sec\n",
      "Training set loss 3.6257593631744385\n",
      "Training set accuracy 0.1808599978685379\n",
      "Test set accuracy 0.19579999148845673\n",
      "-------------------------------\n",
      "221/1000, 0.01 sec, 2.68 sec\n",
      "Training set loss 3.6217687129974365\n",
      "Training set accuracy 0.18101999163627625\n",
      "Test set accuracy 0.19599999487400055\n",
      "-------------------------------\n",
      "222/1000, 0.01 sec, 2.69 sec\n",
      "Training set loss 3.6177818775177\n",
      "Training set accuracy 0.181099995970726\n",
      "Test set accuracy 0.19659999012947083\n",
      "-------------------------------\n",
      "223/1000, 0.01 sec, 2.70 sec\n",
      "Training set loss 3.6137936115264893\n",
      "Training set accuracy 0.18115998804569244\n",
      "Test set accuracy 0.1973000019788742\n",
      "-------------------------------\n",
      "224/1000, 0.01 sec, 2.71 sec\n",
      "Training set loss 3.6098029613494873\n",
      "Training set accuracy 0.18143999576568604\n",
      "Test set accuracy 0.19749999046325684\n",
      "-------------------------------\n",
      "225/1000, 0.01 sec, 2.72 sec\n",
      "Training set loss 3.6058082580566406\n",
      "Training set accuracy 0.18174000084400177\n",
      "Test set accuracy 0.19759999215602875\n",
      "-------------------------------\n",
      "226/1000, 0.01 sec, 2.73 sec\n",
      "Training set loss 3.6018080711364746\n",
      "Training set accuracy 0.18191999197006226\n",
      "Test set accuracy 0.19829998910427094\n",
      "-------------------------------\n",
      "227/1000, 0.01 sec, 2.75 sec\n",
      "Training set loss 3.59780216217041\n",
      "Training set accuracy 0.1823199987411499\n",
      "Test set accuracy 0.19820000231266022\n",
      "-------------------------------\n",
      "228/1000, 0.01 sec, 2.76 sec\n",
      "Training set loss 3.5937907695770264\n",
      "Training set accuracy 0.18239998817443848\n",
      "Test set accuracy 0.19859999418258667\n",
      "-------------------------------\n",
      "229/1000, 0.01 sec, 2.77 sec\n",
      "Training set loss 3.5897698402404785\n",
      "Training set accuracy 0.18291999399662018\n",
      "Test set accuracy 0.19849999248981476\n",
      "-------------------------------\n",
      "230/1000, 0.01 sec, 2.78 sec\n",
      "Training set loss 3.585744857788086\n",
      "Training set accuracy 0.18310000002384186\n",
      "Test set accuracy 0.19839999079704285\n",
      "-------------------------------\n",
      "231/1000, 0.01 sec, 2.79 sec\n",
      "Training set loss 3.581712484359741\n",
      "Training set accuracy 0.18355999886989594\n",
      "Test set accuracy 0.19820000231266022\n",
      "-------------------------------\n",
      "232/1000, 0.01 sec, 2.80 sec\n",
      "Training set loss 3.577676773071289\n",
      "Training set accuracy 0.18371999263763428\n",
      "Test set accuracy 0.19820000231266022\n",
      "-------------------------------\n",
      "233/1000, 0.01 sec, 2.81 sec\n",
      "Training set loss 3.573638677597046\n",
      "Training set accuracy 0.18400000035762787\n",
      "Test set accuracy 0.19859999418258667\n",
      "-------------------------------\n",
      "234/1000, 0.01 sec, 2.83 sec\n",
      "Training set loss 3.5696027278900146\n",
      "Training set accuracy 0.18389999866485596\n",
      "Test set accuracy 0.19869999587535858\n",
      "-------------------------------\n",
      "235/1000, 0.01 sec, 2.84 sec\n",
      "Training set loss 3.565561056137085\n",
      "Training set accuracy 0.18432000279426575\n",
      "Test set accuracy 0.19900000095367432\n",
      "-------------------------------\n",
      "236/1000, 0.01 sec, 2.85 sec\n",
      "Training set loss 3.561511516571045\n",
      "Training set accuracy 0.18441998958587646\n",
      "Test set accuracy 0.19909998774528503\n",
      "-------------------------------\n",
      "237/1000, 0.01 sec, 2.86 sec\n",
      "Training set loss 3.5574612617492676\n",
      "Training set accuracy 0.18493999540805817\n",
      "Test set accuracy 0.19949999451637268\n",
      "-------------------------------\n",
      "238/1000, 0.01 sec, 2.87 sec\n",
      "Training set loss 3.5534074306488037\n",
      "Training set accuracy 0.18463999032974243\n",
      "Test set accuracy 0.19949999451637268\n",
      "-------------------------------\n",
      "239/1000, 0.01 sec, 2.88 sec\n",
      "Training set loss 3.5493533611297607\n",
      "Training set accuracy 0.18517999351024628\n",
      "Test set accuracy 0.20019999146461487\n",
      "-------------------------------\n",
      "240/1000, 0.01 sec, 2.89 sec\n",
      "Training set loss 3.545288562774658\n",
      "Training set accuracy 0.18487998843193054\n",
      "Test set accuracy 0.19949999451637268\n",
      "-------------------------------\n",
      "241/1000, 0.01 sec, 2.90 sec\n",
      "Training set loss 3.541219711303711\n",
      "Training set accuracy 0.18585999310016632\n",
      "Test set accuracy 0.2004999965429306\n",
      "-------------------------------\n",
      "242/1000, 0.01 sec, 2.91 sec\n",
      "Training set loss 3.537156105041504\n",
      "Training set accuracy 0.18497999012470245\n",
      "Test set accuracy 0.19929999113082886\n",
      "-------------------------------\n",
      "243/1000, 0.01 sec, 2.93 sec\n",
      "Training set loss 3.5330991744995117\n",
      "Training set accuracy 0.18679998815059662\n",
      "Test set accuracy 0.2011999934911728\n",
      "-------------------------------\n",
      "244/1000, 0.01 sec, 2.94 sec\n",
      "Training set loss 3.5290493965148926\n",
      "Training set accuracy 0.18517999351024628\n",
      "Test set accuracy 0.19919998943805695\n",
      "-------------------------------\n",
      "245/1000, 0.01 sec, 2.95 sec\n",
      "Training set loss 3.5250167846679688\n",
      "Training set accuracy 0.18751999735832214\n",
      "Test set accuracy 0.20149999856948853\n",
      "-------------------------------\n",
      "246/1000, 0.01 sec, 2.96 sec\n",
      "Training set loss 3.5210092067718506\n",
      "Training set accuracy 0.18505999445915222\n",
      "Test set accuracy 0.19859999418258667\n",
      "-------------------------------\n",
      "247/1000, 0.01 sec, 2.97 sec\n",
      "Training set loss 3.5170648097991943\n",
      "Training set accuracy 0.18841999769210815\n",
      "Test set accuracy 0.2028999924659729\n",
      "-------------------------------\n",
      "248/1000, 0.01 sec, 2.98 sec\n",
      "Training set loss 3.51322340965271\n",
      "Training set accuracy 0.18493999540805817\n",
      "Test set accuracy 0.19789999723434448\n",
      "-------------------------------\n",
      "249/1000, 0.01 sec, 2.99 sec\n",
      "Training set loss 3.5095536708831787\n",
      "Training set accuracy 0.18930000066757202\n",
      "Test set accuracy 0.20419999957084656\n",
      "-------------------------------\n",
      "250/1000, 0.01 sec, 3.01 sec\n",
      "Training set loss 3.5061676502227783\n",
      "Training set accuracy 0.18473999202251434\n",
      "Test set accuracy 0.19789999723434448\n",
      "-------------------------------\n",
      "251/1000, 0.01 sec, 3.02 sec\n",
      "Training set loss 3.5032198429107666\n",
      "Training set accuracy 0.19088000059127808\n",
      "Test set accuracy 0.20589999854564667\n",
      "-------------------------------\n",
      "252/1000, 0.01 sec, 3.03 sec\n",
      "Training set loss 3.5009725093841553\n",
      "Training set accuracy 0.18445999920368195\n",
      "Test set accuracy 0.19709999859333038\n",
      "-------------------------------\n",
      "253/1000, 0.01 sec, 3.04 sec\n",
      "Training set loss 3.500054359436035\n",
      "Training set accuracy 0.19351999461650848\n",
      "Test set accuracy 0.2076999992132187\n",
      "-------------------------------\n",
      "254/1000, 0.01 sec, 3.05 sec\n",
      "Training set loss 3.500847339630127\n",
      "Training set accuracy 0.1823599934577942\n",
      "Test set accuracy 0.19449999928474426\n",
      "-------------------------------\n",
      "255/1000, 0.01 sec, 3.06 sec\n",
      "Training set loss 3.5057623386383057\n",
      "Training set accuracy 0.19591999053955078\n",
      "Test set accuracy 0.211899995803833\n",
      "-------------------------------\n",
      "256/1000, 0.01 sec, 3.07 sec\n",
      "Training set loss 3.5144999027252197\n",
      "Training set accuracy 0.17875999212265015\n",
      "Test set accuracy 0.1881999969482422\n",
      "-------------------------------\n",
      "257/1000, 0.01 sec, 3.08 sec\n",
      "Training set loss 3.536595582962036\n",
      "Training set accuracy 0.19953998923301697\n",
      "Test set accuracy 0.21769998967647552\n",
      "-------------------------------\n",
      "258/1000, 0.01 sec, 3.09 sec\n",
      "Training set loss 3.564218282699585\n",
      "Training set accuracy 0.17372000217437744\n",
      "Test set accuracy 0.17989999055862427\n",
      "-------------------------------\n",
      "259/1000, 0.01 sec, 3.10 sec\n",
      "Training set loss 3.6363821029663086\n",
      "Training set accuracy 0.20389999449253082\n",
      "Test set accuracy 0.21799999475479126\n",
      "-------------------------------\n",
      "260/1000, 0.01 sec, 3.12 sec\n",
      "Training set loss 3.699979543685913\n",
      "Training set accuracy 0.16433998942375183\n",
      "Test set accuracy 0.17029999196529388\n",
      "-------------------------------\n",
      "261/1000, 0.01 sec, 3.13 sec\n",
      "Training set loss 3.9020466804504395\n",
      "Training set accuracy 0.20747999846935272\n",
      "Test set accuracy 0.21709999442100525\n",
      "-------------------------------\n",
      "262/1000, 0.01 sec, 3.14 sec\n",
      "Training set loss 3.980862617492676\n",
      "Training set accuracy 0.15106000006198883\n",
      "Test set accuracy 0.15529999136924744\n",
      "-------------------------------\n",
      "263/1000, 0.01 sec, 3.15 sec\n",
      "Training set loss 4.42940616607666\n",
      "Training set accuracy 0.204599991440773\n",
      "Test set accuracy 0.21490000188350677\n",
      "-------------------------------\n",
      "264/1000, 0.01 sec, 3.16 sec\n",
      "Training set loss 4.288406848907471\n",
      "Training set accuracy 0.14285999536514282\n",
      "Test set accuracy 0.14710000157356262\n",
      "-------------------------------\n",
      "265/1000, 0.01 sec, 3.18 sec\n",
      "Training set loss 4.903032302856445\n",
      "Training set accuracy 0.20503999292850494\n",
      "Test set accuracy 0.21439999341964722\n",
      "-------------------------------\n",
      "266/1000, 0.01 sec, 3.19 sec\n",
      "Training set loss 4.28902530670166\n",
      "Training set accuracy 0.14649999141693115\n",
      "Test set accuracy 0.14899998903274536\n",
      "-------------------------------\n",
      "267/1000, 0.01 sec, 3.20 sec\n",
      "Training set loss 4.7283196449279785\n",
      "Training set accuracy 0.20712000131607056\n",
      "Test set accuracy 0.21689999103546143\n",
      "-------------------------------\n",
      "268/1000, 0.01 sec, 3.21 sec\n",
      "Training set loss 4.094633102416992\n",
      "Training set accuracy 0.15398000180721283\n",
      "Test set accuracy 0.15729999542236328\n",
      "-------------------------------\n",
      "269/1000, 0.01 sec, 3.22 sec\n",
      "Training set loss 4.313354015350342\n",
      "Training set accuracy 0.2079399973154068\n",
      "Test set accuracy 0.21629999577999115\n",
      "-------------------------------\n",
      "270/1000, 0.01 sec, 3.23 sec\n",
      "Training set loss 3.9120426177978516\n",
      "Training set accuracy 0.16099999845027924\n",
      "Test set accuracy 0.16910000145435333\n",
      "-------------------------------\n",
      "271/1000, 0.01 sec, 3.24 sec\n",
      "Training set loss 4.01491117477417\n",
      "Training set accuracy 0.208639994263649\n",
      "Test set accuracy 0.21789999306201935\n",
      "-------------------------------\n",
      "272/1000, 0.01 sec, 3.26 sec\n",
      "Training set loss 3.777587413787842\n",
      "Training set accuracy 0.1662999987602234\n",
      "Test set accuracy 0.17299999296665192\n",
      "-------------------------------\n",
      "273/1000, 0.01 sec, 3.27 sec\n",
      "Training set loss 3.829831838607788\n",
      "Training set accuracy 0.20697999000549316\n",
      "Test set accuracy 0.21969999372959137\n",
      "-------------------------------\n",
      "274/1000, 0.01 sec, 3.28 sec\n",
      "Training set loss 3.687329053878784\n",
      "Training set accuracy 0.1695600003004074\n",
      "Test set accuracy 0.17639999091625214\n",
      "-------------------------------\n",
      "275/1000, 0.01 sec, 3.29 sec\n",
      "Training set loss 3.717437982559204\n",
      "Training set accuracy 0.20495998859405518\n",
      "Test set accuracy 0.22119998931884766\n",
      "-------------------------------\n",
      "276/1000, 0.01 sec, 3.30 sec\n",
      "Training set loss 3.625969171524048\n",
      "Training set accuracy 0.17215999960899353\n",
      "Test set accuracy 0.1793999969959259\n",
      "-------------------------------\n",
      "277/1000, 0.01 sec, 3.32 sec\n",
      "Training set loss 3.646404266357422\n",
      "Training set accuracy 0.20418000221252441\n",
      "Test set accuracy 0.22219999134540558\n",
      "-------------------------------\n",
      "278/1000, 0.01 sec, 3.33 sec\n",
      "Training set loss 3.5838167667388916\n",
      "Training set accuracy 0.17377999424934387\n",
      "Test set accuracy 0.18019999563694\n",
      "-------------------------------\n",
      "279/1000, 0.01 sec, 3.34 sec\n",
      "Training set loss 3.5994436740875244\n",
      "Training set accuracy 0.20362000167369843\n",
      "Test set accuracy 0.2223999947309494\n",
      "-------------------------------\n",
      "280/1000, 0.01 sec, 3.35 sec\n",
      "Training set loss 3.5534188747406006\n",
      "Training set accuracy 0.17517998814582825\n",
      "Test set accuracy 0.18159998953342438\n",
      "-------------------------------\n",
      "281/1000, 0.01 sec, 3.36 sec\n",
      "Training set loss 3.5675971508026123\n",
      "Training set accuracy 0.20419999957084656\n",
      "Test set accuracy 0.2232999950647354\n",
      "-------------------------------\n",
      "282/1000, 0.01 sec, 3.37 sec\n",
      "Training set loss 3.5323634147644043\n",
      "Training set accuracy 0.17625999450683594\n",
      "Test set accuracy 0.18369999527931213\n",
      "-------------------------------\n",
      "283/1000, 0.01 sec, 3.38 sec\n",
      "Training set loss 3.5464491844177246\n",
      "Training set accuracy 0.2043999880552292\n",
      "Test set accuracy 0.22259999811649323\n",
      "-------------------------------\n",
      "284/1000, 0.01 sec, 3.40 sec\n",
      "Training set loss 3.517775535583496\n",
      "Training set accuracy 0.17688000202178955\n",
      "Test set accuracy 0.18369999527931213\n",
      "-------------------------------\n",
      "285/1000, 0.01 sec, 3.41 sec\n",
      "Training set loss 3.5331802368164062\n",
      "Training set accuracy 0.20532000064849854\n",
      "Test set accuracy 0.22360000014305115\n",
      "-------------------------------\n",
      "286/1000, 0.01 sec, 3.42 sec\n",
      "Training set loss 3.5080389976501465\n",
      "Training set accuracy 0.1763399988412857\n",
      "Test set accuracy 0.18369999527931213\n",
      "-------------------------------\n",
      "287/1000, 0.01 sec, 3.43 sec\n",
      "Training set loss 3.5256149768829346\n",
      "Training set accuracy 0.205719992518425\n",
      "Test set accuracy 0.2247999906539917\n",
      "-------------------------------\n",
      "288/1000, 0.01 sec, 3.44 sec\n",
      "Training set loss 3.5028018951416016\n",
      "Training set accuracy 0.1718599945306778\n",
      "Test set accuracy 0.18079999089241028\n",
      "-------------------------------\n",
      "289/1000, 0.01 sec, 3.45 sec\n",
      "Training set loss 3.523378610610962\n",
      "Training set accuracy 0.20615999400615692\n",
      "Test set accuracy 0.22519999742507935\n",
      "-------------------------------\n",
      "290/1000, 0.01 sec, 3.46 sec\n",
      "Training set loss 3.5017707347869873\n",
      "Training set accuracy 0.20216000080108643\n",
      "Test set accuracy 0.2126999944448471\n",
      "-------------------------------\n",
      "291/1000, 0.01 sec, 3.48 sec\n",
      "Training set loss 3.526553153991699\n",
      "Training set accuracy 0.20545999705791473\n",
      "Test set accuracy 0.22339999675750732\n",
      "-------------------------------\n",
      "292/1000, 0.01 sec, 3.49 sec\n",
      "Training set loss 3.5047786235809326\n",
      "Training set accuracy 0.20427998900413513\n",
      "Test set accuracy 0.21449999511241913\n",
      "-------------------------------\n",
      "293/1000, 0.01 sec, 3.50 sec\n",
      "Training set loss 3.535306215286255\n",
      "Training set accuracy 0.2129799872636795\n",
      "Test set accuracy 0.23149999976158142\n",
      "-------------------------------\n",
      "294/1000, 0.01 sec, 3.51 sec\n",
      "Training set loss 3.5120105743408203\n",
      "Training set accuracy 0.20588000118732452\n",
      "Test set accuracy 0.21619999408721924\n",
      "-------------------------------\n",
      "295/1000, 0.01 sec, 3.52 sec\n",
      "Training set loss 3.5492937564849854\n",
      "Training set accuracy 0.21541999280452728\n",
      "Test set accuracy 0.23199999332427979\n",
      "-------------------------------\n",
      "296/1000, 0.01 sec, 3.53 sec\n",
      "Training set loss 3.5228254795074463\n",
      "Training set accuracy 0.20805999636650085\n",
      "Test set accuracy 0.2166999876499176\n",
      "-------------------------------\n",
      "297/1000, 0.01 sec, 3.55 sec\n",
      "Training set loss 3.5683979988098145\n",
      "Training set accuracy 0.2171199917793274\n",
      "Test set accuracy 0.23269999027252197\n",
      "-------------------------------\n",
      "298/1000, 0.01 sec, 3.56 sec\n",
      "Training set loss 3.536134958267212\n",
      "Training set accuracy 0.20904000103473663\n",
      "Test set accuracy 0.2175000011920929\n",
      "-------------------------------\n",
      "299/1000, 0.01 sec, 3.57 sec\n",
      "Training set loss 3.5904104709625244\n",
      "Training set accuracy 0.21852000057697296\n",
      "Test set accuracy 0.23389999568462372\n",
      "-------------------------------\n",
      "300/1000, 0.01 sec, 3.58 sec\n",
      "Training set loss 3.550405502319336\n",
      "Training set accuracy 0.21001999080181122\n",
      "Test set accuracy 0.21879999339580536\n",
      "-------------------------------\n",
      "301/1000, 0.01 sec, 3.60 sec\n",
      "Training set loss 3.6129510402679443\n",
      "Training set accuracy 0.21957999467849731\n",
      "Test set accuracy 0.23359999060630798\n",
      "-------------------------------\n",
      "302/1000, 0.01 sec, 3.61 sec\n",
      "Training set loss 3.563211441040039\n",
      "Training set accuracy 0.2099199891090393\n",
      "Test set accuracy 0.2190999984741211\n",
      "-------------------------------\n",
      "303/1000, 0.01 sec, 3.62 sec\n",
      "Training set loss 3.631903648376465\n",
      "Training set accuracy 0.22067999839782715\n",
      "Test set accuracy 0.23389999568462372\n",
      "-------------------------------\n",
      "304/1000, 0.01 sec, 3.63 sec\n",
      "Training set loss 3.5714049339294434\n",
      "Training set accuracy 0.2101999968290329\n",
      "Test set accuracy 0.21849998831748962\n",
      "-------------------------------\n",
      "305/1000, 0.01 sec, 3.64 sec\n",
      "Training set loss 3.6435954570770264\n",
      "Training set accuracy 0.22169999778270721\n",
      "Test set accuracy 0.23439998924732208\n",
      "-------------------------------\n",
      "306/1000, 0.01 sec, 3.66 sec\n",
      "Training set loss 3.5735385417938232\n",
      "Training set accuracy 0.21130000054836273\n",
      "Test set accuracy 0.219200000166893\n",
      "-------------------------------\n",
      "307/1000, 0.01 sec, 3.67 sec\n",
      "Training set loss 3.6461336612701416\n",
      "Training set accuracy 0.22303999960422516\n",
      "Test set accuracy 0.23549999296665192\n",
      "-------------------------------\n",
      "308/1000, 0.01 sec, 3.68 sec\n",
      "Training set loss 3.569199323654175\n",
      "Training set accuracy 0.21227999031543732\n",
      "Test set accuracy 0.22049999237060547\n",
      "-------------------------------\n",
      "309/1000, 0.01 sec, 3.69 sec\n",
      "Training set loss 3.6396336555480957\n",
      "Training set accuracy 0.22429999709129333\n",
      "Test set accuracy 0.23809999227523804\n",
      "-------------------------------\n",
      "310/1000, 0.01 sec, 3.70 sec\n",
      "Training set loss 3.559511184692383\n",
      "Training set accuracy 0.21323999762535095\n",
      "Test set accuracy 0.22179999947547913\n",
      "-------------------------------\n",
      "311/1000, 0.01 sec, 3.71 sec\n",
      "Training set loss 3.6256823539733887\n",
      "Training set accuracy 0.22567999362945557\n",
      "Test set accuracy 0.23879998922348022\n",
      "-------------------------------\n",
      "312/1000, 0.01 sec, 3.72 sec\n",
      "Training set loss 3.5451231002807617\n",
      "Training set accuracy 0.21455998718738556\n",
      "Test set accuracy 0.22179999947547913\n",
      "-------------------------------\n",
      "313/1000, 0.01 sec, 3.74 sec\n",
      "Training set loss 3.6059112548828125\n",
      "Training set accuracy 0.2266799956560135\n",
      "Test set accuracy 0.23969998955726624\n",
      "-------------------------------\n",
      "314/1000, 0.01 sec, 3.75 sec\n",
      "Training set loss 3.527655601501465\n",
      "Training set accuracy 0.215379998087883\n",
      "Test set accuracy 0.2247999906539917\n",
      "-------------------------------\n",
      "315/1000, 0.01 sec, 3.76 sec\n",
      "Training set loss 3.5828592777252197\n",
      "Training set accuracy 0.2284799963235855\n",
      "Test set accuracy 0.24059998989105225\n",
      "-------------------------------\n",
      "316/1000, 0.01 sec, 3.77 sec\n",
      "Training set loss 3.509395122528076\n",
      "Training set accuracy 0.21728000044822693\n",
      "Test set accuracy 0.22659999132156372\n",
      "-------------------------------\n",
      "317/1000, 0.01 sec, 3.79 sec\n",
      "Training set loss 3.5600249767303467\n",
      "Training set accuracy 0.22939999401569366\n",
      "Test set accuracy 0.241799995303154\n",
      "-------------------------------\n",
      "318/1000, 0.01 sec, 3.80 sec\n",
      "Training set loss 3.491895914077759\n",
      "Training set accuracy 0.21835999190807343\n",
      "Test set accuracy 0.22769999504089355\n",
      "-------------------------------\n",
      "319/1000, 0.01 sec, 3.81 sec\n",
      "Training set loss 3.5387821197509766\n",
      "Training set accuracy 0.23083999752998352\n",
      "Test set accuracy 0.24399998784065247\n",
      "-------------------------------\n",
      "320/1000, 0.01 sec, 3.82 sec\n",
      "Training set loss 3.4757843017578125\n",
      "Training set accuracy 0.21965999901294708\n",
      "Test set accuracy 0.22839999198913574\n",
      "-------------------------------\n",
      "321/1000, 0.01 sec, 3.83 sec\n",
      "Training set loss 3.51993989944458\n",
      "Training set accuracy 0.2326200008392334\n",
      "Test set accuracy 0.24560000002384186\n",
      "-------------------------------\n",
      "322/1000, 0.01 sec, 3.84 sec\n",
      "Training set loss 3.4610886573791504\n",
      "Training set accuracy 0.220660001039505\n",
      "Test set accuracy 0.22949999570846558\n",
      "-------------------------------\n",
      "323/1000, 0.01 sec, 3.85 sec\n",
      "Training set loss 3.5034210681915283\n",
      "Training set accuracy 0.23447999358177185\n",
      "Test set accuracy 0.24709999561309814\n",
      "-------------------------------\n",
      "324/1000, 0.01 sec, 3.87 sec\n",
      "Training set loss 3.4482223987579346\n",
      "Training set accuracy 0.2219799906015396\n",
      "Test set accuracy 0.2312999963760376\n",
      "-------------------------------\n",
      "325/1000, 0.01 sec, 3.88 sec\n",
      "Training set loss 3.4895410537719727\n",
      "Training set accuracy 0.23559999465942383\n",
      "Test set accuracy 0.24859999120235443\n",
      "-------------------------------\n",
      "326/1000, 0.01 sec, 3.89 sec\n",
      "Training set loss 3.4370737075805664\n",
      "Training set accuracy 0.22297999262809753\n",
      "Test set accuracy 0.2320999950170517\n",
      "-------------------------------\n",
      "327/1000, 0.01 sec, 3.90 sec\n",
      "Training set loss 3.4781811237335205\n",
      "Training set accuracy 0.23736000061035156\n",
      "Test set accuracy 0.24979999661445618\n",
      "-------------------------------\n",
      "328/1000, 0.01 sec, 3.91 sec\n",
      "Training set loss 3.4275050163269043\n",
      "Training set accuracy 0.22411999106407166\n",
      "Test set accuracy 0.23309999704360962\n",
      "-------------------------------\n",
      "329/1000, 0.01 sec, 3.93 sec\n",
      "Training set loss 3.469064950942993\n",
      "Training set accuracy 0.23827999830245972\n",
      "Test set accuracy 0.2505999803543091\n",
      "-------------------------------\n",
      "330/1000, 0.01 sec, 3.94 sec\n",
      "Training set loss 3.4197211265563965\n",
      "Training set accuracy 0.22485999763011932\n",
      "Test set accuracy 0.23409999907016754\n",
      "-------------------------------\n",
      "331/1000, 0.01 sec, 3.95 sec\n",
      "Training set loss 3.4621927738189697\n",
      "Training set accuracy 0.2392599880695343\n",
      "Test set accuracy 0.25099998712539673\n",
      "-------------------------------\n",
      "332/1000, 0.01 sec, 3.96 sec\n",
      "Training set loss 3.4133167266845703\n",
      "Training set accuracy 0.22527998685836792\n",
      "Test set accuracy 0.2352999895811081\n",
      "-------------------------------\n",
      "333/1000, 0.01 sec, 3.97 sec\n",
      "Training set loss 3.4571783542633057\n",
      "Training set accuracy 0.240339994430542\n",
      "Test set accuracy 0.25220000743865967\n",
      "-------------------------------\n",
      "334/1000, 0.01 sec, 3.99 sec\n",
      "Training set loss 3.4082305431365967\n",
      "Training set accuracy 0.22571998834609985\n",
      "Test set accuracy 0.23629999160766602\n",
      "-------------------------------\n",
      "335/1000, 0.01 sec, 4.00 sec\n",
      "Training set loss 3.4537811279296875\n",
      "Training set accuracy 0.2416199892759323\n",
      "Test set accuracy 0.2540999948978424\n",
      "-------------------------------\n",
      "336/1000, 0.01 sec, 4.01 sec\n",
      "Training set loss 3.4042205810546875\n",
      "Training set accuracy 0.22633999586105347\n",
      "Test set accuracy 0.2377999871969223\n",
      "-------------------------------\n",
      "337/1000, 0.01 sec, 4.02 sec\n",
      "Training set loss 3.451792001724243\n",
      "Training set accuracy 0.2424199879169464\n",
      "Test set accuracy 0.25679999589920044\n",
      "-------------------------------\n",
      "338/1000, 0.01 sec, 4.03 sec\n",
      "Training set loss 3.4009854793548584\n",
      "Training set accuracy 0.22679999470710754\n",
      "Test set accuracy 0.23899999260902405\n",
      "-------------------------------\n",
      "339/1000, 0.01 sec, 4.04 sec\n",
      "Training set loss 3.450770854949951\n",
      "Training set accuracy 0.24337999522686005\n",
      "Test set accuracy 0.25669997930526733\n",
      "-------------------------------\n",
      "340/1000, 0.01 sec, 4.05 sec\n",
      "Training set loss 3.3980185985565186\n",
      "Training set accuracy 0.22739998996257782\n",
      "Test set accuracy 0.2386999875307083\n",
      "-------------------------------\n",
      "341/1000, 0.01 sec, 4.06 sec\n",
      "Training set loss 3.4499454498291016\n",
      "Training set accuracy 0.2445399910211563\n",
      "Test set accuracy 0.2572999894618988\n",
      "-------------------------------\n",
      "342/1000, 0.01 sec, 4.07 sec\n",
      "Training set loss 3.3947598934173584\n",
      "Training set accuracy 0.22797998785972595\n",
      "Test set accuracy 0.23999999463558197\n",
      "-------------------------------\n",
      "343/1000, 0.01 sec, 4.09 sec\n",
      "Training set loss 3.4481277465820312\n",
      "Training set accuracy 0.24529999494552612\n",
      "Test set accuracy 0.25849997997283936\n",
      "-------------------------------\n",
      "344/1000, 0.01 sec, 4.10 sec\n",
      "Training set loss 3.390925645828247\n",
      "Training set accuracy 0.22817999124526978\n",
      "Test set accuracy 0.23969998955726624\n",
      "-------------------------------\n",
      "345/1000, 0.01 sec, 4.11 sec\n",
      "Training set loss 3.445472478866577\n",
      "Training set accuracy 0.2463199943304062\n",
      "Test set accuracy 0.25939998030662537\n",
      "-------------------------------\n",
      "346/1000, 0.01 sec, 4.12 sec\n",
      "Training set loss 3.3860485553741455\n",
      "Training set accuracy 0.22885999083518982\n",
      "Test set accuracy 0.23989999294281006\n",
      "-------------------------------\n",
      "347/1000, 0.01 sec, 4.13 sec\n",
      "Training set loss 3.4413504600524902\n",
      "Training set accuracy 0.24689999222755432\n",
      "Test set accuracy 0.2603999972343445\n",
      "-------------------------------\n",
      "348/1000, 0.01 sec, 4.14 sec\n",
      "Training set loss 3.3802907466888428\n",
      "Training set accuracy 0.22924000024795532\n",
      "Test set accuracy 0.2401999980211258\n",
      "-------------------------------\n",
      "349/1000, 0.01 sec, 4.15 sec\n",
      "Training set loss 3.436030626296997\n",
      "Training set accuracy 0.2481599897146225\n",
      "Test set accuracy 0.26159998774528503\n",
      "-------------------------------\n",
      "350/1000, 0.01 sec, 4.17 sec\n",
      "Training set loss 3.373154401779175\n",
      "Training set accuracy 0.2299799919128418\n",
      "Test set accuracy 0.24069999158382416\n",
      "-------------------------------\n",
      "351/1000, 0.01 sec, 4.18 sec\n",
      "Training set loss 3.4284555912017822\n",
      "Training set accuracy 0.24925999343395233\n",
      "Test set accuracy 0.26330000162124634\n",
      "-------------------------------\n",
      "352/1000, 0.01 sec, 4.19 sec\n",
      "Training set loss 3.3649017810821533\n",
      "Training set accuracy 0.23051999509334564\n",
      "Test set accuracy 0.24139998853206635\n",
      "-------------------------------\n",
      "353/1000, 0.01 sec, 4.20 sec\n",
      "Training set loss 3.419468641281128\n",
      "Training set accuracy 0.25\n",
      "Test set accuracy 0.26429998874664307\n",
      "-------------------------------\n",
      "354/1000, 0.01 sec, 4.21 sec\n",
      "Training set loss 3.3555655479431152\n",
      "Training set accuracy 0.23111999034881592\n",
      "Test set accuracy 0.2418999969959259\n",
      "-------------------------------\n",
      "355/1000, 0.01 sec, 4.23 sec\n",
      "Training set loss 3.409259796142578\n",
      "Training set accuracy 0.2507599890232086\n",
      "Test set accuracy 0.2651999890804291\n",
      "-------------------------------\n",
      "356/1000, 0.01 sec, 4.24 sec\n",
      "Training set loss 3.3456406593322754\n",
      "Training set accuracy 0.23191998898983002\n",
      "Test set accuracy 0.24309998750686646\n",
      "-------------------------------\n",
      "357/1000, 0.01 sec, 4.25 sec\n",
      "Training set loss 3.398441791534424\n",
      "Training set accuracy 0.2518399953842163\n",
      "Test set accuracy 0.26739999651908875\n",
      "-------------------------------\n",
      "358/1000, 0.01 sec, 4.26 sec\n",
      "Training set loss 3.335425615310669\n",
      "Training set accuracy 0.23271998763084412\n",
      "Test set accuracy 0.24399998784065247\n",
      "-------------------------------\n",
      "359/1000, 0.01 sec, 4.27 sec\n",
      "Training set loss 3.3871798515319824\n",
      "Training set accuracy 0.2529999911785126\n",
      "Test set accuracy 0.26829999685287476\n",
      "-------------------------------\n",
      "360/1000, 0.01 sec, 4.28 sec\n",
      "Training set loss 3.325021266937256\n",
      "Training set accuracy 0.23345999419689178\n",
      "Test set accuracy 0.24479998648166656\n",
      "-------------------------------\n",
      "361/1000, 0.01 sec, 4.30 sec\n",
      "Training set loss 3.376368761062622\n",
      "Training set accuracy 0.2542800009250641\n",
      "Test set accuracy 0.26919999718666077\n",
      "-------------------------------\n",
      "362/1000, 0.01 sec, 4.31 sec\n",
      "Training set loss 3.3149781227111816\n",
      "Training set accuracy 0.23405998945236206\n",
      "Test set accuracy 0.2451999932527542\n",
      "-------------------------------\n",
      "363/1000, 0.01 sec, 4.32 sec\n",
      "Training set loss 3.36584734916687\n",
      "Training set accuracy 0.2555599808692932\n",
      "Test set accuracy 0.2696000039577484\n",
      "-------------------------------\n",
      "364/1000, 0.01 sec, 4.33 sec\n",
      "Training set loss 3.3053805828094482\n",
      "Training set accuracy 0.234499990940094\n",
      "Test set accuracy 0.24560000002384186\n",
      "-------------------------------\n",
      "365/1000, 0.01 sec, 4.34 sec\n",
      "Training set loss 3.355956792831421\n",
      "Training set accuracy 0.2563599944114685\n",
      "Test set accuracy 0.27140000462532043\n",
      "-------------------------------\n",
      "366/1000, 0.01 sec, 4.35 sec\n",
      "Training set loss 3.2961881160736084\n",
      "Training set accuracy 0.23533999919891357\n",
      "Test set accuracy 0.2451999932527542\n",
      "-------------------------------\n",
      "367/1000, 0.01 sec, 4.36 sec\n",
      "Training set loss 3.3469924926757812\n",
      "Training set accuracy 0.2576200067996979\n",
      "Test set accuracy 0.272599995136261\n",
      "-------------------------------\n",
      "368/1000, 0.01 sec, 4.37 sec\n",
      "Training set loss 3.2877862453460693\n",
      "Training set accuracy 0.23589999973773956\n",
      "Test set accuracy 0.24609999358654022\n",
      "-------------------------------\n",
      "369/1000, 0.01 sec, 4.39 sec\n",
      "Training set loss 3.338892936706543\n",
      "Training set accuracy 0.25854000449180603\n",
      "Test set accuracy 0.2728999853134155\n",
      "-------------------------------\n",
      "370/1000, 0.01 sec, 4.40 sec\n",
      "Training set loss 3.2795748710632324\n",
      "Training set accuracy 0.2363400012254715\n",
      "Test set accuracy 0.24719999730587006\n",
      "-------------------------------\n",
      "371/1000, 0.01 sec, 4.41 sec\n",
      "Training set loss 3.331207036972046\n",
      "Training set accuracy 0.2592200040817261\n",
      "Test set accuracy 0.27410000562667847\n",
      "-------------------------------\n",
      "372/1000, 0.01 sec, 4.42 sec\n",
      "Training set loss 3.2715506553649902\n",
      "Training set accuracy 0.2369999885559082\n",
      "Test set accuracy 0.24799999594688416\n",
      "-------------------------------\n",
      "373/1000, 0.01 sec, 4.43 sec\n",
      "Training set loss 3.323549270629883\n",
      "Training set accuracy 0.2603600025177002\n",
      "Test set accuracy 0.275299996137619\n",
      "-------------------------------\n",
      "374/1000, 0.01 sec, 4.44 sec\n",
      "Training set loss 3.2634379863739014\n",
      "Training set accuracy 0.23745998740196228\n",
      "Test set accuracy 0.24849998950958252\n",
      "-------------------------------\n",
      "375/1000, 0.01 sec, 4.45 sec\n",
      "Training set loss 3.3160014152526855\n",
      "Training set accuracy 0.2612200081348419\n",
      "Test set accuracy 0.2766000032424927\n",
      "-------------------------------\n",
      "376/1000, 0.01 sec, 4.47 sec\n",
      "Training set loss 3.2552907466888428\n",
      "Training set accuracy 0.23813998699188232\n",
      "Test set accuracy 0.24939998984336853\n",
      "-------------------------------\n",
      "377/1000, 0.01 sec, 4.48 sec\n",
      "Training set loss 3.3080079555511475\n",
      "Training set accuracy 0.26197999715805054\n",
      "Test set accuracy 0.2766999900341034\n",
      "-------------------------------\n",
      "378/1000, 0.01 sec, 4.49 sec\n",
      "Training set loss 3.247060775756836\n",
      "Training set accuracy 0.23879998922348022\n",
      "Test set accuracy 0.24929998815059662\n",
      "-------------------------------\n",
      "379/1000, 0.01 sec, 4.50 sec\n",
      "Training set loss 3.299863576889038\n",
      "Training set accuracy 0.26308000087738037\n",
      "Test set accuracy 0.27809998393058777\n",
      "-------------------------------\n",
      "380/1000, 0.01 sec, 4.52 sec\n",
      "Training set loss 3.238584280014038\n",
      "Training set accuracy 0.23945999145507812\n",
      "Test set accuracy 0.24969999492168427\n",
      "-------------------------------\n",
      "381/1000, 0.01 sec, 4.53 sec\n",
      "Training set loss 3.29183292388916\n",
      "Training set accuracy 0.26401999592781067\n",
      "Test set accuracy 0.27879998087882996\n",
      "-------------------------------\n",
      "382/1000, 0.01 sec, 4.54 sec\n",
      "Training set loss 3.230027437210083\n",
      "Training set accuracy 0.24003998935222626\n",
      "Test set accuracy 0.2502000033855438\n",
      "-------------------------------\n",
      "383/1000, 0.01 sec, 4.55 sec\n",
      "Training set loss 3.2833807468414307\n",
      "Training set accuracy 0.26462000608444214\n",
      "Test set accuracy 0.2793999910354614\n",
      "-------------------------------\n",
      "384/1000, 0.01 sec, 4.56 sec\n",
      "Training set loss 3.221143960952759\n",
      "Training set accuracy 0.2406199872493744\n",
      "Test set accuracy 0.2505999803543091\n",
      "-------------------------------\n",
      "385/1000, 0.01 sec, 4.57 sec\n",
      "Training set loss 3.2745018005371094\n",
      "Training set accuracy 0.2654999792575836\n",
      "Test set accuracy 0.27950000762939453\n",
      "-------------------------------\n",
      "386/1000, 0.01 sec, 4.59 sec\n",
      "Training set loss 3.2120673656463623\n",
      "Training set accuracy 0.24143999814987183\n",
      "Test set accuracy 0.2514999806880951\n",
      "-------------------------------\n",
      "387/1000, 0.01 sec, 4.60 sec\n",
      "Training set loss 3.265333652496338\n",
      "Training set accuracy 0.2664400041103363\n",
      "Test set accuracy 0.2806999981403351\n",
      "-------------------------------\n",
      "388/1000, 0.01 sec, 4.61 sec\n",
      "Training set loss 3.2026925086975098\n",
      "Training set accuracy 0.242139995098114\n",
      "Test set accuracy 0.25209999084472656\n",
      "-------------------------------\n",
      "389/1000, 0.01 sec, 4.62 sec\n",
      "Training set loss 3.256044387817383\n",
      "Training set accuracy 0.2673400044441223\n",
      "Test set accuracy 0.28189998865127563\n",
      "-------------------------------\n",
      "390/1000, 0.01 sec, 4.63 sec\n",
      "Training set loss 3.193338632583618\n",
      "Training set accuracy 0.2431199997663498\n",
      "Test set accuracy 0.25270000100135803\n",
      "-------------------------------\n",
      "391/1000, 0.01 sec, 4.64 sec\n",
      "Training set loss 3.246664524078369\n",
      "Training set accuracy 0.26829999685287476\n",
      "Test set accuracy 0.282399982213974\n",
      "-------------------------------\n",
      "392/1000, 0.01 sec, 4.66 sec\n",
      "Training set loss 3.183868646621704\n",
      "Training set accuracy 0.24409998953342438\n",
      "Test set accuracy 0.2532999813556671\n",
      "-------------------------------\n",
      "393/1000, 0.01 sec, 4.67 sec\n",
      "Training set loss 3.236992359161377\n",
      "Training set accuracy 0.26914000511169434\n",
      "Test set accuracy 0.28380000591278076\n",
      "-------------------------------\n",
      "394/1000, 0.01 sec, 4.68 sec\n",
      "Training set loss 3.1743764877319336\n",
      "Training set accuracy 0.24503999948501587\n",
      "Test set accuracy 0.2538999915122986\n",
      "-------------------------------\n",
      "395/1000, 0.01 sec, 4.69 sec\n",
      "Training set loss 3.2272298336029053\n",
      "Training set accuracy 0.2702399790287018\n",
      "Test set accuracy 0.28540000319480896\n",
      "-------------------------------\n",
      "396/1000, 0.01 sec, 4.70 sec\n",
      "Training set loss 3.1647679805755615\n",
      "Training set accuracy 0.24569998681545258\n",
      "Test set accuracy 0.25439998507499695\n",
      "-------------------------------\n",
      "397/1000, 0.01 sec, 4.71 sec\n",
      "Training set loss 3.2174618244171143\n",
      "Training set accuracy 0.2716600000858307\n",
      "Test set accuracy 0.2865000069141388\n",
      "-------------------------------\n",
      "398/1000, 0.01 sec, 4.72 sec\n",
      "Training set loss 3.1549911499023438\n",
      "Training set accuracy 0.24653999507427216\n",
      "Test set accuracy 0.2556999921798706\n",
      "-------------------------------\n",
      "399/1000, 0.01 sec, 4.74 sec\n",
      "Training set loss 3.2075090408325195\n",
      "Training set accuracy 0.272599995136261\n",
      "Test set accuracy 0.288100004196167\n",
      "-------------------------------\n",
      "400/1000, 0.01 sec, 4.75 sec\n",
      "Training set loss 3.1449930667877197\n",
      "Training set accuracy 0.24706000089645386\n",
      "Test set accuracy 0.2565999925136566\n",
      "-------------------------------\n",
      "401/1000, 0.01 sec, 4.76 sec\n",
      "Training set loss 3.197368621826172\n",
      "Training set accuracy 0.27320000529289246\n",
      "Test set accuracy 0.28849998116493225\n",
      "-------------------------------\n",
      "402/1000, 0.01 sec, 4.77 sec\n",
      "Training set loss 3.134920120239258\n",
      "Training set accuracy 0.24785999953746796\n",
      "Test set accuracy 0.25769999623298645\n",
      "-------------------------------\n",
      "403/1000, 0.01 sec, 4.78 sec\n",
      "Training set loss 3.1868748664855957\n",
      "Training set accuracy 0.27445998787879944\n",
      "Test set accuracy 0.289000004529953\n",
      "-------------------------------\n",
      "404/1000, 0.01 sec, 4.80 sec\n",
      "Training set loss 3.1249873638153076\n",
      "Training set accuracy 0.24845999479293823\n",
      "Test set accuracy 0.25839999318122864\n",
      "-------------------------------\n",
      "405/1000, 0.01 sec, 4.81 sec\n",
      "Training set loss 3.1768150329589844\n",
      "Training set accuracy 0.27526000142097473\n",
      "Test set accuracy 0.29009997844696045\n",
      "-------------------------------\n",
      "406/1000, 0.02 sec, 4.83 sec\n",
      "Training set loss 3.114698648452759\n",
      "Training set accuracy 0.24913999438285828\n",
      "Test set accuracy 0.2590999901294708\n",
      "-------------------------------\n",
      "407/1000, 0.02 sec, 4.84 sec\n",
      "Training set loss 3.166170597076416\n",
      "Training set accuracy 0.27636000514030457\n",
      "Test set accuracy 0.29009997844696045\n",
      "-------------------------------\n",
      "408/1000, 0.02 sec, 4.86 sec\n",
      "Training set loss 3.104119300842285\n",
      "Training set accuracy 0.2498999983072281\n",
      "Test set accuracy 0.26010000705718994\n",
      "-------------------------------\n",
      "409/1000, 0.01 sec, 4.87 sec\n",
      "Training set loss 3.1552536487579346\n",
      "Training set accuracy 0.27699998021125793\n",
      "Test set accuracy 0.29089999198913574\n",
      "-------------------------------\n",
      "410/1000, 0.01 sec, 4.89 sec\n",
      "Training set loss 3.0928308963775635\n",
      "Training set accuracy 0.25081998109817505\n",
      "Test set accuracy 0.2604999840259552\n",
      "-------------------------------\n",
      "411/1000, 0.01 sec, 4.90 sec\n",
      "Training set loss 3.1436843872070312\n",
      "Training set accuracy 0.2776399850845337\n",
      "Test set accuracy 0.2912999987602234\n",
      "-------------------------------\n",
      "412/1000, 0.01 sec, 4.91 sec\n",
      "Training set loss 3.0814404487609863\n",
      "Training set accuracy 0.2518399953842163\n",
      "Test set accuracy 0.2606000006198883\n",
      "-------------------------------\n",
      "413/1000, 0.01 sec, 4.92 sec\n",
      "Training set loss 3.13149094581604\n",
      "Training set accuracy 0.2782999873161316\n",
      "Test set accuracy 0.2911999821662903\n",
      "-------------------------------\n",
      "414/1000, 0.01 sec, 4.94 sec\n",
      "Training set loss 3.069584846496582\n",
      "Training set accuracy 0.2524600028991699\n",
      "Test set accuracy 0.26159998774528503\n",
      "-------------------------------\n",
      "415/1000, 0.01 sec, 4.95 sec\n",
      "Training set loss 3.1188042163848877\n",
      "Training set accuracy 0.2793799936771393\n",
      "Test set accuracy 0.2921999990940094\n",
      "-------------------------------\n",
      "416/1000, 0.01 sec, 4.96 sec\n",
      "Training set loss 3.057481527328491\n",
      "Training set accuracy 0.25356000661849976\n",
      "Test set accuracy 0.2619999945163727\n",
      "-------------------------------\n",
      "417/1000, 0.01 sec, 4.97 sec\n",
      "Training set loss 3.105745553970337\n",
      "Training set accuracy 0.2802799940109253\n",
      "Test set accuracy 0.2929999828338623\n",
      "-------------------------------\n",
      "418/1000, 0.01 sec, 4.99 sec\n",
      "Training set loss 3.0448718070983887\n",
      "Training set accuracy 0.254720002412796\n",
      "Test set accuracy 0.2637999951839447\n",
      "-------------------------------\n",
      "419/1000, 0.01 sec, 5.00 sec\n",
      "Training set loss 3.0922181606292725\n",
      "Training set accuracy 0.281360000371933\n",
      "Test set accuracy 0.29420000314712524\n",
      "-------------------------------\n",
      "420/1000, 0.01 sec, 5.01 sec\n",
      "Training set loss 3.0322718620300293\n",
      "Training set accuracy 0.2556000053882599\n",
      "Test set accuracy 0.2653000056743622\n",
      "-------------------------------\n",
      "421/1000, 0.01 sec, 5.02 sec\n",
      "Training set loss 3.0787415504455566\n",
      "Training set accuracy 0.282260000705719\n",
      "Test set accuracy 0.2955999970436096\n",
      "-------------------------------\n",
      "422/1000, 0.01 sec, 5.04 sec\n",
      "Training set loss 3.019561767578125\n",
      "Training set accuracy 0.2565000057220459\n",
      "Test set accuracy 0.26659998297691345\n",
      "-------------------------------\n",
      "423/1000, 0.01 sec, 5.05 sec\n",
      "Training set loss 3.065389394760132\n",
      "Training set accuracy 0.2830999791622162\n",
      "Test set accuracy 0.29649999737739563\n",
      "-------------------------------\n",
      "424/1000, 0.01 sec, 5.06 sec\n",
      "Training set loss 3.006988286972046\n",
      "Training set accuracy 0.2574799954891205\n",
      "Test set accuracy 0.26749998331069946\n",
      "-------------------------------\n",
      "425/1000, 0.01 sec, 5.07 sec\n",
      "Training set loss 3.0522143840789795\n",
      "Training set accuracy 0.28411999344825745\n",
      "Test set accuracy 0.29829999804496765\n",
      "-------------------------------\n",
      "426/1000, 0.01 sec, 5.09 sec\n",
      "Training set loss 2.9945664405822754\n",
      "Training set accuracy 0.25812000036239624\n",
      "Test set accuracy 0.26819998025894165\n",
      "-------------------------------\n",
      "427/1000, 0.01 sec, 5.10 sec\n",
      "Training set loss 3.039499521255493\n",
      "Training set accuracy 0.28474000096321106\n",
      "Test set accuracy 0.299699991941452\n",
      "-------------------------------\n",
      "428/1000, 0.01 sec, 5.11 sec\n",
      "Training set loss 2.9826431274414062\n",
      "Training set accuracy 0.25895997881889343\n",
      "Test set accuracy 0.2683999836444855\n",
      "-------------------------------\n",
      "429/1000, 0.01 sec, 5.12 sec\n",
      "Training set loss 3.027278423309326\n",
      "Training set accuracy 0.2856999933719635\n",
      "Test set accuracy 0.30069997906684875\n",
      "-------------------------------\n",
      "430/1000, 0.01 sec, 5.14 sec\n",
      "Training set loss 2.971222162246704\n",
      "Training set accuracy 0.25992000102996826\n",
      "Test set accuracy 0.26919999718666077\n",
      "-------------------------------\n",
      "431/1000, 0.01 sec, 5.15 sec\n",
      "Training set loss 3.015918016433716\n",
      "Training set accuracy 0.28633999824523926\n",
      "Test set accuracy 0.3021000027656555\n",
      "-------------------------------\n",
      "432/1000, 0.01 sec, 5.16 sec\n",
      "Training set loss 2.9603235721588135\n",
      "Training set accuracy 0.2606000006198883\n",
      "Test set accuracy 0.2694999873638153\n",
      "-------------------------------\n",
      "433/1000, 0.01 sec, 5.17 sec\n",
      "Training set loss 3.005655527114868\n",
      "Training set accuracy 0.28711998462677\n",
      "Test set accuracy 0.3037000000476837\n",
      "-------------------------------\n",
      "434/1000, 0.01 sec, 5.18 sec\n",
      "Training set loss 2.950315475463867\n",
      "Training set accuracy 0.26113998889923096\n",
      "Test set accuracy 0.2700999975204468\n",
      "-------------------------------\n",
      "435/1000, 0.01 sec, 5.20 sec\n",
      "Training set loss 2.99643874168396\n",
      "Training set accuracy 0.28779998421669006\n",
      "Test set accuracy 0.3034999966621399\n",
      "-------------------------------\n",
      "436/1000, 0.01 sec, 5.21 sec\n",
      "Training set loss 2.941366195678711\n",
      "Training set accuracy 0.2612600028514862\n",
      "Test set accuracy 0.2710999846458435\n",
      "-------------------------------\n",
      "437/1000, 0.01 sec, 5.22 sec\n",
      "Training set loss 2.9889392852783203\n",
      "Training set accuracy 0.2887600064277649\n",
      "Test set accuracy 0.30309998989105225\n",
      "-------------------------------\n",
      "438/1000, 0.01 sec, 5.24 sec\n",
      "Training set loss 2.9333674907684326\n",
      "Training set accuracy 0.2614399790763855\n",
      "Test set accuracy 0.27160000801086426\n",
      "-------------------------------\n",
      "439/1000, 0.01 sec, 5.25 sec\n",
      "Training set loss 2.9822564125061035\n",
      "Training set accuracy 0.28953999280929565\n",
      "Test set accuracy 0.3046000003814697\n",
      "-------------------------------\n",
      "440/1000, 0.01 sec, 5.26 sec\n",
      "Training set loss 2.925600528717041\n",
      "Training set accuracy 0.261680006980896\n",
      "Test set accuracy 0.2708999812602997\n",
      "-------------------------------\n",
      "441/1000, 0.01 sec, 5.27 sec\n",
      "Training set loss 2.975860118865967\n",
      "Training set accuracy 0.29038000106811523\n",
      "Test set accuracy 0.30559998750686646\n",
      "-------------------------------\n",
      "442/1000, 0.01 sec, 5.29 sec\n",
      "Training set loss 2.918013095855713\n",
      "Training set accuracy 0.2620999813079834\n",
      "Test set accuracy 0.27079999446868896\n",
      "-------------------------------\n",
      "443/1000, 0.01 sec, 5.30 sec\n",
      "Training set loss 2.9697110652923584\n",
      "Training set accuracy 0.29128000140190125\n",
      "Test set accuracy 0.3060999810695648\n",
      "-------------------------------\n",
      "444/1000, 0.01 sec, 5.31 sec\n",
      "Training set loss 2.910243034362793\n",
      "Training set accuracy 0.26190000772476196\n",
      "Test set accuracy 0.2717999815940857\n",
      "-------------------------------\n",
      "445/1000, 0.01 sec, 5.32 sec\n",
      "Training set loss 2.96307373046875\n",
      "Training set accuracy 0.29203999042510986\n",
      "Test set accuracy 0.30629998445510864\n",
      "-------------------------------\n",
      "446/1000, 0.01 sec, 5.33 sec\n",
      "Training set loss 2.9027998447418213\n",
      "Training set accuracy 0.26211997866630554\n",
      "Test set accuracy 0.271699994802475\n",
      "-------------------------------\n",
      "447/1000, 0.01 sec, 5.35 sec\n",
      "Training set loss 2.9567086696624756\n",
      "Training set accuracy 0.2926599979400635\n",
      "Test set accuracy 0.30640000104904175\n",
      "-------------------------------\n",
      "448/1000, 0.01 sec, 5.36 sec\n",
      "Training set loss 2.894937038421631\n",
      "Training set accuracy 0.2622799873352051\n",
      "Test set accuracy 0.27230000495910645\n",
      "-------------------------------\n",
      "449/1000, 0.01 sec, 5.37 sec\n",
      "Training set loss 2.950120449066162\n",
      "Training set accuracy 0.2933399975299835\n",
      "Test set accuracy 0.3075000047683716\n",
      "-------------------------------\n",
      "450/1000, 0.01 sec, 5.38 sec\n",
      "Training set loss 2.8872289657592773\n",
      "Training set accuracy 0.2625199854373932\n",
      "Test set accuracy 0.2726999819278717\n",
      "-------------------------------\n",
      "451/1000, 0.01 sec, 5.40 sec\n",
      "Training set loss 2.943580150604248\n",
      "Training set accuracy 0.2938999831676483\n",
      "Test set accuracy 0.30889999866485596\n",
      "-------------------------------\n",
      "452/1000, 0.01 sec, 5.41 sec\n",
      "Training set loss 2.8794965744018555\n",
      "Training set accuracy 0.26260000467300415\n",
      "Test set accuracy 0.27300000190734863\n",
      "-------------------------------\n",
      "453/1000, 0.01 sec, 5.42 sec\n",
      "Training set loss 2.9367918968200684\n",
      "Training set accuracy 0.294979989528656\n",
      "Test set accuracy 0.30979999899864197\n",
      "-------------------------------\n",
      "454/1000, 0.01 sec, 5.43 sec\n",
      "Training set loss 2.871408700942993\n",
      "Training set accuracy 0.26319998502731323\n",
      "Test set accuracy 0.27379998564720154\n",
      "-------------------------------\n",
      "455/1000, 0.01 sec, 5.45 sec\n",
      "Training set loss 2.9292571544647217\n",
      "Training set accuracy 0.2960599958896637\n",
      "Test set accuracy 0.31049999594688416\n",
      "-------------------------------\n",
      "456/1000, 0.01 sec, 5.46 sec\n",
      "Training set loss 2.863121747970581\n",
      "Training set accuracy 0.2632800042629242\n",
      "Test set accuracy 0.2733999788761139\n",
      "-------------------------------\n",
      "457/1000, 0.01 sec, 5.47 sec\n",
      "Training set loss 2.921718120574951\n",
      "Training set accuracy 0.29635998606681824\n",
      "Test set accuracy 0.3105999827384949\n",
      "-------------------------------\n",
      "458/1000, 0.01 sec, 5.48 sec\n",
      "Training set loss 2.8544764518737793\n",
      "Training set accuracy 0.2633199989795685\n",
      "Test set accuracy 0.27379998564720154\n",
      "-------------------------------\n",
      "459/1000, 0.01 sec, 5.50 sec\n",
      "Training set loss 2.913675546646118\n",
      "Training set accuracy 0.29712000489234924\n",
      "Test set accuracy 0.3105999827384949\n",
      "-------------------------------\n",
      "460/1000, 0.01 sec, 5.51 sec\n",
      "Training set loss 2.8456242084503174\n",
      "Training set accuracy 0.2636999785900116\n",
      "Test set accuracy 0.2736999988555908\n",
      "-------------------------------\n",
      "461/1000, 0.01 sec, 5.52 sec\n",
      "Training set loss 2.9047961235046387\n",
      "Training set accuracy 0.29782000184059143\n",
      "Test set accuracy 0.3105999827384949\n",
      "-------------------------------\n",
      "462/1000, 0.01 sec, 5.53 sec\n",
      "Training set loss 2.8362374305725098\n",
      "Training set accuracy 0.2640799880027771\n",
      "Test set accuracy 0.27390000224113464\n",
      "-------------------------------\n",
      "463/1000, 0.01 sec, 5.55 sec\n",
      "Training set loss 2.895721912384033\n",
      "Training set accuracy 0.2990399897098541\n",
      "Test set accuracy 0.3116999864578247\n",
      "-------------------------------\n",
      "464/1000, 0.01 sec, 5.56 sec\n",
      "Training set loss 2.82665753364563\n",
      "Training set accuracy 0.26440000534057617\n",
      "Test set accuracy 0.27410000562667847\n",
      "-------------------------------\n",
      "465/1000, 0.01 sec, 5.57 sec\n",
      "Training set loss 2.8859517574310303\n",
      "Training set accuracy 0.2999599874019623\n",
      "Test set accuracy 0.3130999803543091\n",
      "-------------------------------\n",
      "466/1000, 0.01 sec, 5.58 sec\n",
      "Training set loss 2.8162331581115723\n",
      "Training set accuracy 0.2648800015449524\n",
      "Test set accuracy 0.27480000257492065\n",
      "-------------------------------\n",
      "467/1000, 0.01 sec, 5.60 sec\n",
      "Training set loss 2.8753113746643066\n",
      "Training set accuracy 0.3003999888896942\n",
      "Test set accuracy 0.31360000371932983\n",
      "-------------------------------\n",
      "468/1000, 0.01 sec, 5.61 sec\n",
      "Training set loss 2.8055849075317383\n",
      "Training set accuracy 0.26517999172210693\n",
      "Test set accuracy 0.27480000257492065\n",
      "-------------------------------\n",
      "469/1000, 0.01 sec, 5.62 sec\n",
      "Training set loss 2.864529848098755\n",
      "Training set accuracy 0.30121999979019165\n",
      "Test set accuracy 0.3140999972820282\n",
      "-------------------------------\n",
      "470/1000, 0.01 sec, 5.63 sec\n",
      "Training set loss 2.7953402996063232\n",
      "Training set accuracy 0.2654399871826172\n",
      "Test set accuracy 0.27570000290870667\n",
      "-------------------------------\n",
      "471/1000, 0.01 sec, 5.65 sec\n",
      "Training set loss 2.8544561862945557\n",
      "Training set accuracy 0.30171999335289\n",
      "Test set accuracy 0.3150999844074249\n",
      "-------------------------------\n",
      "472/1000, 0.01 sec, 5.66 sec\n",
      "Training set loss 2.7854464054107666\n",
      "Training set accuracy 0.2659600079059601\n",
      "Test set accuracy 0.27570000290870667\n",
      "-------------------------------\n",
      "473/1000, 0.01 sec, 5.67 sec\n",
      "Training set loss 2.8443808555603027\n",
      "Training set accuracy 0.3019999861717224\n",
      "Test set accuracy 0.31630000472068787\n",
      "-------------------------------\n",
      "474/1000, 0.01 sec, 5.68 sec\n",
      "Training set loss 2.7755067348480225\n",
      "Training set accuracy 0.2664400041103363\n",
      "Test set accuracy 0.27649998664855957\n",
      "-------------------------------\n",
      "475/1000, 0.01 sec, 5.70 sec\n",
      "Training set loss 2.834425926208496\n",
      "Training set accuracy 0.3027999997138977\n",
      "Test set accuracy 0.31700000166893005\n",
      "-------------------------------\n",
      "476/1000, 0.01 sec, 5.71 sec\n",
      "Training set loss 2.7659099102020264\n",
      "Training set accuracy 0.26688000559806824\n",
      "Test set accuracy 0.27639999985694885\n",
      "-------------------------------\n",
      "477/1000, 0.01 sec, 5.72 sec\n",
      "Training set loss 2.825129508972168\n",
      "Training set accuracy 0.30375999212265015\n",
      "Test set accuracy 0.3181000053882599\n",
      "-------------------------------\n",
      "478/1000, 0.01 sec, 5.73 sec\n",
      "Training set loss 2.7565853595733643\n",
      "Training set accuracy 0.2672399878501892\n",
      "Test set accuracy 0.2760999798774719\n",
      "-------------------------------\n",
      "479/1000, 0.01 sec, 5.74 sec\n",
      "Training set loss 2.8164587020874023\n",
      "Training set accuracy 0.3044799864292145\n",
      "Test set accuracy 0.31949999928474426\n",
      "-------------------------------\n",
      "480/1000, 0.01 sec, 5.76 sec\n",
      "Training set loss 2.747469663619995\n",
      "Training set accuracy 0.2676199972629547\n",
      "Test set accuracy 0.27639999985694885\n",
      "-------------------------------\n",
      "481/1000, 0.01 sec, 5.77 sec\n",
      "Training set loss 2.807823896408081\n",
      "Training set accuracy 0.30535998940467834\n",
      "Test set accuracy 0.32029998302459717\n",
      "-------------------------------\n",
      "482/1000, 0.01 sec, 5.78 sec\n",
      "Training set loss 2.738593101501465\n",
      "Training set accuracy 0.2674199938774109\n",
      "Test set accuracy 0.2766999900341034\n",
      "-------------------------------\n",
      "483/1000, 0.01 sec, 5.79 sec\n",
      "Training set loss 2.7992749214172363\n",
      "Training set accuracy 0.3059599995613098\n",
      "Test set accuracy 0.320499986410141\n",
      "-------------------------------\n",
      "484/1000, 0.01 sec, 5.81 sec\n",
      "Training set loss 2.7296128273010254\n",
      "Training set accuracy 0.2678000032901764\n",
      "Test set accuracy 0.27709999680519104\n",
      "-------------------------------\n",
      "485/1000, 0.01 sec, 5.82 sec\n",
      "Training set loss 2.790349245071411\n",
      "Training set accuracy 0.30687999725341797\n",
      "Test set accuracy 0.3220999836921692\n",
      "-------------------------------\n",
      "486/1000, 0.01 sec, 5.83 sec\n",
      "Training set loss 2.7201881408691406\n",
      "Training set accuracy 0.26805999875068665\n",
      "Test set accuracy 0.27699998021125793\n",
      "-------------------------------\n",
      "487/1000, 0.01 sec, 5.84 sec\n",
      "Training set loss 2.7810230255126953\n",
      "Training set accuracy 0.30757999420166016\n",
      "Test set accuracy 0.32179999351501465\n",
      "-------------------------------\n",
      "488/1000, 0.01 sec, 5.86 sec\n",
      "Training set loss 2.710216760635376\n",
      "Training set accuracy 0.2681399881839752\n",
      "Test set accuracy 0.2768000066280365\n",
      "-------------------------------\n",
      "489/1000, 0.01 sec, 5.87 sec\n",
      "Training set loss 2.771144390106201\n",
      "Training set accuracy 0.3079400062561035\n",
      "Test set accuracy 0.322299987077713\n",
      "-------------------------------\n",
      "490/1000, 0.01 sec, 5.88 sec\n",
      "Training set loss 2.700443744659424\n",
      "Training set accuracy 0.2682799994945526\n",
      "Test set accuracy 0.27699998021125793\n",
      "-------------------------------\n",
      "491/1000, 0.01 sec, 5.89 sec\n",
      "Training set loss 2.761582136154175\n",
      "Training set accuracy 0.30889999866485596\n",
      "Test set accuracy 0.3228999972343445\n",
      "-------------------------------\n",
      "492/1000, 0.01 sec, 5.91 sec\n",
      "Training set loss 2.6901772022247314\n",
      "Training set accuracy 0.26829999685287476\n",
      "Test set accuracy 0.2773999869823456\n",
      "-------------------------------\n",
      "493/1000, 0.01 sec, 5.92 sec\n",
      "Training set loss 2.75079083442688\n",
      "Training set accuracy 0.30959999561309814\n",
      "Test set accuracy 0.32330000400543213\n",
      "-------------------------------\n",
      "494/1000, 0.01 sec, 5.93 sec\n",
      "Training set loss 2.67936372756958\n",
      "Training set accuracy 0.26872000098228455\n",
      "Test set accuracy 0.2775999903678894\n",
      "-------------------------------\n",
      "495/1000, 0.01 sec, 5.94 sec\n",
      "Training set loss 2.739556074142456\n",
      "Training set accuracy 0.3104400038719177\n",
      "Test set accuracy 0.32349997758865356\n",
      "-------------------------------\n",
      "496/1000, 0.01 sec, 5.95 sec\n",
      "Training set loss 2.668678045272827\n",
      "Training set accuracy 0.268779993057251\n",
      "Test set accuracy 0.2784000039100647\n",
      "-------------------------------\n",
      "497/1000, 0.01 sec, 5.97 sec\n",
      "Training set loss 2.7288084030151367\n",
      "Training set accuracy 0.3110399842262268\n",
      "Test set accuracy 0.3229999840259552\n",
      "-------------------------------\n",
      "498/1000, 0.01 sec, 5.98 sec\n",
      "Training set loss 2.6577436923980713\n",
      "Training set accuracy 0.26872000098228455\n",
      "Test set accuracy 0.27889999747276306\n",
      "-------------------------------\n",
      "499/1000, 0.01 sec, 5.99 sec\n",
      "Training set loss 2.7172887325286865\n",
      "Training set accuracy 0.31161999702453613\n",
      "Test set accuracy 0.32409998774528503\n",
      "-------------------------------\n",
      "500/1000, 0.01 sec, 6.00 sec\n",
      "Training set loss 2.6469480991363525\n",
      "Training set accuracy 0.26914000511169434\n",
      "Test set accuracy 0.2793000042438507\n",
      "-------------------------------\n",
      "501/1000, 0.01 sec, 6.02 sec\n",
      "Training set loss 2.7065460681915283\n",
      "Training set accuracy 0.3123199939727783\n",
      "Test set accuracy 0.3240000009536743\n",
      "-------------------------------\n",
      "502/1000, 0.01 sec, 6.03 sec\n",
      "Training set loss 2.6368868350982666\n",
      "Training set accuracy 0.26975998282432556\n",
      "Test set accuracy 0.27879998087882996\n",
      "-------------------------------\n",
      "503/1000, 0.01 sec, 6.04 sec\n",
      "Training set loss 2.6965086460113525\n",
      "Training set accuracy 0.3128199875354767\n",
      "Test set accuracy 0.3243999779224396\n",
      "-------------------------------\n",
      "504/1000, 0.01 sec, 6.05 sec\n",
      "Training set loss 2.6269643306732178\n",
      "Training set accuracy 0.26985999941825867\n",
      "Test set accuracy 0.27889999747276306\n",
      "-------------------------------\n",
      "505/1000, 0.01 sec, 6.07 sec\n",
      "Training set loss 2.6867964267730713\n",
      "Training set accuracy 0.31345999240875244\n",
      "Test set accuracy 0.3261999785900116\n",
      "-------------------------------\n",
      "506/1000, 0.01 sec, 6.08 sec\n",
      "Training set loss 2.617560625076294\n",
      "Training set accuracy 0.2699599862098694\n",
      "Test set accuracy 0.2793999910354614\n",
      "-------------------------------\n",
      "507/1000, 0.01 sec, 6.09 sec\n",
      "Training set loss 2.67738938331604\n",
      "Training set accuracy 0.31404000520706177\n",
      "Test set accuracy 0.32739999890327454\n",
      "-------------------------------\n",
      "508/1000, 0.01 sec, 6.10 sec\n",
      "Training set loss 2.6079230308532715\n",
      "Training set accuracy 0.2702600061893463\n",
      "Test set accuracy 0.2800000011920929\n",
      "-------------------------------\n",
      "509/1000, 0.01 sec, 6.12 sec\n",
      "Training set loss 2.6678977012634277\n",
      "Training set accuracy 0.3143799901008606\n",
      "Test set accuracy 0.32690000534057617\n",
      "-------------------------------\n",
      "510/1000, 0.01 sec, 6.13 sec\n",
      "Training set loss 2.598564624786377\n",
      "Training set accuracy 0.2704800069332123\n",
      "Test set accuracy 0.27950000762939453\n",
      "-------------------------------\n",
      "511/1000, 0.01 sec, 6.14 sec\n",
      "Training set loss 2.6586873531341553\n",
      "Training set accuracy 0.3150999844074249\n",
      "Test set accuracy 0.3269999921321869\n",
      "-------------------------------\n",
      "512/1000, 0.01 sec, 6.15 sec\n",
      "Training set loss 2.5888733863830566\n",
      "Training set accuracy 0.27059999108314514\n",
      "Test set accuracy 0.27959999442100525\n",
      "-------------------------------\n",
      "513/1000, 0.01 sec, 6.16 sec\n",
      "Training set loss 2.648984670639038\n",
      "Training set accuracy 0.31547999382019043\n",
      "Test set accuracy 0.3278000056743622\n",
      "-------------------------------\n",
      "514/1000, 0.01 sec, 6.18 sec\n",
      "Training set loss 2.5788955688476562\n",
      "Training set accuracy 0.2710999846458435\n",
      "Test set accuracy 0.27959999442100525\n",
      "-------------------------------\n",
      "515/1000, 0.01 sec, 6.19 sec\n",
      "Training set loss 2.638739824295044\n",
      "Training set accuracy 0.31632000207901\n",
      "Test set accuracy 0.3278000056743622\n",
      "-------------------------------\n",
      "516/1000, 0.01 sec, 6.20 sec\n",
      "Training set loss 2.56886625289917\n",
      "Training set accuracy 0.27135998010635376\n",
      "Test set accuracy 0.2798999845981598\n",
      "-------------------------------\n",
      "517/1000, 0.01 sec, 6.21 sec\n",
      "Training set loss 2.6286299228668213\n",
      "Training set accuracy 0.3164999783039093\n",
      "Test set accuracy 0.3287999927997589\n",
      "-------------------------------\n",
      "518/1000, 0.01 sec, 6.23 sec\n",
      "Training set loss 2.5587449073791504\n",
      "Training set accuracy 0.27195999026298523\n",
      "Test set accuracy 0.2793000042438507\n",
      "-------------------------------\n",
      "519/1000, 0.01 sec, 6.24 sec\n",
      "Training set loss 2.6181490421295166\n",
      "Training set accuracy 0.31713998317718506\n",
      "Test set accuracy 0.3292999863624573\n",
      "-------------------------------\n",
      "520/1000, 0.01 sec, 6.25 sec\n",
      "Training set loss 2.5485551357269287\n",
      "Training set accuracy 0.27230000495910645\n",
      "Test set accuracy 0.2793000042438507\n",
      "-------------------------------\n",
      "521/1000, 0.01 sec, 6.27 sec\n",
      "Training set loss 2.6073925495147705\n",
      "Training set accuracy 0.3174999952316284\n",
      "Test set accuracy 0.32989999651908875\n",
      "-------------------------------\n",
      "522/1000, 0.01 sec, 6.28 sec\n",
      "Training set loss 2.5382378101348877\n",
      "Training set accuracy 0.2726999819278717\n",
      "Test set accuracy 0.27950000762939453\n",
      "-------------------------------\n",
      "523/1000, 0.01 sec, 6.29 sec\n",
      "Training set loss 2.596651554107666\n",
      "Training set accuracy 0.3182999789714813\n",
      "Test set accuracy 0.33009999990463257\n",
      "-------------------------------\n",
      "524/1000, 0.01 sec, 6.30 sec\n",
      "Training set loss 2.5276453495025635\n",
      "Training set accuracy 0.2726999819278717\n",
      "Test set accuracy 0.27959999442100525\n",
      "-------------------------------\n",
      "525/1000, 0.01 sec, 6.32 sec\n",
      "Training set loss 2.5855398178100586\n",
      "Training set accuracy 0.3190000057220459\n",
      "Test set accuracy 0.3312000036239624\n",
      "-------------------------------\n",
      "526/1000, 0.01 sec, 6.33 sec\n",
      "Training set loss 2.517216444015503\n",
      "Training set accuracy 0.2733999788761139\n",
      "Test set accuracy 0.27959999442100525\n",
      "-------------------------------\n",
      "527/1000, 0.01 sec, 6.34 sec\n",
      "Training set loss 2.57466721534729\n",
      "Training set accuracy 0.3197399973869324\n",
      "Test set accuracy 0.3310999870300293\n",
      "-------------------------------\n",
      "528/1000, 0.01 sec, 6.35 sec\n",
      "Training set loss 2.5069468021392822\n",
      "Training set accuracy 0.2733999788761139\n",
      "Test set accuracy 0.28029999136924744\n",
      "-------------------------------\n",
      "529/1000, 0.01 sec, 6.37 sec\n",
      "Training set loss 2.5643506050109863\n",
      "Training set accuracy 0.32043999433517456\n",
      "Test set accuracy 0.33169999718666077\n",
      "-------------------------------\n",
      "530/1000, 0.01 sec, 6.38 sec\n",
      "Training set loss 2.4969637393951416\n",
      "Training set accuracy 0.2735999822616577\n",
      "Test set accuracy 0.2800999879837036\n",
      "-------------------------------\n",
      "531/1000, 0.01 sec, 6.39 sec\n",
      "Training set loss 2.5541045665740967\n",
      "Training set accuracy 0.3213199973106384\n",
      "Test set accuracy 0.3317999839782715\n",
      "-------------------------------\n",
      "532/1000, 0.01 sec, 6.40 sec\n",
      "Training set loss 2.486959457397461\n",
      "Training set accuracy 0.2739599943161011\n",
      "Test set accuracy 0.28049999475479126\n",
      "-------------------------------\n",
      "533/1000, 0.01 sec, 6.42 sec\n",
      "Training set loss 2.5438714027404785\n",
      "Training set accuracy 0.3218199908733368\n",
      "Test set accuracy 0.33219999074935913\n",
      "-------------------------------\n",
      "534/1000, 0.01 sec, 6.43 sec\n",
      "Training set loss 2.477064609527588\n",
      "Training set accuracy 0.274399995803833\n",
      "Test set accuracy 0.2800999879837036\n",
      "-------------------------------\n",
      "535/1000, 0.01 sec, 6.44 sec\n",
      "Training set loss 2.533674478530884\n",
      "Training set accuracy 0.32249999046325684\n",
      "Test set accuracy 0.33329999446868896\n",
      "-------------------------------\n",
      "536/1000, 0.01 sec, 6.45 sec\n",
      "Training set loss 2.467101573944092\n",
      "Training set accuracy 0.2748199999332428\n",
      "Test set accuracy 0.2798999845981598\n",
      "-------------------------------\n",
      "537/1000, 0.01 sec, 6.47 sec\n",
      "Training set loss 2.523362398147583\n",
      "Training set accuracy 0.3233200013637543\n",
      "Test set accuracy 0.3333999812602997\n",
      "-------------------------------\n",
      "538/1000, 0.01 sec, 6.48 sec\n",
      "Training set loss 2.456967353820801\n",
      "Training set accuracy 0.2754400074481964\n",
      "Test set accuracy 0.2800000011920929\n",
      "-------------------------------\n",
      "539/1000, 0.01 sec, 6.49 sec\n",
      "Training set loss 2.5123724937438965\n",
      "Training set accuracy 0.32363998889923096\n",
      "Test set accuracy 0.33480000495910645\n",
      "-------------------------------\n",
      "540/1000, 0.01 sec, 6.51 sec\n",
      "Training set loss 2.446420192718506\n",
      "Training set accuracy 0.27636000514030457\n",
      "Test set accuracy 0.28139999508857727\n",
      "-------------------------------\n",
      "541/1000, 0.01 sec, 6.52 sec\n",
      "Training set loss 2.5008580684661865\n",
      "Training set accuracy 0.3244999945163727\n",
      "Test set accuracy 0.3343999981880188\n",
      "-------------------------------\n",
      "542/1000, 0.01 sec, 6.53 sec\n",
      "Training set loss 2.435811758041382\n",
      "Training set accuracy 0.27709999680519104\n",
      "Test set accuracy 0.2822999954223633\n",
      "-------------------------------\n",
      "543/1000, 0.01 sec, 6.54 sec\n",
      "Training set loss 2.489323616027832\n",
      "Training set accuracy 0.3249000012874603\n",
      "Test set accuracy 0.3349999785423279\n",
      "-------------------------------\n",
      "544/1000, 0.01 sec, 6.55 sec\n",
      "Training set loss 2.4252915382385254\n",
      "Training set accuracy 0.27765998244285583\n",
      "Test set accuracy 0.2830999791622162\n",
      "-------------------------------\n",
      "545/1000, 0.01 sec, 6.56 sec\n",
      "Training set loss 2.4781107902526855\n",
      "Training set accuracy 0.3258799910545349\n",
      "Test set accuracy 0.3358999788761139\n",
      "-------------------------------\n",
      "546/1000, 0.01 sec, 6.58 sec\n",
      "Training set loss 2.414680004119873\n",
      "Training set accuracy 0.2788199782371521\n",
      "Test set accuracy 0.2856000065803528\n",
      "-------------------------------\n",
      "547/1000, 0.01 sec, 6.59 sec\n",
      "Training set loss 2.4667630195617676\n",
      "Training set accuracy 0.32670000195503235\n",
      "Test set accuracy 0.33709999918937683\n",
      "-------------------------------\n",
      "548/1000, 0.01 sec, 6.60 sec\n",
      "Training set loss 2.4040989875793457\n",
      "Training set accuracy 0.280379980802536\n",
      "Test set accuracy 0.28779998421669006\n",
      "-------------------------------\n",
      "549/1000, 0.01 sec, 6.61 sec\n",
      "Training set loss 2.4559173583984375\n",
      "Training set accuracy 0.3277199864387512\n",
      "Test set accuracy 0.3385999798774719\n",
      "-------------------------------\n",
      "550/1000, 0.01 sec, 6.63 sec\n",
      "Training set loss 2.394041061401367\n",
      "Training set accuracy 0.28237998485565186\n",
      "Test set accuracy 0.28929999470710754\n",
      "-------------------------------\n",
      "551/1000, 0.01 sec, 6.64 sec\n",
      "Training set loss 2.445342540740967\n",
      "Training set accuracy 0.32872000336647034\n",
      "Test set accuracy 0.34039998054504395\n",
      "-------------------------------\n",
      "552/1000, 0.01 sec, 6.65 sec\n",
      "Training set loss 2.3843681812286377\n",
      "Training set accuracy 0.2846599817276001\n",
      "Test set accuracy 0.29099997878074646\n",
      "-------------------------------\n",
      "553/1000, 0.01 sec, 6.66 sec\n",
      "Training set loss 2.4356534481048584\n",
      "Training set accuracy 0.3299599885940552\n",
      "Test set accuracy 0.34139999747276306\n",
      "-------------------------------\n",
      "554/1000, 0.01 sec, 6.68 sec\n",
      "Training set loss 2.3749449253082275\n",
      "Training set accuracy 0.28735998272895813\n",
      "Test set accuracy 0.29249998927116394\n",
      "-------------------------------\n",
      "555/1000, 0.01 sec, 6.69 sec\n",
      "Training set loss 2.426164388656616\n",
      "Training set accuracy 0.3315599858760834\n",
      "Test set accuracy 0.3431999981403351\n",
      "-------------------------------\n",
      "556/1000, 0.01 sec, 6.70 sec\n",
      "Training set loss 2.3657898902893066\n",
      "Training set accuracy 0.2904599905014038\n",
      "Test set accuracy 0.2944999933242798\n",
      "-------------------------------\n",
      "557/1000, 0.01 sec, 6.71 sec\n",
      "Training set loss 2.417020320892334\n",
      "Training set accuracy 0.3333599865436554\n",
      "Test set accuracy 0.344899982213974\n",
      "-------------------------------\n",
      "558/1000, 0.01 sec, 6.73 sec\n",
      "Training set loss 2.3567378520965576\n",
      "Training set accuracy 0.2932800054550171\n",
      "Test set accuracy 0.2976999878883362\n",
      "-------------------------------\n",
      "559/1000, 0.01 sec, 6.74 sec\n",
      "Training set loss 2.408034324645996\n",
      "Training set accuracy 0.33587998151779175\n",
      "Test set accuracy 0.3471999764442444\n",
      "-------------------------------\n",
      "560/1000, 0.01 sec, 6.75 sec\n",
      "Training set loss 2.347590208053589\n",
      "Training set accuracy 0.29673999547958374\n",
      "Test set accuracy 0.30149999260902405\n",
      "-------------------------------\n",
      "561/1000, 0.01 sec, 6.76 sec\n",
      "Training set loss 2.3987457752227783\n",
      "Training set accuracy 0.33813998103141785\n",
      "Test set accuracy 0.35019999742507935\n",
      "-------------------------------\n",
      "562/1000, 0.01 sec, 6.77 sec\n",
      "Training set loss 2.3384664058685303\n",
      "Training set accuracy 0.2995399832725525\n",
      "Test set accuracy 0.30489999055862427\n",
      "-------------------------------\n",
      "563/1000, 0.01 sec, 6.79 sec\n",
      "Training set loss 2.3891761302948\n",
      "Training set accuracy 0.34119999408721924\n",
      "Test set accuracy 0.35249999165534973\n",
      "-------------------------------\n",
      "564/1000, 0.01 sec, 6.80 sec\n",
      "Training set loss 2.3288769721984863\n",
      "Training set accuracy 0.3028799891471863\n",
      "Test set accuracy 0.30799999833106995\n",
      "-------------------------------\n",
      "565/1000, 0.01 sec, 6.81 sec\n",
      "Training set loss 2.3790650367736816\n",
      "Training set accuracy 0.3436200022697449\n",
      "Test set accuracy 0.3553999960422516\n",
      "-------------------------------\n",
      "566/1000, 0.01 sec, 6.82 sec\n",
      "Training set loss 2.319293737411499\n",
      "Training set accuracy 0.3062399923801422\n",
      "Test set accuracy 0.3109000027179718\n",
      "-------------------------------\n",
      "567/1000, 0.01 sec, 6.84 sec\n",
      "Training set loss 2.368727684020996\n",
      "Training set accuracy 0.3469599783420563\n",
      "Test set accuracy 0.357699990272522\n",
      "-------------------------------\n",
      "568/1000, 0.01 sec, 6.85 sec\n",
      "Training set loss 2.3095974922180176\n",
      "Training set accuracy 0.309579998254776\n",
      "Test set accuracy 0.31459999084472656\n",
      "-------------------------------\n",
      "569/1000, 0.01 sec, 6.86 sec\n",
      "Training set loss 2.358422040939331\n",
      "Training set accuracy 0.35033997893333435\n",
      "Test set accuracy 0.3610999882221222\n",
      "-------------------------------\n",
      "570/1000, 0.01 sec, 6.87 sec\n",
      "Training set loss 2.3002777099609375\n",
      "Training set accuracy 0.3129599988460541\n",
      "Test set accuracy 0.3174999952316284\n",
      "-------------------------------\n",
      "571/1000, 0.01 sec, 6.88 sec\n",
      "Training set loss 2.3493361473083496\n",
      "Training set accuracy 0.35349997878074646\n",
      "Test set accuracy 0.3650999963283539\n",
      "-------------------------------\n",
      "572/1000, 0.01 sec, 6.90 sec\n",
      "Training set loss 2.2916502952575684\n",
      "Training set accuracy 0.3158800005912781\n",
      "Test set accuracy 0.32179999351501465\n",
      "-------------------------------\n",
      "573/1000, 0.01 sec, 6.91 sec\n",
      "Training set loss 2.3407342433929443\n",
      "Training set accuracy 0.35624000430107117\n",
      "Test set accuracy 0.3682999908924103\n",
      "-------------------------------\n",
      "574/1000, 0.01 sec, 6.92 sec\n",
      "Training set loss 2.2830779552459717\n",
      "Training set accuracy 0.319240003824234\n",
      "Test set accuracy 0.32589998841285706\n",
      "-------------------------------\n",
      "575/1000, 0.01 sec, 6.93 sec\n",
      "Training set loss 2.3318634033203125\n",
      "Training set accuracy 0.35905998945236206\n",
      "Test set accuracy 0.3709000051021576\n",
      "-------------------------------\n",
      "576/1000, 0.01 sec, 6.94 sec\n",
      "Training set loss 2.2740166187286377\n",
      "Training set accuracy 0.322299987077713\n",
      "Test set accuracy 0.32979997992515564\n",
      "-------------------------------\n",
      "577/1000, 0.01 sec, 6.96 sec\n",
      "Training set loss 2.322162628173828\n",
      "Training set accuracy 0.3626999855041504\n",
      "Test set accuracy 0.374099999666214\n",
      "-------------------------------\n",
      "578/1000, 0.01 sec, 6.97 sec\n",
      "Training set loss 2.2648990154266357\n",
      "Training set accuracy 0.3258799910545349\n",
      "Test set accuracy 0.3328999876976013\n",
      "-------------------------------\n",
      "579/1000, 0.01 sec, 6.98 sec\n",
      "Training set loss 2.312361001968384\n",
      "Training set accuracy 0.3657599985599518\n",
      "Test set accuracy 0.3773999810218811\n",
      "-------------------------------\n",
      "580/1000, 0.01 sec, 6.99 sec\n",
      "Training set loss 2.255342483520508\n",
      "Training set accuracy 0.3293199837207794\n",
      "Test set accuracy 0.33640000224113464\n",
      "-------------------------------\n",
      "581/1000, 0.01 sec, 7.00 sec\n",
      "Training set loss 2.302140235900879\n",
      "Training set accuracy 0.36875998973846436\n",
      "Test set accuracy 0.38169997930526733\n",
      "-------------------------------\n",
      "582/1000, 0.01 sec, 7.02 sec\n",
      "Training set loss 2.2454049587249756\n",
      "Training set accuracy 0.3325600028038025\n",
      "Test set accuracy 0.33949998021125793\n",
      "-------------------------------\n",
      "583/1000, 0.01 sec, 7.03 sec\n",
      "Training set loss 2.291658639907837\n",
      "Training set accuracy 0.3721199929714203\n",
      "Test set accuracy 0.3853999972343445\n",
      "-------------------------------\n",
      "584/1000, 0.01 sec, 7.04 sec\n",
      "Training set loss 2.2354013919830322\n",
      "Training set accuracy 0.33535999059677124\n",
      "Test set accuracy 0.3416000008583069\n",
      "-------------------------------\n",
      "585/1000, 0.01 sec, 7.05 sec\n",
      "Training set loss 2.280778169631958\n",
      "Training set accuracy 0.37505999207496643\n",
      "Test set accuracy 0.38929998874664307\n",
      "-------------------------------\n",
      "586/1000, 0.01 sec, 7.06 sec\n",
      "Training set loss 2.225743293762207\n",
      "Training set accuracy 0.33781999349594116\n",
      "Test set accuracy 0.34439998865127563\n",
      "-------------------------------\n",
      "587/1000, 0.01 sec, 7.07 sec\n",
      "Training set loss 2.2703142166137695\n",
      "Training set accuracy 0.37863999605178833\n",
      "Test set accuracy 0.3921999931335449\n",
      "-------------------------------\n",
      "588/1000, 0.01 sec, 7.09 sec\n",
      "Training set loss 2.215913772583008\n",
      "Training set accuracy 0.3403399884700775\n",
      "Test set accuracy 0.346699982881546\n",
      "-------------------------------\n",
      "589/1000, 0.01 sec, 7.10 sec\n",
      "Training set loss 2.260035753250122\n",
      "Training set accuracy 0.3819199800491333\n",
      "Test set accuracy 0.3955000042915344\n",
      "-------------------------------\n",
      "590/1000, 0.01 sec, 7.11 sec\n",
      "Training set loss 2.2059504985809326\n",
      "Training set accuracy 0.3420400023460388\n",
      "Test set accuracy 0.3483999967575073\n",
      "-------------------------------\n",
      "591/1000, 0.01 sec, 7.12 sec\n",
      "Training set loss 2.2496840953826904\n",
      "Training set accuracy 0.3850399851799011\n",
      "Test set accuracy 0.3982999920845032\n",
      "-------------------------------\n",
      "592/1000, 0.01 sec, 7.14 sec\n",
      "Training set loss 2.196601152420044\n",
      "Training set accuracy 0.34411999583244324\n",
      "Test set accuracy 0.35040000081062317\n",
      "-------------------------------\n",
      "593/1000, 0.01 sec, 7.15 sec\n",
      "Training set loss 2.2399251461029053\n",
      "Training set accuracy 0.3871999979019165\n",
      "Test set accuracy 0.4007999897003174\n",
      "-------------------------------\n",
      "594/1000, 0.01 sec, 7.16 sec\n",
      "Training set loss 2.1872315406799316\n",
      "Training set accuracy 0.34627997875213623\n",
      "Test set accuracy 0.3515999913215637\n",
      "-------------------------------\n",
      "595/1000, 0.01 sec, 7.17 sec\n",
      "Training set loss 2.2298879623413086\n",
      "Training set accuracy 0.3895999789237976\n",
      "Test set accuracy 0.4023999869823456\n",
      "-------------------------------\n",
      "596/1000, 0.01 sec, 7.18 sec\n",
      "Training set loss 2.1778757572174072\n",
      "Training set accuracy 0.34849998354911804\n",
      "Test set accuracy 0.35249999165534973\n",
      "-------------------------------\n",
      "597/1000, 0.01 sec, 7.19 sec\n",
      "Training set loss 2.220052480697632\n",
      "Training set accuracy 0.3921400010585785\n",
      "Test set accuracy 0.4032999873161316\n",
      "-------------------------------\n",
      "598/1000, 0.01 sec, 7.20 sec\n",
      "Training set loss 2.1687698364257812\n",
      "Training set accuracy 0.3503199815750122\n",
      "Test set accuracy 0.3546999990940094\n",
      "-------------------------------\n",
      "599/1000, 0.01 sec, 7.22 sec\n",
      "Training set loss 2.210771083831787\n",
      "Training set accuracy 0.3941600024700165\n",
      "Test set accuracy 0.40639999508857727\n",
      "-------------------------------\n",
      "600/1000, 0.01 sec, 7.23 sec\n",
      "Training set loss 2.160254716873169\n",
      "Training set accuracy 0.3520999848842621\n",
      "Test set accuracy 0.3568999767303467\n",
      "-------------------------------\n",
      "601/1000, 0.01 sec, 7.24 sec\n",
      "Training set loss 2.2020764350891113\n",
      "Training set accuracy 0.39593997597694397\n",
      "Test set accuracy 0.40799999237060547\n",
      "-------------------------------\n",
      "602/1000, 0.01 sec, 7.25 sec\n",
      "Training set loss 2.152134656906128\n",
      "Training set accuracy 0.35398000478744507\n",
      "Test set accuracy 0.35819998383522034\n",
      "-------------------------------\n",
      "603/1000, 0.01 sec, 7.26 sec\n",
      "Training set loss 2.193897247314453\n",
      "Training set accuracy 0.3977600038051605\n",
      "Test set accuracy 0.4098999798297882\n",
      "-------------------------------\n",
      "604/1000, 0.01 sec, 7.27 sec\n",
      "Training set loss 2.1443252563476562\n",
      "Training set accuracy 0.3554999828338623\n",
      "Test set accuracy 0.3596999943256378\n",
      "-------------------------------\n",
      "605/1000, 0.01 sec, 7.29 sec\n",
      "Training set loss 2.186243772506714\n",
      "Training set accuracy 0.399399995803833\n",
      "Test set accuracy 0.4108999967575073\n",
      "-------------------------------\n",
      "606/1000, 0.01 sec, 7.30 sec\n",
      "Training set loss 2.136498212814331\n",
      "Training set accuracy 0.35707998275756836\n",
      "Test set accuracy 0.3610000014305115\n",
      "-------------------------------\n",
      "607/1000, 0.01 sec, 7.31 sec\n",
      "Training set loss 2.1781864166259766\n",
      "Training set accuracy 0.4009599983692169\n",
      "Test set accuracy 0.4131999909877777\n",
      "-------------------------------\n",
      "608/1000, 0.01 sec, 7.32 sec\n",
      "Training set loss 2.128586530685425\n",
      "Training set accuracy 0.358379989862442\n",
      "Test set accuracy 0.3628999888896942\n",
      "-------------------------------\n",
      "609/1000, 0.01 sec, 7.33 sec\n",
      "Training set loss 2.169950246810913\n",
      "Training set accuracy 0.40261998772621155\n",
      "Test set accuracy 0.41509997844696045\n",
      "-------------------------------\n",
      "610/1000, 0.01 sec, 7.35 sec\n",
      "Training set loss 2.1203434467315674\n",
      "Training set accuracy 0.3595599830150604\n",
      "Test set accuracy 0.36419999599456787\n",
      "-------------------------------\n",
      "611/1000, 0.01 sec, 7.36 sec\n",
      "Training set loss 2.1611549854278564\n",
      "Training set accuracy 0.40432000160217285\n",
      "Test set accuracy 0.4165000021457672\n",
      "-------------------------------\n",
      "612/1000, 0.01 sec, 7.37 sec\n",
      "Training set loss 2.1119637489318848\n",
      "Training set accuracy 0.36052000522613525\n",
      "Test set accuracy 0.3643999993801117\n",
      "-------------------------------\n",
      "613/1000, 0.01 sec, 7.38 sec\n",
      "Training set loss 2.1524367332458496\n",
      "Training set accuracy 0.40601998567581177\n",
      "Test set accuracy 0.41909998655319214\n",
      "-------------------------------\n",
      "614/1000, 0.01 sec, 7.39 sec\n",
      "Training set loss 2.103830337524414\n",
      "Training set accuracy 0.36159998178482056\n",
      "Test set accuracy 0.3649999797344208\n",
      "-------------------------------\n",
      "615/1000, 0.01 sec, 7.40 sec\n",
      "Training set loss 2.143868923187256\n",
      "Training set accuracy 0.40751999616622925\n",
      "Test set accuracy 0.42010000348091125\n",
      "-------------------------------\n",
      "616/1000, 0.01 sec, 7.41 sec\n",
      "Training set loss 2.095747709274292\n",
      "Training set accuracy 0.3625600039958954\n",
      "Test set accuracy 0.36730000376701355\n",
      "-------------------------------\n",
      "617/1000, 0.01 sec, 7.43 sec\n",
      "Training set loss 2.1353893280029297\n",
      "Training set accuracy 0.4089199900627136\n",
      "Test set accuracy 0.42159998416900635\n",
      "-------------------------------\n",
      "618/1000, 0.01 sec, 7.44 sec\n",
      "Training set loss 2.0875556468963623\n",
      "Training set accuracy 0.3635599911212921\n",
      "Test set accuracy 0.36800000071525574\n",
      "-------------------------------\n",
      "619/1000, 0.01 sec, 7.45 sec\n",
      "Training set loss 2.12684965133667\n",
      "Training set accuracy 0.4105999767780304\n",
      "Test set accuracy 0.42260000109672546\n",
      "-------------------------------\n",
      "620/1000, 0.01 sec, 7.46 sec\n",
      "Training set loss 2.079436779022217\n",
      "Training set accuracy 0.36419999599456787\n",
      "Test set accuracy 0.3691999912261963\n",
      "-------------------------------\n",
      "621/1000, 0.01 sec, 7.47 sec\n",
      "Training set loss 2.118076801300049\n",
      "Training set accuracy 0.4113999903202057\n",
      "Test set accuracy 0.42329999804496765\n",
      "-------------------------------\n",
      "622/1000, 0.01 sec, 7.48 sec\n",
      "Training set loss 2.0711562633514404\n",
      "Training set accuracy 0.36528000235557556\n",
      "Test set accuracy 0.36980000138282776\n",
      "-------------------------------\n",
      "623/1000, 0.01 sec, 7.50 sec\n",
      "Training set loss 2.1089470386505127\n",
      "Training set accuracy 0.4127199947834015\n",
      "Test set accuracy 0.4250999987125397\n",
      "-------------------------------\n",
      "624/1000, 0.01 sec, 7.51 sec\n",
      "Training set loss 2.0626518726348877\n",
      "Training set accuracy 0.36591997742652893\n",
      "Test set accuracy 0.3716000020503998\n",
      "-------------------------------\n",
      "625/1000, 0.01 sec, 7.52 sec\n",
      "Training set loss 2.099613666534424\n",
      "Training set accuracy 0.4132799804210663\n",
      "Test set accuracy 0.42579999566078186\n",
      "-------------------------------\n",
      "626/1000, 0.01 sec, 7.53 sec\n",
      "Training set loss 2.054142475128174\n",
      "Training set accuracy 0.36687999963760376\n",
      "Test set accuracy 0.3718999922275543\n",
      "-------------------------------\n",
      "627/1000, 0.01 sec, 7.54 sec\n",
      "Training set loss 2.0904574394226074\n",
      "Training set accuracy 0.414139986038208\n",
      "Test set accuracy 0.42649999260902405\n",
      "-------------------------------\n",
      "628/1000, 0.01 sec, 7.56 sec\n",
      "Training set loss 2.045835018157959\n",
      "Training set accuracy 0.3676599860191345\n",
      "Test set accuracy 0.3725000023841858\n",
      "-------------------------------\n",
      "629/1000, 0.01 sec, 7.57 sec\n",
      "Training set loss 2.0816807746887207\n",
      "Training set accuracy 0.4153999984264374\n",
      "Test set accuracy 0.4274999797344208\n",
      "-------------------------------\n",
      "630/1000, 0.01 sec, 7.58 sec\n",
      "Training set loss 2.0378406047821045\n",
      "Training set accuracy 0.3685799837112427\n",
      "Test set accuracy 0.3730999827384949\n",
      "-------------------------------\n",
      "631/1000, 0.01 sec, 7.59 sec\n",
      "Training set loss 2.0730156898498535\n",
      "Training set accuracy 0.416159987449646\n",
      "Test set accuracy 0.429099977016449\n",
      "-------------------------------\n",
      "632/1000, 0.01 sec, 7.60 sec\n",
      "Training set loss 2.0300581455230713\n",
      "Training set accuracy 0.36890000104904175\n",
      "Test set accuracy 0.37459999322891235\n",
      "-------------------------------\n",
      "633/1000, 0.01 sec, 7.61 sec\n",
      "Training set loss 2.06491756439209\n",
      "Training set accuracy 0.417059987783432\n",
      "Test set accuracy 0.42969998717308044\n",
      "-------------------------------\n",
      "634/1000, 0.01 sec, 7.63 sec\n",
      "Training set loss 2.022498607635498\n",
      "Training set accuracy 0.3695399761199951\n",
      "Test set accuracy 0.37469998002052307\n",
      "-------------------------------\n",
      "635/1000, 0.01 sec, 7.64 sec\n",
      "Training set loss 2.0567708015441895\n",
      "Training set accuracy 0.41819998621940613\n",
      "Test set accuracy 0.4302999973297119\n",
      "-------------------------------\n",
      "636/1000, 0.01 sec, 7.65 sec\n",
      "Training set loss 2.0148866176605225\n",
      "Training set accuracy 0.37003999948501587\n",
      "Test set accuracy 0.37529999017715454\n",
      "-------------------------------\n",
      "637/1000, 0.01 sec, 7.66 sec\n",
      "Training set loss 2.0487236976623535\n",
      "Training set accuracy 0.4194200038909912\n",
      "Test set accuracy 0.4307999908924103\n",
      "-------------------------------\n",
      "638/1000, 0.01 sec, 7.67 sec\n",
      "Training set loss 2.0073931217193604\n",
      "Training set accuracy 0.3706199824810028\n",
      "Test set accuracy 0.37629997730255127\n",
      "-------------------------------\n",
      "639/1000, 0.01 sec, 7.68 sec\n",
      "Training set loss 2.0410518646240234\n",
      "Training set accuracy 0.4204999804496765\n",
      "Test set accuracy 0.4311999976634979\n",
      "-------------------------------\n",
      "640/1000, 0.01 sec, 7.70 sec\n",
      "Training set loss 2.0000507831573486\n",
      "Training set accuracy 0.3714199960231781\n",
      "Test set accuracy 0.37689998745918274\n",
      "-------------------------------\n",
      "641/1000, 0.01 sec, 7.71 sec\n",
      "Training set loss 2.033432960510254\n",
      "Training set accuracy 0.4215199947357178\n",
      "Test set accuracy 0.4316999912261963\n",
      "-------------------------------\n",
      "642/1000, 0.01 sec, 7.72 sec\n",
      "Training set loss 1.9930192232131958\n",
      "Training set accuracy 0.37195998430252075\n",
      "Test set accuracy 0.3773999810218811\n",
      "-------------------------------\n",
      "643/1000, 0.01 sec, 7.73 sec\n",
      "Training set loss 2.0261991024017334\n",
      "Training set accuracy 0.42236000299453735\n",
      "Test set accuracy 0.43289998173713684\n",
      "-------------------------------\n",
      "644/1000, 0.01 sec, 7.74 sec\n",
      "Training set loss 1.9862761497497559\n",
      "Training set accuracy 0.37219998240470886\n",
      "Test set accuracy 0.37790000438690186\n",
      "-------------------------------\n",
      "645/1000, 0.01 sec, 7.76 sec\n",
      "Training set loss 2.019252061843872\n",
      "Training set accuracy 0.42319998145103455\n",
      "Test set accuracy 0.43299999833106995\n",
      "-------------------------------\n",
      "646/1000, 0.01 sec, 7.77 sec\n",
      "Training set loss 1.9796360731124878\n",
      "Training set accuracy 0.37265998125076294\n",
      "Test set accuracy 0.3781999945640564\n",
      "-------------------------------\n",
      "647/1000, 0.01 sec, 7.78 sec\n",
      "Training set loss 2.012390375137329\n",
      "Training set accuracy 0.42399999499320984\n",
      "Test set accuracy 0.43379998207092285\n",
      "-------------------------------\n",
      "648/1000, 0.01 sec, 7.79 sec\n",
      "Training set loss 1.9729955196380615\n",
      "Training set accuracy 0.3730199933052063\n",
      "Test set accuracy 0.37869998812675476\n",
      "-------------------------------\n",
      "649/1000, 0.01 sec, 7.81 sec\n",
      "Training set loss 2.0055625438690186\n",
      "Training set accuracy 0.4248799979686737\n",
      "Test set accuracy 0.43459999561309814\n",
      "-------------------------------\n",
      "650/1000, 0.01 sec, 7.82 sec\n",
      "Training set loss 1.966601848602295\n",
      "Training set accuracy 0.37355998158454895\n",
      "Test set accuracy 0.3789999783039093\n",
      "-------------------------------\n",
      "651/1000, 0.01 sec, 7.83 sec\n",
      "Training set loss 1.9990415573120117\n",
      "Training set accuracy 0.42555999755859375\n",
      "Test set accuracy 0.43529999256134033\n",
      "-------------------------------\n",
      "652/1000, 0.01 sec, 7.84 sec\n",
      "Training set loss 1.9605098962783813\n",
      "Training set accuracy 0.373879998922348\n",
      "Test set accuracy 0.3791999816894531\n",
      "-------------------------------\n",
      "653/1000, 0.01 sec, 7.85 sec\n",
      "Training set loss 1.992761254310608\n",
      "Training set accuracy 0.42649999260902405\n",
      "Test set accuracy 0.43629997968673706\n",
      "-------------------------------\n",
      "654/1000, 0.01 sec, 7.87 sec\n",
      "Training set loss 1.9544252157211304\n",
      "Training set accuracy 0.3739999830722809\n",
      "Test set accuracy 0.37950000166893005\n",
      "-------------------------------\n",
      "655/1000, 0.01 sec, 7.88 sec\n",
      "Training set loss 1.9866666793823242\n",
      "Training set accuracy 0.42729997634887695\n",
      "Test set accuracy 0.4373999834060669\n",
      "-------------------------------\n",
      "656/1000, 0.01 sec, 7.89 sec\n",
      "Training set loss 1.9485245943069458\n",
      "Training set accuracy 0.37425997853279114\n",
      "Test set accuracy 0.37869998812675476\n",
      "-------------------------------\n",
      "657/1000, 0.01 sec, 7.90 sec\n",
      "Training set loss 1.980538249015808\n",
      "Training set accuracy 0.42791998386383057\n",
      "Test set accuracy 0.43860000371932983\n",
      "-------------------------------\n",
      "658/1000, 0.01 sec, 7.91 sec\n",
      "Training set loss 1.9425244331359863\n",
      "Training set accuracy 0.37435999512672424\n",
      "Test set accuracy 0.3791999816894531\n",
      "-------------------------------\n",
      "659/1000, 0.01 sec, 7.93 sec\n",
      "Training set loss 1.9743270874023438\n",
      "Training set accuracy 0.42857998609542847\n",
      "Test set accuracy 0.438400000333786\n",
      "-------------------------------\n",
      "660/1000, 0.01 sec, 7.94 sec\n",
      "Training set loss 1.9367396831512451\n",
      "Training set accuracy 0.3746199905872345\n",
      "Test set accuracy 0.3797999918460846\n",
      "-------------------------------\n",
      "661/1000, 0.01 sec, 7.95 sec\n",
      "Training set loss 1.9683958292007446\n",
      "Training set accuracy 0.42923998832702637\n",
      "Test set accuracy 0.4389999806880951\n",
      "-------------------------------\n",
      "662/1000, 0.01 sec, 7.96 sec\n",
      "Training set loss 1.9309574365615845\n",
      "Training set accuracy 0.3748199939727783\n",
      "Test set accuracy 0.37959998846054077\n",
      "-------------------------------\n",
      "663/1000, 0.01 sec, 7.97 sec\n",
      "Training set loss 1.9625979661941528\n",
      "Training set accuracy 0.43007999658584595\n",
      "Test set accuracy 0.4400999844074249\n",
      "-------------------------------\n",
      "664/1000, 0.01 sec, 7.98 sec\n",
      "Training set loss 1.9253473281860352\n",
      "Training set accuracy 0.37477999925613403\n",
      "Test set accuracy 0.3798999786376953\n",
      "-------------------------------\n",
      "665/1000, 0.01 sec, 7.99 sec\n",
      "Training set loss 1.956644058227539\n",
      "Training set accuracy 0.4308199882507324\n",
      "Test set accuracy 0.4400999844074249\n",
      "-------------------------------\n",
      "666/1000, 0.01 sec, 8.01 sec\n",
      "Training set loss 1.9194421768188477\n",
      "Training set accuracy 0.3746599853038788\n",
      "Test set accuracy 0.3797000050544739\n",
      "-------------------------------\n",
      "667/1000, 0.01 sec, 8.02 sec\n",
      "Training set loss 1.9504809379577637\n",
      "Training set accuracy 0.43153998255729675\n",
      "Test set accuracy 0.4407999813556671\n",
      "-------------------------------\n",
      "668/1000, 0.01 sec, 8.03 sec\n",
      "Training set loss 1.9135959148406982\n",
      "Training set accuracy 0.37425997853279114\n",
      "Test set accuracy 0.3807999789714813\n",
      "-------------------------------\n",
      "669/1000, 0.01 sec, 8.04 sec\n",
      "Training set loss 1.9443359375\n",
      "Training set accuracy 0.4321399927139282\n",
      "Test set accuracy 0.4416999816894531\n",
      "-------------------------------\n",
      "670/1000, 0.01 sec, 8.05 sec\n",
      "Training set loss 1.9076588153839111\n",
      "Training set accuracy 0.3741599917411804\n",
      "Test set accuracy 0.38109999895095825\n",
      "-------------------------------\n",
      "671/1000, 0.01 sec, 8.07 sec\n",
      "Training set loss 1.9379531145095825\n",
      "Training set accuracy 0.4328399896621704\n",
      "Test set accuracy 0.44200000166893005\n",
      "-------------------------------\n",
      "672/1000, 0.01 sec, 8.08 sec\n",
      "Training set loss 1.9017893075942993\n",
      "Training set accuracy 0.3739999830722809\n",
      "Test set accuracy 0.38089999556541443\n",
      "-------------------------------\n",
      "673/1000, 0.01 sec, 8.09 sec\n",
      "Training set loss 1.9318671226501465\n",
      "Training set accuracy 0.4335799813270569\n",
      "Test set accuracy 0.4422999918460846\n",
      "-------------------------------\n",
      "674/1000, 0.01 sec, 8.10 sec\n",
      "Training set loss 1.895900011062622\n",
      "Training set accuracy 0.3741599917411804\n",
      "Test set accuracy 0.38089999556541443\n",
      "-------------------------------\n",
      "675/1000, 0.01 sec, 8.11 sec\n",
      "Training set loss 1.9256656169891357\n",
      "Training set accuracy 0.43413999676704407\n",
      "Test set accuracy 0.4430999755859375\n",
      "-------------------------------\n",
      "676/1000, 0.01 sec, 8.12 sec\n",
      "Training set loss 1.8901379108428955\n",
      "Training set accuracy 0.374239981174469\n",
      "Test set accuracy 0.38109999895095825\n",
      "-------------------------------\n",
      "677/1000, 0.01 sec, 8.14 sec\n",
      "Training set loss 1.9197636842727661\n",
      "Training set accuracy 0.4347800016403198\n",
      "Test set accuracy 0.44349998235702515\n",
      "-------------------------------\n",
      "678/1000, 0.01 sec, 8.15 sec\n",
      "Training set loss 1.8843237161636353\n",
      "Training set accuracy 0.37449997663497925\n",
      "Test set accuracy 0.38109999895095825\n",
      "-------------------------------\n",
      "679/1000, 0.01 sec, 8.16 sec\n",
      "Training set loss 1.9138718843460083\n",
      "Training set accuracy 0.43546000123023987\n",
      "Test set accuracy 0.4438000023365021\n",
      "-------------------------------\n",
      "680/1000, 0.01 sec, 8.17 sec\n",
      "Training set loss 1.8788303136825562\n",
      "Training set accuracy 0.37421998381614685\n",
      "Test set accuracy 0.38119998574256897\n",
      "-------------------------------\n",
      "681/1000, 0.01 sec, 8.18 sec\n",
      "Training set loss 1.908272624015808\n",
      "Training set accuracy 0.4360399842262268\n",
      "Test set accuracy 0.4448999762535095\n",
      "-------------------------------\n",
      "682/1000, 0.01 sec, 8.19 sec\n",
      "Training set loss 1.8735034465789795\n",
      "Training set accuracy 0.374019980430603\n",
      "Test set accuracy 0.38119998574256897\n",
      "-------------------------------\n",
      "683/1000, 0.01 sec, 8.21 sec\n",
      "Training set loss 1.902922511100769\n",
      "Training set accuracy 0.43658000230789185\n",
      "Test set accuracy 0.4453999996185303\n",
      "-------------------------------\n",
      "684/1000, 0.01 sec, 8.22 sec\n",
      "Training set loss 1.8686217069625854\n",
      "Training set accuracy 0.37351998686790466\n",
      "Test set accuracy 0.3806999921798706\n",
      "-------------------------------\n",
      "685/1000, 0.01 sec, 8.23 sec\n",
      "Training set loss 1.897965908050537\n",
      "Training set accuracy 0.43713998794555664\n",
      "Test set accuracy 0.4456000030040741\n",
      "-------------------------------\n",
      "686/1000, 0.01 sec, 8.24 sec\n",
      "Training set loss 1.8640003204345703\n",
      "Training set accuracy 0.3732599914073944\n",
      "Test set accuracy 0.3799999952316284\n",
      "-------------------------------\n",
      "687/1000, 0.01 sec, 8.25 sec\n",
      "Training set loss 1.8930902481079102\n",
      "Training set accuracy 0.4379599988460541\n",
      "Test set accuracy 0.4453999996185303\n",
      "-------------------------------\n",
      "688/1000, 0.01 sec, 8.26 sec\n",
      "Training set loss 1.8590013980865479\n",
      "Training set accuracy 0.37307998538017273\n",
      "Test set accuracy 0.38040000200271606\n",
      "-------------------------------\n",
      "689/1000, 0.01 sec, 8.27 sec\n",
      "Training set loss 1.8877097368240356\n",
      "Training set accuracy 0.43849998712539673\n",
      "Test set accuracy 0.44609999656677246\n",
      "-------------------------------\n",
      "690/1000, 0.01 sec, 8.29 sec\n",
      "Training set loss 1.8536721467971802\n",
      "Training set accuracy 0.37285998463630676\n",
      "Test set accuracy 0.38019999861717224\n",
      "-------------------------------\n",
      "691/1000, 0.01 sec, 8.30 sec\n",
      "Training set loss 1.8819210529327393\n",
      "Training set accuracy 0.43884000182151794\n",
      "Test set accuracy 0.44599997997283936\n",
      "-------------------------------\n",
      "692/1000, 0.01 sec, 8.31 sec\n",
      "Training set loss 1.8483151197433472\n",
      "Training set accuracy 0.3729199767112732\n",
      "Test set accuracy 0.38040000200271606\n",
      "-------------------------------\n",
      "693/1000, 0.01 sec, 8.32 sec\n",
      "Training set loss 1.8763432502746582\n",
      "Training set accuracy 0.4391999840736389\n",
      "Test set accuracy 0.4465999901294708\n",
      "-------------------------------\n",
      "694/1000, 0.01 sec, 8.33 sec\n",
      "Training set loss 1.8430830240249634\n",
      "Training set accuracy 0.3726999759674072\n",
      "Test set accuracy 0.3805999755859375\n",
      "-------------------------------\n",
      "695/1000, 0.01 sec, 8.34 sec\n",
      "Training set loss 1.8707902431488037\n",
      "Training set accuracy 0.4400799870491028\n",
      "Test set accuracy 0.44679999351501465\n",
      "-------------------------------\n",
      "696/1000, 0.01 sec, 8.36 sec\n",
      "Training set loss 1.8379203081130981\n",
      "Training set accuracy 0.37257999181747437\n",
      "Test set accuracy 0.38009998202323914\n",
      "-------------------------------\n",
      "697/1000, 0.01 sec, 8.37 sec\n",
      "Training set loss 1.865134835243225\n",
      "Training set accuracy 0.4405199885368347\n",
      "Test set accuracy 0.446399986743927\n",
      "-------------------------------\n",
      "698/1000, 0.01 sec, 8.38 sec\n",
      "Training set loss 1.832573413848877\n",
      "Training set accuracy 0.37209999561309814\n",
      "Test set accuracy 0.3797999918460846\n",
      "-------------------------------\n",
      "699/1000, 0.01 sec, 8.39 sec\n",
      "Training set loss 1.8594239950180054\n",
      "Training set accuracy 0.4408399760723114\n",
      "Test set accuracy 0.44689998030662537\n",
      "-------------------------------\n",
      "700/1000, 0.01 sec, 8.40 sec\n",
      "Training set loss 1.8271280527114868\n",
      "Training set accuracy 0.3718400001525879\n",
      "Test set accuracy 0.37939998507499695\n",
      "-------------------------------\n",
      "701/1000, 0.01 sec, 8.41 sec\n",
      "Training set loss 1.8535974025726318\n",
      "Training set accuracy 0.44165998697280884\n",
      "Test set accuracy 0.44699999690055847\n",
      "-------------------------------\n",
      "702/1000, 0.01 sec, 8.43 sec\n",
      "Training set loss 1.821410894393921\n",
      "Training set accuracy 0.37153998017311096\n",
      "Test set accuracy 0.3797000050544739\n",
      "-------------------------------\n",
      "703/1000, 0.01 sec, 8.44 sec\n",
      "Training set loss 1.8473939895629883\n",
      "Training set accuracy 0.4421199858188629\n",
      "Test set accuracy 0.44759997725486755\n",
      "-------------------------------\n",
      "704/1000, 0.01 sec, 8.45 sec\n",
      "Training set loss 1.815845251083374\n",
      "Training set accuracy 0.3714599907398224\n",
      "Test set accuracy 0.3790999948978424\n",
      "-------------------------------\n",
      "705/1000, 0.01 sec, 8.46 sec\n",
      "Training set loss 1.8415409326553345\n",
      "Training set accuracy 0.44246000051498413\n",
      "Test set accuracy 0.44769999384880066\n",
      "-------------------------------\n",
      "706/1000, 0.01 sec, 8.48 sec\n",
      "Training set loss 1.8104568719863892\n",
      "Training set accuracy 0.37133997678756714\n",
      "Test set accuracy 0.3783999979496002\n",
      "-------------------------------\n",
      "707/1000, 0.01 sec, 8.49 sec\n",
      "Training set loss 1.8359524011611938\n",
      "Training set accuracy 0.44312000274658203\n",
      "Test set accuracy 0.447299987077713\n",
      "-------------------------------\n",
      "708/1000, 0.01 sec, 8.50 sec\n",
      "Training set loss 1.805411458015442\n",
      "Training set accuracy 0.3708399832248688\n",
      "Test set accuracy 0.3780999779701233\n",
      "-------------------------------\n",
      "709/1000, 0.01 sec, 8.52 sec\n",
      "Training set loss 1.830762267112732\n",
      "Training set accuracy 0.44331997632980347\n",
      "Test set accuracy 0.44739997386932373\n",
      "-------------------------------\n",
      "710/1000, 0.01 sec, 8.53 sec\n",
      "Training set loss 1.800541639328003\n",
      "Training set accuracy 0.37053999304771423\n",
      "Test set accuracy 0.3775999844074249\n",
      "-------------------------------\n",
      "711/1000, 0.01 sec, 8.54 sec\n",
      "Training set loss 1.8256434202194214\n",
      "Training set accuracy 0.44363999366760254\n",
      "Test set accuracy 0.4481000006198883\n",
      "-------------------------------\n",
      "712/1000, 0.01 sec, 8.55 sec\n",
      "Training set loss 1.7956184148788452\n",
      "Training set accuracy 0.37039998173713684\n",
      "Test set accuracy 0.37779998779296875\n",
      "-------------------------------\n",
      "713/1000, 0.01 sec, 8.57 sec\n",
      "Training set loss 1.820701241493225\n",
      "Training set accuracy 0.44373998045921326\n",
      "Test set accuracy 0.4487999975681305\n",
      "-------------------------------\n",
      "714/1000, 0.01 sec, 8.58 sec\n",
      "Training set loss 1.7909818887710571\n",
      "Training set accuracy 0.36987999081611633\n",
      "Test set accuracy 0.376800000667572\n",
      "-------------------------------\n",
      "715/1000, 0.01 sec, 8.59 sec\n",
      "Training set loss 1.815948724746704\n",
      "Training set accuracy 0.44426000118255615\n",
      "Test set accuracy 0.44919997453689575\n",
      "-------------------------------\n",
      "716/1000, 0.01 sec, 8.60 sec\n",
      "Training set loss 1.786566972732544\n",
      "Training set accuracy 0.36969998478889465\n",
      "Test set accuracy 0.375900000333786\n",
      "-------------------------------\n",
      "717/1000, 0.01 sec, 8.61 sec\n",
      "Training set loss 1.8113757371902466\n",
      "Training set accuracy 0.44481998682022095\n",
      "Test set accuracy 0.45009997487068176\n",
      "-------------------------------\n",
      "718/1000, 0.01 sec, 8.63 sec\n",
      "Training set loss 1.7823671102523804\n",
      "Training set accuracy 0.369159996509552\n",
      "Test set accuracy 0.3748999834060669\n",
      "-------------------------------\n",
      "719/1000, 0.01 sec, 8.64 sec\n",
      "Training set loss 1.8071010112762451\n",
      "Training set accuracy 0.4452599883079529\n",
      "Test set accuracy 0.44999998807907104\n",
      "-------------------------------\n",
      "720/1000, 0.01 sec, 8.65 sec\n",
      "Training set loss 1.7782645225524902\n",
      "Training set accuracy 0.3686999976634979\n",
      "Test set accuracy 0.374099999666214\n",
      "-------------------------------\n",
      "721/1000, 0.01 sec, 8.66 sec\n",
      "Training set loss 1.8030478954315186\n",
      "Training set accuracy 0.4456000030040741\n",
      "Test set accuracy 0.45080000162124634\n",
      "-------------------------------\n",
      "722/1000, 0.01 sec, 8.68 sec\n",
      "Training set loss 1.7742341756820679\n",
      "Training set accuracy 0.3684999942779541\n",
      "Test set accuracy 0.3734999895095825\n",
      "-------------------------------\n",
      "723/1000, 0.01 sec, 8.69 sec\n",
      "Training set loss 1.7989068031311035\n",
      "Training set accuracy 0.44579997658729553\n",
      "Test set accuracy 0.4510999917984009\n",
      "-------------------------------\n",
      "724/1000, 0.01 sec, 8.70 sec\n",
      "Training set loss 1.7702335119247437\n",
      "Training set accuracy 0.3681599795818329\n",
      "Test set accuracy 0.37279999256134033\n",
      "-------------------------------\n",
      "725/1000, 0.01 sec, 8.71 sec\n",
      "Training set loss 1.794702410697937\n",
      "Training set accuracy 0.44603997468948364\n",
      "Test set accuracy 0.45179998874664307\n",
      "-------------------------------\n",
      "726/1000, 0.01 sec, 8.72 sec\n",
      "Training set loss 1.7661467790603638\n",
      "Training set accuracy 0.3680799901485443\n",
      "Test set accuracy 0.3723999857902527\n",
      "-------------------------------\n",
      "727/1000, 0.01 sec, 8.74 sec\n",
      "Training set loss 1.7905538082122803\n",
      "Training set accuracy 0.4467199742794037\n",
      "Test set accuracy 0.4526999890804291\n",
      "-------------------------------\n",
      "728/1000, 0.01 sec, 8.75 sec\n",
      "Training set loss 1.7618671655654907\n",
      "Training set accuracy 0.36777999997138977\n",
      "Test set accuracy 0.3725999891757965\n",
      "-------------------------------\n",
      "729/1000, 0.01 sec, 8.76 sec\n",
      "Training set loss 1.7861477136611938\n",
      "Training set accuracy 0.447299987077713\n",
      "Test set accuracy 0.4529999792575836\n",
      "-------------------------------\n",
      "730/1000, 0.01 sec, 8.77 sec\n",
      "Training set loss 1.7574522495269775\n",
      "Training set accuracy 0.3673799932003021\n",
      "Test set accuracy 0.3718000054359436\n",
      "-------------------------------\n",
      "731/1000, 0.01 sec, 8.79 sec\n",
      "Training set loss 1.7815150022506714\n",
      "Training set accuracy 0.4475799798965454\n",
      "Test set accuracy 0.4536999762058258\n",
      "-------------------------------\n",
      "732/1000, 0.01 sec, 8.80 sec\n",
      "Training set loss 1.753037452697754\n",
      "Training set accuracy 0.3671799898147583\n",
      "Test set accuracy 0.3714999854564667\n",
      "-------------------------------\n",
      "733/1000, 0.01 sec, 8.81 sec\n",
      "Training set loss 1.7769430875778198\n",
      "Training set accuracy 0.4475399851799011\n",
      "Test set accuracy 0.45399999618530273\n",
      "-------------------------------\n",
      "734/1000, 0.01 sec, 8.82 sec\n",
      "Training set loss 1.7487008571624756\n",
      "Training set accuracy 0.3667599856853485\n",
      "Test set accuracy 0.3698999881744385\n",
      "-------------------------------\n",
      "735/1000, 0.01 sec, 8.84 sec\n",
      "Training set loss 1.772475242614746\n",
      "Training set accuracy 0.4477999806404114\n",
      "Test set accuracy 0.45350000262260437\n",
      "-------------------------------\n",
      "736/1000, 0.01 sec, 8.85 sec\n",
      "Training set loss 1.7444740533828735\n",
      "Training set accuracy 0.3666999936103821\n",
      "Test set accuracy 0.36949998140335083\n",
      "-------------------------------\n",
      "737/1000, 0.01 sec, 8.86 sec\n",
      "Training set loss 1.7681270837783813\n",
      "Training set accuracy 0.44843998551368713\n",
      "Test set accuracy 0.4537999927997589\n",
      "-------------------------------\n",
      "738/1000, 0.01 sec, 8.87 sec\n",
      "Training set loss 1.7405258417129517\n",
      "Training set accuracy 0.36653998494148254\n",
      "Test set accuracy 0.3691999912261963\n",
      "-------------------------------\n",
      "739/1000, 0.01 sec, 8.88 sec\n",
      "Training set loss 1.7640763521194458\n",
      "Training set accuracy 0.4488599896430969\n",
      "Test set accuracy 0.4544000029563904\n",
      "-------------------------------\n",
      "740/1000, 0.01 sec, 8.90 sec\n",
      "Training set loss 1.736756682395935\n",
      "Training set accuracy 0.36635997891426086\n",
      "Test set accuracy 0.3685999810695648\n",
      "-------------------------------\n",
      "741/1000, 0.01 sec, 8.91 sec\n",
      "Training set loss 1.7602511644363403\n",
      "Training set accuracy 0.44905999302864075\n",
      "Test set accuracy 0.4542999863624573\n",
      "-------------------------------\n",
      "742/1000, 0.01 sec, 8.92 sec\n",
      "Training set loss 1.7329318523406982\n",
      "Training set accuracy 0.36591997742652893\n",
      "Test set accuracy 0.36800000071525574\n",
      "-------------------------------\n",
      "743/1000, 0.01 sec, 8.93 sec\n",
      "Training set loss 1.7562870979309082\n",
      "Training set accuracy 0.4492799937725067\n",
      "Test set accuracy 0.4542999863624573\n",
      "-------------------------------\n",
      "744/1000, 0.01 sec, 8.95 sec\n",
      "Training set loss 1.7291080951690674\n",
      "Training set accuracy 0.3658999800682068\n",
      "Test set accuracy 0.3682999908924103\n",
      "-------------------------------\n",
      "745/1000, 0.01 sec, 8.96 sec\n",
      "Training set loss 1.7525345087051392\n",
      "Training set accuracy 0.4494199752807617\n",
      "Test set accuracy 0.4545999765396118\n",
      "-------------------------------\n",
      "746/1000, 0.01 sec, 8.97 sec\n",
      "Training set loss 1.7254027128219604\n",
      "Training set accuracy 0.3653799891471863\n",
      "Test set accuracy 0.36730000376701355\n",
      "-------------------------------\n",
      "747/1000, 0.01 sec, 8.98 sec\n",
      "Training set loss 1.7490118741989136\n",
      "Training set accuracy 0.44991999864578247\n",
      "Test set accuracy 0.4542999863624573\n",
      "-------------------------------\n",
      "748/1000, 0.01 sec, 9.00 sec\n",
      "Training set loss 1.7218290567398071\n",
      "Training set accuracy 0.36484000086784363\n",
      "Test set accuracy 0.3676999807357788\n",
      "-------------------------------\n",
      "749/1000, 0.01 sec, 9.01 sec\n",
      "Training set loss 1.7453452348709106\n",
      "Training set accuracy 0.4500199854373932\n",
      "Test set accuracy 0.45409998297691345\n",
      "-------------------------------\n",
      "750/1000, 0.01 sec, 9.02 sec\n",
      "Training set loss 1.7182343006134033\n",
      "Training set accuracy 0.36409997940063477\n",
      "Test set accuracy 0.36719998717308044\n",
      "-------------------------------\n",
      "751/1000, 0.01 sec, 9.03 sec\n",
      "Training set loss 1.7417525053024292\n",
      "Training set accuracy 0.450219988822937\n",
      "Test set accuracy 0.45419999957084656\n",
      "-------------------------------\n",
      "752/1000, 0.01 sec, 9.04 sec\n",
      "Training set loss 1.7146154642105103\n",
      "Training set accuracy 0.3634600043296814\n",
      "Test set accuracy 0.367499977350235\n",
      "-------------------------------\n",
      "753/1000, 0.01 sec, 9.06 sec\n",
      "Training set loss 1.737899899482727\n",
      "Training set accuracy 0.4507799744606018\n",
      "Test set accuracy 0.45409998297691345\n",
      "-------------------------------\n",
      "754/1000, 0.01 sec, 9.07 sec\n",
      "Training set loss 1.7108405828475952\n",
      "Training set accuracy 0.3630400002002716\n",
      "Test set accuracy 0.3671000003814697\n",
      "-------------------------------\n",
      "755/1000, 0.01 sec, 9.08 sec\n",
      "Training set loss 1.7339309453964233\n",
      "Training set accuracy 0.451339989900589\n",
      "Test set accuracy 0.45479997992515564\n",
      "-------------------------------\n",
      "756/1000, 0.01 sec, 9.09 sec\n",
      "Training set loss 1.7071443796157837\n",
      "Training set accuracy 0.36267998814582825\n",
      "Test set accuracy 0.36629998683929443\n",
      "-------------------------------\n",
      "757/1000, 0.01 sec, 9.11 sec\n",
      "Training set loss 1.7301042079925537\n",
      "Training set accuracy 0.4516799747943878\n",
      "Test set accuracy 0.4546999931335449\n",
      "-------------------------------\n",
      "758/1000, 0.01 sec, 9.12 sec\n",
      "Training set loss 1.703513741493225\n",
      "Training set accuracy 0.3624599874019623\n",
      "Test set accuracy 0.36640000343322754\n",
      "-------------------------------\n",
      "759/1000, 0.01 sec, 9.13 sec\n",
      "Training set loss 1.7262649536132812\n",
      "Training set accuracy 0.45221999287605286\n",
      "Test set accuracy 0.45509999990463257\n",
      "-------------------------------\n",
      "760/1000, 0.01 sec, 9.14 sec\n",
      "Training set loss 1.6996046304702759\n",
      "Training set accuracy 0.362199991941452\n",
      "Test set accuracy 0.36640000343322754\n",
      "-------------------------------\n",
      "761/1000, 0.01 sec, 9.15 sec\n",
      "Training set loss 1.722116231918335\n",
      "Training set accuracy 0.4524199962615967\n",
      "Test set accuracy 0.45649999380111694\n",
      "-------------------------------\n",
      "762/1000, 0.01 sec, 9.17 sec\n",
      "Training set loss 1.6954734325408936\n",
      "Training set accuracy 0.3619599938392639\n",
      "Test set accuracy 0.3659999966621399\n",
      "-------------------------------\n",
      "763/1000, 0.01 sec, 9.18 sec\n",
      "Training set loss 1.7177520990371704\n",
      "Training set accuracy 0.4527599811553955\n",
      "Test set accuracy 0.4562999904155731\n",
      "-------------------------------\n",
      "764/1000, 0.01 sec, 9.19 sec\n",
      "Training set loss 1.6914442777633667\n",
      "Training set accuracy 0.3619000017642975\n",
      "Test set accuracy 0.3658999800682068\n",
      "-------------------------------\n",
      "765/1000, 0.01 sec, 9.20 sec\n",
      "Training set loss 1.7134714126586914\n",
      "Training set accuracy 0.4530799984931946\n",
      "Test set accuracy 0.45619997382164\n",
      "-------------------------------\n",
      "766/1000, 0.01 sec, 9.21 sec\n",
      "Training set loss 1.6875001192092896\n",
      "Training set accuracy 0.3614799976348877\n",
      "Test set accuracy 0.36579999327659607\n",
      "-------------------------------\n",
      "767/1000, 0.01 sec, 9.23 sec\n",
      "Training set loss 1.7094234228134155\n",
      "Training set accuracy 0.4536399841308594\n",
      "Test set accuracy 0.45669999718666077\n",
      "-------------------------------\n",
      "768/1000, 0.01 sec, 9.24 sec\n",
      "Training set loss 1.6837427616119385\n",
      "Training set accuracy 0.36131998896598816\n",
      "Test set accuracy 0.36550000309944153\n",
      "-------------------------------\n",
      "769/1000, 0.01 sec, 9.25 sec\n",
      "Training set loss 1.7054537534713745\n",
      "Training set accuracy 0.45361998677253723\n",
      "Test set accuracy 0.4569000005722046\n",
      "-------------------------------\n",
      "770/1000, 0.01 sec, 9.27 sec\n",
      "Training set loss 1.6798160076141357\n",
      "Training set accuracy 0.3611399829387665\n",
      "Test set accuracy 0.3651999831199646\n",
      "-------------------------------\n",
      "771/1000, 0.01 sec, 9.28 sec\n",
      "Training set loss 1.7015173435211182\n",
      "Training set accuracy 0.45361998677253723\n",
      "Test set accuracy 0.457099974155426\n",
      "-------------------------------\n",
      "772/1000, 0.01 sec, 9.29 sec\n",
      "Training set loss 1.6760892868041992\n",
      "Training set accuracy 0.36079999804496765\n",
      "Test set accuracy 0.36489999294281006\n",
      "-------------------------------\n",
      "773/1000, 0.01 sec, 9.30 sec\n",
      "Training set loss 1.6978946924209595\n",
      "Training set accuracy 0.4540199935436249\n",
      "Test set accuracy 0.4578000009059906\n",
      "-------------------------------\n",
      "774/1000, 0.01 sec, 9.32 sec\n",
      "Training set loss 1.6727265119552612\n",
      "Training set accuracy 0.36052000522613525\n",
      "Test set accuracy 0.3643999993801117\n",
      "-------------------------------\n",
      "775/1000, 0.01 sec, 9.33 sec\n",
      "Training set loss 1.6947027444839478\n",
      "Training set accuracy 0.4545799791812897\n",
      "Test set accuracy 0.45799997448921204\n",
      "-------------------------------\n",
      "776/1000, 0.01 sec, 9.34 sec\n",
      "Training set loss 1.6697332859039307\n",
      "Training set accuracy 0.36023998260498047\n",
      "Test set accuracy 0.3643999993801117\n",
      "-------------------------------\n",
      "777/1000, 0.01 sec, 9.35 sec\n",
      "Training set loss 1.6916999816894531\n",
      "Training set accuracy 0.45465999841690063\n",
      "Test set accuracy 0.45809999108314514\n",
      "-------------------------------\n",
      "778/1000, 0.01 sec, 9.37 sec\n",
      "Training set loss 1.6666759252548218\n",
      "Training set accuracy 0.3596400022506714\n",
      "Test set accuracy 0.3637999892234802\n",
      "-------------------------------\n",
      "779/1000, 0.01 sec, 9.38 sec\n",
      "Training set loss 1.6886743307113647\n",
      "Training set accuracy 0.45499998331069946\n",
      "Test set accuracy 0.45799997448921204\n",
      "-------------------------------\n",
      "780/1000, 0.01 sec, 9.39 sec\n",
      "Training set loss 1.6636959314346313\n",
      "Training set accuracy 0.35923999547958374\n",
      "Test set accuracy 0.3633999824523926\n",
      "-------------------------------\n",
      "781/1000, 0.01 sec, 9.40 sec\n",
      "Training set loss 1.6858752965927124\n",
      "Training set accuracy 0.4551999866962433\n",
      "Test set accuracy 0.45819997787475586\n",
      "-------------------------------\n",
      "782/1000, 0.01 sec, 9.42 sec\n",
      "Training set loss 1.6606332063674927\n",
      "Training set accuracy 0.35899999737739563\n",
      "Test set accuracy 0.36309999227523804\n",
      "-------------------------------\n",
      "783/1000, 0.01 sec, 9.43 sec\n",
      "Training set loss 1.6828436851501465\n",
      "Training set accuracy 0.45527997612953186\n",
      "Test set accuracy 0.45879998803138733\n",
      "-------------------------------\n",
      "784/1000, 0.01 sec, 9.44 sec\n",
      "Training set loss 1.6576212644577026\n",
      "Training set accuracy 0.358379989862442\n",
      "Test set accuracy 0.3633999824523926\n",
      "-------------------------------\n",
      "785/1000, 0.01 sec, 9.45 sec\n",
      "Training set loss 1.6797993183135986\n",
      "Training set accuracy 0.4553599953651428\n",
      "Test set accuracy 0.459199994802475\n",
      "-------------------------------\n",
      "786/1000, 0.01 sec, 9.46 sec\n",
      "Training set loss 1.654642105102539\n",
      "Training set accuracy 0.3580999970436096\n",
      "Test set accuracy 0.3628000020980835\n",
      "-------------------------------\n",
      "787/1000, 0.01 sec, 9.47 sec\n",
      "Training set loss 1.6769264936447144\n",
      "Training set accuracy 0.4555400013923645\n",
      "Test set accuracy 0.45989999175071716\n",
      "-------------------------------\n",
      "788/1000, 0.01 sec, 9.48 sec\n",
      "Training set loss 1.6517248153686523\n",
      "Training set accuracy 0.35797998309135437\n",
      "Test set accuracy 0.36239999532699585\n",
      "-------------------------------\n",
      "789/1000, 0.01 sec, 9.50 sec\n",
      "Training set loss 1.6739615201950073\n",
      "Training set accuracy 0.45579999685287476\n",
      "Test set accuracy 0.4602999985218048\n",
      "-------------------------------\n",
      "790/1000, 0.01 sec, 9.51 sec\n",
      "Training set loss 1.6487056016921997\n",
      "Training set accuracy 0.3578599989414215\n",
      "Test set accuracy 0.3621000051498413\n",
      "-------------------------------\n",
      "791/1000, 0.01 sec, 9.52 sec\n",
      "Training set loss 1.6708521842956543\n",
      "Training set accuracy 0.45607998967170715\n",
      "Test set accuracy 0.46050000190734863\n",
      "-------------------------------\n",
      "792/1000, 0.01 sec, 9.53 sec\n",
      "Training set loss 1.6457196474075317\n",
      "Training set accuracy 0.35773998498916626\n",
      "Test set accuracy 0.3619000017642975\n",
      "-------------------------------\n",
      "793/1000, 0.01 sec, 9.54 sec\n",
      "Training set loss 1.6678919792175293\n",
      "Training set accuracy 0.45625999569892883\n",
      "Test set accuracy 0.4607999920845032\n",
      "-------------------------------\n",
      "794/1000, 0.01 sec, 9.55 sec\n",
      "Training set loss 1.6425989866256714\n",
      "Training set accuracy 0.35731998085975647\n",
      "Test set accuracy 0.3619000017642975\n",
      "-------------------------------\n",
      "795/1000, 0.01 sec, 9.56 sec\n",
      "Training set loss 1.6647228002548218\n",
      "Training set accuracy 0.45681998133659363\n",
      "Test set accuracy 0.46149998903274536\n",
      "-------------------------------\n",
      "796/1000, 0.01 sec, 9.58 sec\n",
      "Training set loss 1.6394513845443726\n",
      "Training set accuracy 0.3570999801158905\n",
      "Test set accuracy 0.3612000048160553\n",
      "-------------------------------\n",
      "797/1000, 0.01 sec, 9.59 sec\n",
      "Training set loss 1.6615880727767944\n",
      "Training set accuracy 0.45719999074935913\n",
      "Test set accuracy 0.4615999758243561\n",
      "-------------------------------\n",
      "798/1000, 0.01 sec, 9.60 sec\n",
      "Training set loss 1.6363096237182617\n",
      "Training set accuracy 0.3567200005054474\n",
      "Test set accuracy 0.3610000014305115\n",
      "-------------------------------\n",
      "799/1000, 0.01 sec, 9.61 sec\n",
      "Training set loss 1.6584511995315552\n",
      "Training set accuracy 0.4575600028038025\n",
      "Test set accuracy 0.46129998564720154\n",
      "-------------------------------\n",
      "800/1000, 0.01 sec, 9.62 sec\n",
      "Training set loss 1.6331721544265747\n",
      "Training set accuracy 0.35635998845100403\n",
      "Test set accuracy 0.3603000044822693\n",
      "-------------------------------\n",
      "801/1000, 0.01 sec, 9.63 sec\n",
      "Training set loss 1.655153751373291\n",
      "Training set accuracy 0.45781999826431274\n",
      "Test set accuracy 0.46140000224113464\n",
      "-------------------------------\n",
      "802/1000, 0.01 sec, 9.65 sec\n",
      "Training set loss 1.6300983428955078\n",
      "Training set accuracy 0.35637998580932617\n",
      "Test set accuracy 0.3601999878883362\n",
      "-------------------------------\n",
      "803/1000, 0.01 sec, 9.66 sec\n",
      "Training set loss 1.65188467502594\n",
      "Training set accuracy 0.45799997448921204\n",
      "Test set accuracy 0.4619999825954437\n",
      "-------------------------------\n",
      "804/1000, 0.01 sec, 9.67 sec\n",
      "Training set loss 1.6269034147262573\n",
      "Training set accuracy 0.3559799790382385\n",
      "Test set accuracy 0.3604999780654907\n",
      "-------------------------------\n",
      "805/1000, 0.01 sec, 9.68 sec\n",
      "Training set loss 1.648599624633789\n",
      "Training set accuracy 0.45843997597694397\n",
      "Test set accuracy 0.4624999761581421\n",
      "-------------------------------\n",
      "806/1000, 0.01 sec, 9.69 sec\n",
      "Training set loss 1.6238056421279907\n",
      "Training set accuracy 0.3558399975299835\n",
      "Test set accuracy 0.3604999780654907\n",
      "-------------------------------\n",
      "807/1000, 0.01 sec, 9.70 sec\n",
      "Training set loss 1.6452293395996094\n",
      "Training set accuracy 0.45889997482299805\n",
      "Test set accuracy 0.46289998292922974\n",
      "-------------------------------\n",
      "808/1000, 0.01 sec, 9.72 sec\n",
      "Training set loss 1.6206125020980835\n",
      "Training set accuracy 0.35565999150276184\n",
      "Test set accuracy 0.36059999465942383\n",
      "-------------------------------\n",
      "809/1000, 0.01 sec, 9.73 sec\n",
      "Training set loss 1.6419472694396973\n",
      "Training set accuracy 0.45909997820854187\n",
      "Test set accuracy 0.46379998326301575\n",
      "-------------------------------\n",
      "810/1000, 0.01 sec, 9.74 sec\n",
      "Training set loss 1.6175365447998047\n",
      "Training set accuracy 0.3553600013256073\n",
      "Test set accuracy 0.35979998111724854\n",
      "-------------------------------\n",
      "811/1000, 0.01 sec, 9.75 sec\n",
      "Training set loss 1.638788104057312\n",
      "Training set accuracy 0.45914000272750854\n",
      "Test set accuracy 0.4640999734401703\n",
      "-------------------------------\n",
      "812/1000, 0.01 sec, 9.76 sec\n",
      "Training set loss 1.6145650148391724\n",
      "Training set accuracy 0.3549000024795532\n",
      "Test set accuracy 0.36010000109672546\n",
      "-------------------------------\n",
      "813/1000, 0.01 sec, 9.78 sec\n",
      "Training set loss 1.6357192993164062\n",
      "Training set accuracy 0.45945999026298523\n",
      "Test set accuracy 0.46449998021125793\n",
      "-------------------------------\n",
      "814/1000, 0.01 sec, 9.79 sec\n",
      "Training set loss 1.611648440361023\n",
      "Training set accuracy 0.3543799817562103\n",
      "Test set accuracy 0.35989999771118164\n",
      "-------------------------------\n",
      "815/1000, 0.01 sec, 9.80 sec\n",
      "Training set loss 1.6327413320541382\n",
      "Training set accuracy 0.4597399830818176\n",
      "Test set accuracy 0.46389999985694885\n",
      "-------------------------------\n",
      "816/1000, 0.01 sec, 9.81 sec\n",
      "Training set loss 1.6087645292282104\n",
      "Training set accuracy 0.3538999855518341\n",
      "Test set accuracy 0.35979998111724854\n",
      "-------------------------------\n",
      "817/1000, 0.01 sec, 9.83 sec\n",
      "Training set loss 1.62994384765625\n",
      "Training set accuracy 0.45983999967575073\n",
      "Test set accuracy 0.4643999934196472\n",
      "-------------------------------\n",
      "818/1000, 0.01 sec, 9.84 sec\n",
      "Training set loss 1.6058539152145386\n",
      "Training set accuracy 0.35354000329971313\n",
      "Test set accuracy 0.35999998450279236\n",
      "-------------------------------\n",
      "819/1000, 0.01 sec, 9.85 sec\n",
      "Training set loss 1.6269136667251587\n",
      "Training set accuracy 0.45969998836517334\n",
      "Test set accuracy 0.4641999900341034\n",
      "-------------------------------\n",
      "820/1000, 0.01 sec, 9.86 sec\n",
      "Training set loss 1.602869987487793\n",
      "Training set accuracy 0.35332000255584717\n",
      "Test set accuracy 0.3594000041484833\n",
      "-------------------------------\n",
      "821/1000, 0.01 sec, 9.88 sec\n",
      "Training set loss 1.6239930391311646\n",
      "Training set accuracy 0.45983999967575073\n",
      "Test set accuracy 0.46469998359680176\n",
      "-------------------------------\n",
      "822/1000, 0.01 sec, 9.89 sec\n",
      "Training set loss 1.600031852722168\n",
      "Training set accuracy 0.35332000255584717\n",
      "Test set accuracy 0.3594000041484833\n",
      "-------------------------------\n",
      "823/1000, 0.01 sec, 9.90 sec\n",
      "Training set loss 1.6211442947387695\n",
      "Training set accuracy 0.45983999967575073\n",
      "Test set accuracy 0.46459999680519104\n",
      "-------------------------------\n",
      "824/1000, 0.01 sec, 9.91 sec\n",
      "Training set loss 1.597369909286499\n",
      "Training set accuracy 0.35315999388694763\n",
      "Test set accuracy 0.3587999939918518\n",
      "-------------------------------\n",
      "825/1000, 0.01 sec, 9.92 sec\n",
      "Training set loss 1.6185297966003418\n",
      "Training set accuracy 0.4602999985218048\n",
      "Test set accuracy 0.46469998359680176\n",
      "-------------------------------\n",
      "826/1000, 0.01 sec, 9.94 sec\n",
      "Training set loss 1.5948337316513062\n",
      "Training set accuracy 0.3527999818325043\n",
      "Test set accuracy 0.35920000076293945\n",
      "-------------------------------\n",
      "827/1000, 0.01 sec, 9.95 sec\n",
      "Training set loss 1.6160919666290283\n",
      "Training set accuracy 0.46041998267173767\n",
      "Test set accuracy 0.4652999937534332\n",
      "-------------------------------\n",
      "828/1000, 0.01 sec, 9.96 sec\n",
      "Training set loss 1.5922846794128418\n",
      "Training set accuracy 0.3524399995803833\n",
      "Test set accuracy 0.3588999807834625\n",
      "-------------------------------\n",
      "829/1000, 0.01 sec, 9.97 sec\n",
      "Training set loss 1.6134909391403198\n",
      "Training set accuracy 0.46087998151779175\n",
      "Test set accuracy 0.4657999873161316\n",
      "-------------------------------\n",
      "830/1000, 0.01 sec, 9.99 sec\n",
      "Training set loss 1.589694619178772\n",
      "Training set accuracy 0.3523799777030945\n",
      "Test set accuracy 0.35830000042915344\n",
      "-------------------------------\n",
      "831/1000, 0.01 sec, 10.00 sec\n",
      "Training set loss 1.6108989715576172\n",
      "Training set accuracy 0.4613799750804901\n",
      "Test set accuracy 0.4658999741077423\n",
      "-------------------------------\n",
      "832/1000, 0.01 sec, 10.01 sec\n",
      "Training set loss 1.5871453285217285\n",
      "Training set accuracy 0.35221999883651733\n",
      "Test set accuracy 0.35850000381469727\n",
      "-------------------------------\n",
      "833/1000, 0.01 sec, 10.02 sec\n",
      "Training set loss 1.6084609031677246\n",
      "Training set accuracy 0.4615199863910675\n",
      "Test set accuracy 0.4666000008583069\n",
      "-------------------------------\n",
      "834/1000, 0.01 sec, 10.04 sec\n",
      "Training set loss 1.5846737623214722\n",
      "Training set accuracy 0.35203999280929565\n",
      "Test set accuracy 0.35830000042915344\n",
      "-------------------------------\n",
      "835/1000, 0.01 sec, 10.05 sec\n",
      "Training set loss 1.6061036586761475\n",
      "Training set accuracy 0.46181997656822205\n",
      "Test set accuracy 0.4668999910354614\n",
      "-------------------------------\n",
      "836/1000, 0.01 sec, 10.06 sec\n",
      "Training set loss 1.582135796546936\n",
      "Training set accuracy 0.35213997960090637\n",
      "Test set accuracy 0.3578999936580658\n",
      "-------------------------------\n",
      "837/1000, 0.01 sec, 10.07 sec\n",
      "Training set loss 1.6035499572753906\n",
      "Training set accuracy 0.4621399939060211\n",
      "Test set accuracy 0.4673999845981598\n",
      "-------------------------------\n",
      "838/1000, 0.01 sec, 10.08 sec\n",
      "Training set loss 1.5795950889587402\n",
      "Training set accuracy 0.35175999999046326\n",
      "Test set accuracy 0.35830000042915344\n",
      "-------------------------------\n",
      "839/1000, 0.01 sec, 10.09 sec\n",
      "Training set loss 1.6010048389434814\n",
      "Training set accuracy 0.4619999825954437\n",
      "Test set accuracy 0.46799999475479126\n",
      "-------------------------------\n",
      "840/1000, 0.01 sec, 10.10 sec\n",
      "Training set loss 1.5770044326782227\n",
      "Training set accuracy 0.351639986038208\n",
      "Test set accuracy 0.3578999936580658\n",
      "-------------------------------\n",
      "841/1000, 0.01 sec, 10.11 sec\n",
      "Training set loss 1.5983468294143677\n",
      "Training set accuracy 0.46233999729156494\n",
      "Test set accuracy 0.4681999981403351\n",
      "-------------------------------\n",
      "842/1000, 0.01 sec, 10.13 sec\n",
      "Training set loss 1.5742889642715454\n",
      "Training set accuracy 0.3517400026321411\n",
      "Test set accuracy 0.35760000348091125\n",
      "-------------------------------\n",
      "843/1000, 0.01 sec, 10.14 sec\n",
      "Training set loss 1.5956454277038574\n",
      "Training set accuracy 0.46247997879981995\n",
      "Test set accuracy 0.46859997510910034\n",
      "-------------------------------\n",
      "844/1000, 0.01 sec, 10.15 sec\n",
      "Training set loss 1.5717374086380005\n",
      "Training set accuracy 0.35165998339653015\n",
      "Test set accuracy 0.3577999770641327\n",
      "-------------------------------\n",
      "845/1000, 0.01 sec, 10.16 sec\n",
      "Training set loss 1.5930880308151245\n",
      "Training set accuracy 0.46295997500419617\n",
      "Test set accuracy 0.4684000015258789\n",
      "-------------------------------\n",
      "846/1000, 0.01 sec, 10.17 sec\n",
      "Training set loss 1.5690674781799316\n",
      "Training set accuracy 0.35147997736930847\n",
      "Test set accuracy 0.3579999804496765\n",
      "-------------------------------\n",
      "847/1000, 0.01 sec, 10.18 sec\n",
      "Training set loss 1.5903252363204956\n",
      "Training set accuracy 0.46327999234199524\n",
      "Test set accuracy 0.4690999984741211\n",
      "-------------------------------\n",
      "848/1000, 0.01 sec, 10.19 sec\n",
      "Training set loss 1.5664359331130981\n",
      "Training set accuracy 0.3512199819087982\n",
      "Test set accuracy 0.35760000348091125\n",
      "-------------------------------\n",
      "849/1000, 0.01 sec, 10.20 sec\n",
      "Training set loss 1.5875728130340576\n",
      "Training set accuracy 0.4635399878025055\n",
      "Test set accuracy 0.4691999852657318\n",
      "-------------------------------\n",
      "850/1000, 0.01 sec, 10.22 sec\n",
      "Training set loss 1.5638152360916138\n",
      "Training set accuracy 0.3509199917316437\n",
      "Test set accuracy 0.3572999835014343\n",
      "-------------------------------\n",
      "851/1000, 0.01 sec, 10.23 sec\n",
      "Training set loss 1.5849636793136597\n",
      "Training set accuracy 0.4636799991130829\n",
      "Test set accuracy 0.47029998898506165\n",
      "-------------------------------\n",
      "852/1000, 0.01 sec, 10.24 sec\n",
      "Training set loss 1.5612282752990723\n",
      "Training set accuracy 0.3509199917316437\n",
      "Test set accuracy 0.3571999967098236\n",
      "-------------------------------\n",
      "853/1000, 0.01 sec, 10.25 sec\n",
      "Training set loss 1.5822510719299316\n",
      "Training set accuracy 0.4640199840068817\n",
      "Test set accuracy 0.4705999791622162\n",
      "-------------------------------\n",
      "854/1000, 0.01 sec, 10.27 sec\n",
      "Training set loss 1.5585732460021973\n",
      "Training set accuracy 0.35085999965667725\n",
      "Test set accuracy 0.35740000009536743\n",
      "-------------------------------\n",
      "855/1000, 0.01 sec, 10.28 sec\n",
      "Training set loss 1.5796175003051758\n",
      "Training set accuracy 0.4642999768257141\n",
      "Test set accuracy 0.47079998254776\n",
      "-------------------------------\n",
      "856/1000, 0.01 sec, 10.29 sec\n",
      "Training set loss 1.5561213493347168\n",
      "Training set accuracy 0.35085999965667725\n",
      "Test set accuracy 0.35740000009536743\n",
      "-------------------------------\n",
      "857/1000, 0.01 sec, 10.30 sec\n",
      "Training set loss 1.577162742614746\n",
      "Training set accuracy 0.46445998549461365\n",
      "Test set accuracy 0.47110000252723694\n",
      "-------------------------------\n",
      "858/1000, 0.01 sec, 10.31 sec\n",
      "Training set loss 1.5537703037261963\n",
      "Training set accuracy 0.3504999876022339\n",
      "Test set accuracy 0.35760000348091125\n",
      "-------------------------------\n",
      "859/1000, 0.01 sec, 10.32 sec\n",
      "Training set loss 1.5747636556625366\n",
      "Training set accuracy 0.4647599756717682\n",
      "Test set accuracy 0.4713999927043915\n",
      "-------------------------------\n",
      "860/1000, 0.01 sec, 10.33 sec\n",
      "Training set loss 1.5510510206222534\n",
      "Training set accuracy 0.35089999437332153\n",
      "Test set accuracy 0.3578999936580658\n",
      "-------------------------------\n",
      "861/1000, 0.01 sec, 10.35 sec\n",
      "Training set loss 1.571824312210083\n",
      "Training set accuracy 0.464959979057312\n",
      "Test set accuracy 0.4713999927043915\n",
      "-------------------------------\n",
      "862/1000, 0.01 sec, 10.36 sec\n",
      "Training set loss 1.5481170415878296\n",
      "Training set accuracy 0.35099998116493225\n",
      "Test set accuracy 0.358599990606308\n",
      "-------------------------------\n",
      "863/1000, 0.01 sec, 10.37 sec\n",
      "Training set loss 1.5687490701675415\n",
      "Training set accuracy 0.4650999903678894\n",
      "Test set accuracy 0.47119998931884766\n",
      "-------------------------------\n",
      "864/1000, 0.01 sec, 10.38 sec\n",
      "Training set loss 1.545376181602478\n",
      "Training set accuracy 0.3509399890899658\n",
      "Test set accuracy 0.35839998722076416\n",
      "-------------------------------\n",
      "865/1000, 0.01 sec, 10.39 sec\n",
      "Training set loss 1.5658093690872192\n",
      "Training set accuracy 0.4652799963951111\n",
      "Test set accuracy 0.4715999960899353\n",
      "-------------------------------\n",
      "866/1000, 0.01 sec, 10.40 sec\n",
      "Training set loss 1.5425300598144531\n",
      "Training set accuracy 0.3511999845504761\n",
      "Test set accuracy 0.35830000042915344\n",
      "-------------------------------\n",
      "867/1000, 0.01 sec, 10.41 sec\n",
      "Training set loss 1.5629558563232422\n",
      "Training set accuracy 0.46539998054504395\n",
      "Test set accuracy 0.47209998965263367\n",
      "-------------------------------\n",
      "868/1000, 0.01 sec, 10.43 sec\n",
      "Training set loss 1.5397887229919434\n",
      "Training set accuracy 0.35113999247550964\n",
      "Test set accuracy 0.3580999970436096\n",
      "-------------------------------\n",
      "869/1000, 0.01 sec, 10.44 sec\n",
      "Training set loss 1.5601059198379517\n",
      "Training set accuracy 0.4657999873161316\n",
      "Test set accuracy 0.47209998965263367\n",
      "-------------------------------\n",
      "870/1000, 0.01 sec, 10.45 sec\n",
      "Training set loss 1.5370486974716187\n",
      "Training set accuracy 0.3508799970149994\n",
      "Test set accuracy 0.35839998722076416\n",
      "-------------------------------\n",
      "871/1000, 0.01 sec, 10.47 sec\n",
      "Training set loss 1.5573346614837646\n",
      "Training set accuracy 0.46605998277664185\n",
      "Test set accuracy 0.4729999899864197\n",
      "-------------------------------\n",
      "872/1000, 0.01 sec, 10.48 sec\n",
      "Training set loss 1.5345102548599243\n",
      "Training set accuracy 0.3509799838066101\n",
      "Test set accuracy 0.3578999936580658\n",
      "-------------------------------\n",
      "873/1000, 0.01 sec, 10.49 sec\n",
      "Training set loss 1.5548001527786255\n",
      "Training set accuracy 0.46643999218940735\n",
      "Test set accuracy 0.47369998693466187\n",
      "-------------------------------\n",
      "874/1000, 0.01 sec, 10.50 sec\n",
      "Training set loss 1.53208589553833\n",
      "Training set accuracy 0.35113999247550964\n",
      "Test set accuracy 0.357699990272522\n",
      "-------------------------------\n",
      "875/1000, 0.01 sec, 10.52 sec\n",
      "Training set loss 1.5524827241897583\n",
      "Training set accuracy 0.46709999442100525\n",
      "Test set accuracy 0.4740999937057495\n",
      "-------------------------------\n",
      "876/1000, 0.01 sec, 10.53 sec\n",
      "Training set loss 1.5297960042953491\n",
      "Training set accuracy 0.3510199785232544\n",
      "Test set accuracy 0.35760000348091125\n",
      "-------------------------------\n",
      "877/1000, 0.01 sec, 10.54 sec\n",
      "Training set loss 1.550193190574646\n",
      "Training set accuracy 0.46751999855041504\n",
      "Test set accuracy 0.47450000047683716\n",
      "-------------------------------\n",
      "878/1000, 0.01 sec, 10.55 sec\n",
      "Training set loss 1.527503252029419\n",
      "Training set accuracy 0.350600004196167\n",
      "Test set accuracy 0.3572999835014343\n",
      "-------------------------------\n",
      "879/1000, 0.01 sec, 10.57 sec\n",
      "Training set loss 1.5481117963790894\n",
      "Training set accuracy 0.467960000038147\n",
      "Test set accuracy 0.4745999872684479\n",
      "-------------------------------\n",
      "880/1000, 0.01 sec, 10.58 sec\n",
      "Training set loss 1.5253524780273438\n",
      "Training set accuracy 0.35019999742507935\n",
      "Test set accuracy 0.3569999933242798\n",
      "-------------------------------\n",
      "881/1000, 0.01 sec, 10.59 sec\n",
      "Training set loss 1.5461012125015259\n",
      "Training set accuracy 0.4683799743652344\n",
      "Test set accuracy 0.47540000081062317\n",
      "-------------------------------\n",
      "882/1000, 0.01 sec, 10.60 sec\n",
      "Training set loss 1.5232137441635132\n",
      "Training set accuracy 0.34995999932289124\n",
      "Test set accuracy 0.3569999933242798\n",
      "-------------------------------\n",
      "883/1000, 0.01 sec, 10.61 sec\n",
      "Training set loss 1.5440118312835693\n",
      "Training set accuracy 0.46855998039245605\n",
      "Test set accuracy 0.4754999876022339\n",
      "-------------------------------\n",
      "884/1000, 0.01 sec, 10.63 sec\n",
      "Training set loss 1.520972728729248\n",
      "Training set accuracy 0.349839985370636\n",
      "Test set accuracy 0.3564999997615814\n",
      "-------------------------------\n",
      "885/1000, 0.01 sec, 10.64 sec\n",
      "Training set loss 1.5415607690811157\n",
      "Training set accuracy 0.4685799777507782\n",
      "Test set accuracy 0.47540000081062317\n",
      "-------------------------------\n",
      "886/1000, 0.01 sec, 10.65 sec\n",
      "Training set loss 1.5185331106185913\n",
      "Training set accuracy 0.3499400019645691\n",
      "Test set accuracy 0.3560999929904938\n",
      "-------------------------------\n",
      "887/1000, 0.01 sec, 10.66 sec\n",
      "Training set loss 1.538825273513794\n",
      "Training set accuracy 0.4690999984741211\n",
      "Test set accuracy 0.47589999437332153\n",
      "-------------------------------\n",
      "888/1000, 0.01 sec, 10.67 sec\n",
      "Training set loss 1.515859603881836\n",
      "Training set accuracy 0.34985998272895813\n",
      "Test set accuracy 0.3561999797821045\n",
      "-------------------------------\n",
      "889/1000, 0.01 sec, 10.69 sec\n",
      "Training set loss 1.5360033512115479\n",
      "Training set accuracy 0.46935999393463135\n",
      "Test set accuracy 0.4757999777793884\n",
      "-------------------------------\n",
      "890/1000, 0.01 sec, 10.70 sec\n",
      "Training set loss 1.513155221939087\n",
      "Training set accuracy 0.35001999139785767\n",
      "Test set accuracy 0.3560999929904938\n",
      "-------------------------------\n",
      "891/1000, 0.01 sec, 10.71 sec\n",
      "Training set loss 1.5332380533218384\n",
      "Training set accuracy 0.4695200026035309\n",
      "Test set accuracy 0.47669997811317444\n",
      "-------------------------------\n",
      "892/1000, 0.01 sec, 10.72 sec\n",
      "Training set loss 1.5105531215667725\n",
      "Training set accuracy 0.3498799800872803\n",
      "Test set accuracy 0.3561999797821045\n",
      "-------------------------------\n",
      "893/1000, 0.01 sec, 10.73 sec\n",
      "Training set loss 1.5305215120315552\n",
      "Training set accuracy 0.46977999806404114\n",
      "Test set accuracy 0.4765999913215637\n",
      "-------------------------------\n",
      "894/1000, 0.01 sec, 10.74 sec\n",
      "Training set loss 1.5078991651535034\n",
      "Training set accuracy 0.3499400019645691\n",
      "Test set accuracy 0.35580000281333923\n",
      "-------------------------------\n",
      "895/1000, 0.01 sec, 10.75 sec\n",
      "Training set loss 1.52772057056427\n",
      "Training set accuracy 0.46998000144958496\n",
      "Test set accuracy 0.4773999750614166\n",
      "-------------------------------\n",
      "896/1000, 0.01 sec, 10.76 sec\n",
      "Training set loss 1.5052495002746582\n",
      "Training set accuracy 0.3498999774456024\n",
      "Test set accuracy 0.35580000281333923\n",
      "-------------------------------\n",
      "897/1000, 0.01 sec, 10.78 sec\n",
      "Training set loss 1.5251343250274658\n",
      "Training set accuracy 0.47039997577667236\n",
      "Test set accuracy 0.4778999984264374\n",
      "-------------------------------\n",
      "898/1000, 0.01 sec, 10.79 sec\n",
      "Training set loss 1.50282621383667\n",
      "Training set accuracy 0.34967997670173645\n",
      "Test set accuracy 0.3560999929904938\n",
      "-------------------------------\n",
      "899/1000, 0.01 sec, 10.80 sec\n",
      "Training set loss 1.5226502418518066\n",
      "Training set accuracy 0.4705599844455719\n",
      "Test set accuracy 0.47849997878074646\n",
      "-------------------------------\n",
      "900/1000, 0.01 sec, 10.81 sec\n",
      "Training set loss 1.500622034072876\n",
      "Training set accuracy 0.34946000576019287\n",
      "Test set accuracy 0.35599997639656067\n",
      "-------------------------------\n",
      "901/1000, 0.01 sec, 10.82 sec\n",
      "Training set loss 1.520641565322876\n",
      "Training set accuracy 0.4708999991416931\n",
      "Test set accuracy 0.4790000021457672\n",
      "-------------------------------\n",
      "902/1000, 0.01 sec, 10.83 sec\n",
      "Training set loss 1.498581886291504\n",
      "Training set accuracy 0.3492799997329712\n",
      "Test set accuracy 0.3555999994277954\n",
      "-------------------------------\n",
      "903/1000, 0.01 sec, 10.84 sec\n",
      "Training set loss 1.518500566482544\n",
      "Training set accuracy 0.4711399972438812\n",
      "Test set accuracy 0.4795999825000763\n",
      "-------------------------------\n",
      "904/1000, 0.01 sec, 10.86 sec\n",
      "Training set loss 1.4965342283248901\n",
      "Training set accuracy 0.3490999937057495\n",
      "Test set accuracy 0.35569998621940613\n",
      "-------------------------------\n",
      "905/1000, 0.01 sec, 10.87 sec\n",
      "Training set loss 1.5165467262268066\n",
      "Training set accuracy 0.47123998403549194\n",
      "Test set accuracy 0.47939997911453247\n",
      "-------------------------------\n",
      "906/1000, 0.01 sec, 10.88 sec\n",
      "Training set loss 1.4945480823516846\n",
      "Training set accuracy 0.3489999771118164\n",
      "Test set accuracy 0.3553999960422516\n",
      "-------------------------------\n",
      "907/1000, 0.01 sec, 10.89 sec\n",
      "Training set loss 1.5145596265792847\n",
      "Training set accuracy 0.47151997685432434\n",
      "Test set accuracy 0.47999998927116394\n",
      "-------------------------------\n",
      "908/1000, 0.01 sec, 10.90 sec\n",
      "Training set loss 1.4926159381866455\n",
      "Training set accuracy 0.3489599823951721\n",
      "Test set accuracy 0.3552999794483185\n",
      "-------------------------------\n",
      "909/1000, 0.01 sec, 10.91 sec\n",
      "Training set loss 1.512682318687439\n",
      "Training set accuracy 0.471699982881546\n",
      "Test set accuracy 0.47939997911453247\n",
      "-------------------------------\n",
      "910/1000, 0.01 sec, 10.92 sec\n",
      "Training set loss 1.4906668663024902\n",
      "Training set accuracy 0.34897997975349426\n",
      "Test set accuracy 0.3547999858856201\n",
      "-------------------------------\n",
      "911/1000, 0.01 sec, 10.94 sec\n",
      "Training set loss 1.5106581449508667\n",
      "Training set accuracy 0.4719799757003784\n",
      "Test set accuracy 0.47909998893737793\n",
      "-------------------------------\n",
      "912/1000, 0.01 sec, 10.95 sec\n",
      "Training set loss 1.4886988401412964\n",
      "Training set accuracy 0.3490400016307831\n",
      "Test set accuracy 0.3547999858856201\n",
      "-------------------------------\n",
      "913/1000, 0.01 sec, 10.96 sec\n",
      "Training set loss 1.5086590051651\n",
      "Training set accuracy 0.4723399877548218\n",
      "Test set accuracy 0.4795999825000763\n",
      "-------------------------------\n",
      "914/1000, 0.01 sec, 10.97 sec\n",
      "Training set loss 1.486690640449524\n",
      "Training set accuracy 0.3490599989891052\n",
      "Test set accuracy 0.35519999265670776\n",
      "-------------------------------\n",
      "915/1000, 0.01 sec, 10.98 sec\n",
      "Training set loss 1.506667971611023\n",
      "Training set accuracy 0.47283998131752014\n",
      "Test set accuracy 0.4795999825000763\n",
      "-------------------------------\n",
      "916/1000, 0.01 sec, 11.00 sec\n",
      "Training set loss 1.4845898151397705\n",
      "Training set accuracy 0.34902000427246094\n",
      "Test set accuracy 0.3545999825000763\n",
      "-------------------------------\n",
      "917/1000, 0.01 sec, 11.01 sec\n",
      "Training set loss 1.5045771598815918\n",
      "Training set accuracy 0.4728999733924866\n",
      "Test set accuracy 0.4803999960422516\n",
      "-------------------------------\n",
      "918/1000, 0.01 sec, 11.02 sec\n",
      "Training set loss 1.482473373413086\n",
      "Training set accuracy 0.34902000427246094\n",
      "Test set accuracy 0.3545999825000763\n",
      "-------------------------------\n",
      "919/1000, 0.01 sec, 11.03 sec\n",
      "Training set loss 1.5022727251052856\n",
      "Training set accuracy 0.47311997413635254\n",
      "Test set accuracy 0.4803999960422516\n",
      "-------------------------------\n",
      "920/1000, 0.01 sec, 11.04 sec\n",
      "Training set loss 1.4801450967788696\n",
      "Training set accuracy 0.3491399884223938\n",
      "Test set accuracy 0.35429999232292175\n",
      "-------------------------------\n",
      "921/1000, 0.01 sec, 11.05 sec\n",
      "Training set loss 1.499970555305481\n",
      "Training set accuracy 0.4732399880886078\n",
      "Test set accuracy 0.4804999828338623\n",
      "-------------------------------\n",
      "922/1000, 0.01 sec, 11.07 sec\n",
      "Training set loss 1.477899193763733\n",
      "Training set accuracy 0.3490599989891052\n",
      "Test set accuracy 0.3536999821662903\n",
      "-------------------------------\n",
      "923/1000, 0.01 sec, 11.08 sec\n",
      "Training set loss 1.4977396726608276\n",
      "Training set accuracy 0.47336000204086304\n",
      "Test set accuracy 0.4803999960422516\n",
      "-------------------------------\n",
      "924/1000, 0.01 sec, 11.09 sec\n",
      "Training set loss 1.4758104085922241\n",
      "Training set accuracy 0.34902000427246094\n",
      "Test set accuracy 0.353300005197525\n",
      "-------------------------------\n",
      "925/1000, 0.01 sec, 11.10 sec\n",
      "Training set loss 1.4957693815231323\n",
      "Training set accuracy 0.47369998693466187\n",
      "Test set accuracy 0.48159998655319214\n",
      "-------------------------------\n",
      "926/1000, 0.01 sec, 11.11 sec\n",
      "Training set loss 1.4738863706588745\n",
      "Training set accuracy 0.34891998767852783\n",
      "Test set accuracy 0.35359999537467957\n",
      "-------------------------------\n",
      "927/1000, 0.01 sec, 11.12 sec\n",
      "Training set loss 1.4939017295837402\n",
      "Training set accuracy 0.47411999106407166\n",
      "Test set accuracy 0.48169997334480286\n",
      "-------------------------------\n",
      "928/1000, 0.01 sec, 11.13 sec\n",
      "Training set loss 1.4719090461730957\n",
      "Training set accuracy 0.3488599956035614\n",
      "Test set accuracy 0.35349997878074646\n",
      "-------------------------------\n",
      "929/1000, 0.01 sec, 11.15 sec\n",
      "Training set loss 1.4918440580368042\n",
      "Training set accuracy 0.4745599925518036\n",
      "Test set accuracy 0.4822999835014343\n",
      "-------------------------------\n",
      "930/1000, 0.01 sec, 11.16 sec\n",
      "Training set loss 1.4698030948638916\n",
      "Training set accuracy 0.34891998767852783\n",
      "Test set accuracy 0.3531999886035919\n",
      "-------------------------------\n",
      "931/1000, 0.01 sec, 11.17 sec\n",
      "Training set loss 1.4898905754089355\n",
      "Training set accuracy 0.47485998272895813\n",
      "Test set accuracy 0.48319998383522034\n",
      "-------------------------------\n",
      "932/1000, 0.01 sec, 11.18 sec\n",
      "Training set loss 1.467941164970398\n",
      "Training set accuracy 0.3489999771118164\n",
      "Test set accuracy 0.3531000018119812\n",
      "-------------------------------\n",
      "933/1000, 0.01 sec, 11.19 sec\n",
      "Training set loss 1.4879443645477295\n",
      "Training set accuracy 0.4752599895000458\n",
      "Test set accuracy 0.48409998416900635\n",
      "-------------------------------\n",
      "934/1000, 0.01 sec, 11.20 sec\n",
      "Training set loss 1.465859293937683\n",
      "Training set accuracy 0.34873998165130615\n",
      "Test set accuracy 0.3531000018119812\n",
      "-------------------------------\n",
      "935/1000, 0.01 sec, 11.21 sec\n",
      "Training set loss 1.4858609437942505\n",
      "Training set accuracy 0.4754999876022339\n",
      "Test set accuracy 0.48479998111724854\n",
      "-------------------------------\n",
      "936/1000, 0.01 sec, 11.23 sec\n",
      "Training set loss 1.4637471437454224\n",
      "Training set accuracy 0.34860000014305115\n",
      "Test set accuracy 0.35269999504089355\n",
      "-------------------------------\n",
      "937/1000, 0.01 sec, 11.24 sec\n",
      "Training set loss 1.483701229095459\n",
      "Training set accuracy 0.47557997703552246\n",
      "Test set accuracy 0.48559999465942383\n",
      "-------------------------------\n",
      "938/1000, 0.01 sec, 11.25 sec\n",
      "Training set loss 1.461524486541748\n",
      "Training set accuracy 0.3485199809074402\n",
      "Test set accuracy 0.35269999504089355\n",
      "-------------------------------\n",
      "939/1000, 0.01 sec, 11.26 sec\n",
      "Training set loss 1.4813965559005737\n",
      "Training set accuracy 0.4760199785232544\n",
      "Test set accuracy 0.4860000014305115\n",
      "-------------------------------\n",
      "940/1000, 0.01 sec, 11.28 sec\n",
      "Training set loss 1.4592727422714233\n",
      "Training set accuracy 0.3484799861907959\n",
      "Test set accuracy 0.3522999882698059\n",
      "-------------------------------\n",
      "941/1000, 0.01 sec, 11.29 sec\n",
      "Training set loss 1.479002594947815\n",
      "Training set accuracy 0.47637999057769775\n",
      "Test set accuracy 0.4860000014305115\n",
      "-------------------------------\n",
      "942/1000, 0.01 sec, 11.30 sec\n",
      "Training set loss 1.457026720046997\n",
      "Training set accuracy 0.34845998883247375\n",
      "Test set accuracy 0.3520999848842621\n",
      "-------------------------------\n",
      "943/1000, 0.01 sec, 11.31 sec\n",
      "Training set loss 1.4766793251037598\n",
      "Training set accuracy 0.4765399992465973\n",
      "Test set accuracy 0.4860000014305115\n",
      "-------------------------------\n",
      "944/1000, 0.01 sec, 11.32 sec\n",
      "Training set loss 1.4546873569488525\n",
      "Training set accuracy 0.34853997826576233\n",
      "Test set accuracy 0.352400004863739\n",
      "-------------------------------\n",
      "945/1000, 0.01 sec, 11.33 sec\n",
      "Training set loss 1.4742902517318726\n",
      "Training set accuracy 0.4769599735736847\n",
      "Test set accuracy 0.4860999882221222\n",
      "-------------------------------\n",
      "946/1000, 0.01 sec, 11.34 sec\n",
      "Training set loss 1.4523375034332275\n",
      "Training set accuracy 0.3486199975013733\n",
      "Test set accuracy 0.3520999848842621\n",
      "-------------------------------\n",
      "947/1000, 0.01 sec, 11.36 sec\n",
      "Training set loss 1.4719109535217285\n",
      "Training set accuracy 0.4772599935531616\n",
      "Test set accuracy 0.48659998178482056\n",
      "-------------------------------\n",
      "948/1000, 0.01 sec, 11.37 sec\n",
      "Training set loss 1.4501129388809204\n",
      "Training set accuracy 0.3485199809074402\n",
      "Test set accuracy 0.35169997811317444\n",
      "-------------------------------\n",
      "949/1000, 0.01 sec, 11.38 sec\n",
      "Training set loss 1.4696046113967896\n",
      "Training set accuracy 0.47742000222206116\n",
      "Test set accuracy 0.4867999851703644\n",
      "-------------------------------\n",
      "950/1000, 0.01 sec, 11.39 sec\n",
      "Training set loss 1.4478635787963867\n",
      "Training set accuracy 0.3484399914741516\n",
      "Test set accuracy 0.35179999470710754\n",
      "-------------------------------\n",
      "951/1000, 0.01 sec, 11.40 sec\n",
      "Training set loss 1.467222809791565\n",
      "Training set accuracy 0.47769999504089355\n",
      "Test set accuracy 0.487199991941452\n",
      "-------------------------------\n",
      "952/1000, 0.01 sec, 11.42 sec\n",
      "Training set loss 1.4455482959747314\n",
      "Training set accuracy 0.3486599922180176\n",
      "Test set accuracy 0.3513999879360199\n",
      "-------------------------------\n",
      "953/1000, 0.01 sec, 11.43 sec\n",
      "Training set loss 1.4647974967956543\n",
      "Training set accuracy 0.4779999852180481\n",
      "Test set accuracy 0.48749998211860657\n",
      "-------------------------------\n",
      "954/1000, 0.01 sec, 11.44 sec\n",
      "Training set loss 1.4432460069656372\n",
      "Training set accuracy 0.34856000542640686\n",
      "Test set accuracy 0.35199999809265137\n",
      "-------------------------------\n",
      "955/1000, 0.01 sec, 11.45 sec\n",
      "Training set loss 1.4621293544769287\n",
      "Training set accuracy 0.4782799780368805\n",
      "Test set accuracy 0.4878000020980835\n",
      "-------------------------------\n",
      "956/1000, 0.01 sec, 11.46 sec\n",
      "Training set loss 1.4407099485397339\n",
      "Training set accuracy 0.34849998354911804\n",
      "Test set accuracy 0.35189998149871826\n",
      "-------------------------------\n",
      "957/1000, 0.01 sec, 11.48 sec\n",
      "Training set loss 1.4595086574554443\n",
      "Training set accuracy 0.4783799946308136\n",
      "Test set accuracy 0.4878999888896942\n",
      "-------------------------------\n",
      "958/1000, 0.01 sec, 11.49 sec\n",
      "Training set loss 1.4382314682006836\n",
      "Training set accuracy 0.3488200008869171\n",
      "Test set accuracy 0.3522000014781952\n",
      "-------------------------------\n",
      "959/1000, 0.01 sec, 11.50 sec\n",
      "Training set loss 1.4568328857421875\n",
      "Training set accuracy 0.4787599742412567\n",
      "Test set accuracy 0.4884999990463257\n",
      "-------------------------------\n",
      "960/1000, 0.01 sec, 11.51 sec\n",
      "Training set loss 1.435782790184021\n",
      "Training set accuracy 0.34887999296188354\n",
      "Test set accuracy 0.35259997844696045\n",
      "-------------------------------\n",
      "961/1000, 0.01 sec, 11.52 sec\n",
      "Training set loss 1.4543203115463257\n",
      "Training set accuracy 0.47881999611854553\n",
      "Test set accuracy 0.48909997940063477\n",
      "-------------------------------\n",
      "962/1000, 0.01 sec, 11.53 sec\n",
      "Training set loss 1.4335436820983887\n",
      "Training set accuracy 0.3488200008869171\n",
      "Test set accuracy 0.35249999165534973\n",
      "-------------------------------\n",
      "963/1000, 0.01 sec, 11.54 sec\n",
      "Training set loss 1.4519810676574707\n",
      "Training set accuracy 0.47905999422073364\n",
      "Test set accuracy 0.4894999861717224\n",
      "-------------------------------\n",
      "964/1000, 0.01 sec, 11.56 sec\n",
      "Training set loss 1.431436538696289\n",
      "Training set accuracy 0.34869998693466187\n",
      "Test set accuracy 0.3527999818325043\n",
      "-------------------------------\n",
      "965/1000, 0.01 sec, 11.57 sec\n",
      "Training set loss 1.4499461650848389\n",
      "Training set accuracy 0.47947999835014343\n",
      "Test set accuracy 0.48969998955726624\n",
      "-------------------------------\n",
      "966/1000, 0.01 sec, 11.58 sec\n",
      "Training set loss 1.429601788520813\n",
      "Training set accuracy 0.34860000014305115\n",
      "Test set accuracy 0.3529999852180481\n",
      "-------------------------------\n",
      "967/1000, 0.01 sec, 11.59 sec\n",
      "Training set loss 1.4481821060180664\n",
      "Training set accuracy 0.4799000024795532\n",
      "Test set accuracy 0.48979997634887695\n",
      "-------------------------------\n",
      "968/1000, 0.01 sec, 11.60 sec\n",
      "Training set loss 1.4278112649917603\n",
      "Training set accuracy 0.34849998354911804\n",
      "Test set accuracy 0.3531000018119812\n",
      "-------------------------------\n",
      "969/1000, 0.01 sec, 11.61 sec\n",
      "Training set loss 1.446410894393921\n",
      "Training set accuracy 0.4801599979400635\n",
      "Test set accuracy 0.48989999294281006\n",
      "-------------------------------\n",
      "970/1000, 0.01 sec, 11.62 sec\n",
      "Training set loss 1.4260337352752686\n",
      "Training set accuracy 0.3485199809074402\n",
      "Test set accuracy 0.3528999984264374\n",
      "-------------------------------\n",
      "971/1000, 0.01 sec, 11.63 sec\n",
      "Training set loss 1.4446454048156738\n",
      "Training set accuracy 0.4805999994277954\n",
      "Test set accuracy 0.4902999997138977\n",
      "-------------------------------\n",
      "972/1000, 0.01 sec, 11.65 sec\n",
      "Training set loss 1.4242846965789795\n",
      "Training set accuracy 0.3482799828052521\n",
      "Test set accuracy 0.35249999165534973\n",
      "-------------------------------\n",
      "973/1000, 0.01 sec, 11.66 sec\n",
      "Training set loss 1.4429714679718018\n",
      "Training set accuracy 0.4808799922466278\n",
      "Test set accuracy 0.49079999327659607\n",
      "-------------------------------\n",
      "974/1000, 0.01 sec, 11.67 sec\n",
      "Training set loss 1.4225358963012695\n",
      "Training set accuracy 0.3482999801635742\n",
      "Test set accuracy 0.3520999848842621\n",
      "-------------------------------\n",
      "975/1000, 0.01 sec, 11.68 sec\n",
      "Training set loss 1.4411665201187134\n",
      "Training set accuracy 0.4810999929904938\n",
      "Test set accuracy 0.48979997634887695\n",
      "-------------------------------\n",
      "976/1000, 0.01 sec, 11.69 sec\n",
      "Training set loss 1.420754075050354\n",
      "Training set accuracy 0.3483799993991852\n",
      "Test set accuracy 0.3513000011444092\n",
      "-------------------------------\n",
      "977/1000, 0.01 sec, 11.70 sec\n",
      "Training set loss 1.4393806457519531\n",
      "Training set accuracy 0.4816399812698364\n",
      "Test set accuracy 0.4900999963283539\n",
      "-------------------------------\n",
      "978/1000, 0.01 sec, 11.72 sec\n",
      "Training set loss 1.4189143180847168\n",
      "Training set accuracy 0.34856000542640686\n",
      "Test set accuracy 0.35099998116493225\n",
      "-------------------------------\n",
      "979/1000, 0.01 sec, 11.73 sec\n",
      "Training set loss 1.437507152557373\n",
      "Training set accuracy 0.48201999068260193\n",
      "Test set accuracy 0.4901999831199646\n",
      "-------------------------------\n",
      "980/1000, 0.01 sec, 11.74 sec\n",
      "Training set loss 1.417048454284668\n",
      "Training set accuracy 0.3483999967575073\n",
      "Test set accuracy 0.3504999876022339\n",
      "-------------------------------\n",
      "981/1000, 0.01 sec, 11.75 sec\n",
      "Training set loss 1.4356014728546143\n",
      "Training set accuracy 0.4823399782180786\n",
      "Test set accuracy 0.49049997329711914\n",
      "-------------------------------\n",
      "982/1000, 0.01 sec, 11.76 sec\n",
      "Training set loss 1.415144681930542\n",
      "Training set accuracy 0.34863999485969543\n",
      "Test set accuracy 0.35009998083114624\n",
      "-------------------------------\n",
      "983/1000, 0.01 sec, 11.78 sec\n",
      "Training set loss 1.4335546493530273\n",
      "Training set accuracy 0.4827599823474884\n",
      "Test set accuracy 0.49069997668266296\n",
      "-------------------------------\n",
      "984/1000, 0.01 sec, 11.79 sec\n",
      "Training set loss 1.4129743576049805\n",
      "Training set accuracy 0.3484799861907959\n",
      "Test set accuracy 0.3498999774456024\n",
      "-------------------------------\n",
      "985/1000, 0.01 sec, 11.80 sec\n",
      "Training set loss 1.4313993453979492\n",
      "Training set accuracy 0.4830999970436096\n",
      "Test set accuracy 0.4910999834537506\n",
      "-------------------------------\n",
      "986/1000, 0.01 sec, 11.81 sec\n",
      "Training set loss 1.4108084440231323\n",
      "Training set accuracy 0.34860000014305115\n",
      "Test set accuracy 0.3499999940395355\n",
      "-------------------------------\n",
      "987/1000, 0.01 sec, 11.82 sec\n",
      "Training set loss 1.4292203187942505\n",
      "Training set accuracy 0.4836599826812744\n",
      "Test set accuracy 0.4916999936103821\n",
      "-------------------------------\n",
      "988/1000, 0.01 sec, 11.84 sec\n",
      "Training set loss 1.4087493419647217\n",
      "Training set accuracy 0.34853997826576233\n",
      "Test set accuracy 0.3497999906539917\n",
      "-------------------------------\n",
      "989/1000, 0.01 sec, 11.85 sec\n",
      "Training set loss 1.4270631074905396\n",
      "Training set accuracy 0.4837599992752075\n",
      "Test set accuracy 0.49239999055862427\n",
      "-------------------------------\n",
      "990/1000, 0.01 sec, 11.86 sec\n",
      "Training set loss 1.406740427017212\n",
      "Training set accuracy 0.3486199975013733\n",
      "Test set accuracy 0.35009998083114624\n",
      "-------------------------------\n",
      "991/1000, 0.01 sec, 11.87 sec\n",
      "Training set loss 1.4250311851501465\n",
      "Training set accuracy 0.48433998227119446\n",
      "Test set accuracy 0.4932999908924103\n",
      "-------------------------------\n",
      "992/1000, 0.01 sec, 11.88 sec\n",
      "Training set loss 1.4048837423324585\n",
      "Training set accuracy 0.3486599922180176\n",
      "Test set accuracy 0.3504999876022339\n",
      "-------------------------------\n",
      "993/1000, 0.01 sec, 11.89 sec\n",
      "Training set loss 1.4231406450271606\n",
      "Training set accuracy 0.48429998755455017\n",
      "Test set accuracy 0.493399977684021\n",
      "-------------------------------\n",
      "994/1000, 0.01 sec, 11.91 sec\n",
      "Training set loss 1.4029861688613892\n",
      "Training set accuracy 0.34860000014305115\n",
      "Test set accuracy 0.350600004196167\n",
      "-------------------------------\n",
      "995/1000, 0.01 sec, 11.92 sec\n",
      "Training set loss 1.4210715293884277\n",
      "Training set accuracy 0.4844599962234497\n",
      "Test set accuracy 0.49390000104904175\n",
      "-------------------------------\n",
      "996/1000, 0.01 sec, 11.93 sec\n",
      "Training set loss 1.4010255336761475\n",
      "Training set accuracy 0.34860000014305115\n",
      "Test set accuracy 0.3507999777793884\n",
      "-------------------------------\n",
      "997/1000, 0.01 sec, 11.94 sec\n",
      "Training set loss 1.419067144393921\n",
      "Training set accuracy 0.48465999960899353\n",
      "Test set accuracy 0.4940999746322632\n",
      "-------------------------------\n",
      "998/1000, 0.01 sec, 11.95 sec\n",
      "Training set loss 1.3990874290466309\n",
      "Training set accuracy 0.348580002784729\n",
      "Test set accuracy 0.3504999876022339\n",
      "-------------------------------\n",
      "999/1000, 0.01 sec, 11.96 sec\n",
      "Training set loss 1.4171209335327148\n",
      "Training set accuracy 0.48495998978614807\n",
      "Test set accuracy 0.4941999912261963\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(params, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[50000] i32 x∈[0, 9] μ=4.449 σ=2.891 gpu:0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jax.scipy.special import logsumexp\n",
    "# # $Sofmax(x_1,\\cdots, x_n)= \\frac{e^{x_i}}{\\sum^{n}_1 e^{x_i}}$\n",
    "# # $LogSoftmax(x_1,\\cdots, x_n)= \\log \\frac{e^{x_i}}{\\sum^{n}_1 e^{x_i}} = x_i - \\log \\sum^{n}_1 e^{x_i}$\n",
    "# # $LogSumExp(x_1,\\cdots, x_n)= \\log \\sum^{n}_1 e^{x_i}$\n",
    "# def logsoftmax(logits): return logits + logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsoftmax(preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(fastcore.transform.Transform):\n",
    "  \"A transform to apply linear transformation to the input\"\n",
    "  def __init__(self, w,b):\n",
    "    self.w, self.b = w,b\n",
    "  def encodes(self,x):\n",
    "    \"x: input data\"\n",
    "    return x@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initLinear(key, n_inp, n_out, scale=1)->Linear:\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n_inp, n_out)), scale * random.normal(b_key, (n_out,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,\n",
       " 784,\n",
       " Array i32 gpu:0 10,\n",
       " Array[784, 50] n=39200 x∈[-3.969, 4.653] μ=0.001 σ=1.008 gpu:0,\n",
       " Array[50] x∈[-2.799, 1.671] μ=-0.163 σ=1.007 gpu:0,\n",
       " Array[50, 1] x∈[-2.454, 2.309] μ=-0.180 σ=1.006 gpu:0,\n",
       " Array[1] gpu:0 [0.195])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50\n",
    "w1,b1 =initLinear(random.PRNGKey(42), m, nh)\n",
    "w2,b2 =initLinear(random.PRNGKey(43), nh,1)\n",
    "n,m,c,w1,b1,w2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[50000, 50] n=2500000 x∈[-51.121, 54.617] μ=0.504 σ=8.873 gpu:0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fastcore.transform.Pipeline([Linear(w1,b1)])\n",
    "model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fastcore.transform.Transform\n",
    "def ReLU(x): return jnp.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array[50000, 784] n=39200000 x∈[0., 0.996] μ=0.130 σ=0.307 gpu:0,\n",
       " Array[50000, 1] x∈[-171.777, 84.371] μ=-39.972 σ=28.197 gpu:0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fastcore.transform.Pipeline([Linear(w1,b1), ReLU, Linear(w2,b2)])\n",
    "x_train, model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(params, x, y):  \n",
    "  error, grads = value_and_grad(loss)(params, x, y)\n",
    "  return error, [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array gpu:0 2.793e+03"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(model(x_train), y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_stateless_apply(params, x):\n",
    "  return params['w'] * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state which holds the parameters for the transformed function.\n",
    "# get_param uses this to know where to get params from.\n",
    "current_params = []\n",
    "\n",
    "def transform(f):\n",
    "\n",
    "  def apply_f(params, *args, **kwargs):\n",
    "    current_params.append(params)\n",
    "    outs = f(*args, **kwargs)\n",
    "    current_params.pop()\n",
    "    return outs\n",
    "\n",
    "  return apply_f\n",
    "\n",
    "\n",
    "def get_param(identifier):\n",
    "  return current_params[-1][identifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': 5}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(w=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = dict(w=5)\n",
    "my_stateless_apply(params, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule:\n",
    "  def apply(self, x):\n",
    "    return get_param('w') * x\n",
    "\n",
    "transform(MyModule().apply)(params, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[10, 5] n=50 x∈[4.000, 4.000] μ=4.000 σ=0. gpu:0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear(x):\n",
    "  return x @ get_param('w') + get_param('b')\n",
    "\n",
    "params = dict(w=jnp.ones((3, 5)), b=jnp.ones((5,)))\n",
    "apply = transform(linear)\n",
    "\n",
    "jax.jit(apply)(params, jnp.ones((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict, Callable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we're tracking more than just the current params,\n",
    "# we introduce the concept of a frame as the object that holds\n",
    "# state during a transformed execution.\n",
    "frame_stack = []\n",
    "frame_stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame(NamedTuple):\n",
    "  \"\"\"Tracks what's going on during a call of a transformed function.\"\"\"\n",
    "  params: Dict[str, jnp.ndarray]\n",
    "  is_initialising: bool = False\n",
    "\n",
    "def current_frame():\n",
    "  return frame_stack[-1]\n",
    "\n",
    "class Transformed(NamedTuple):\n",
    "  init: Callable\n",
    "  apply: Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating the code here just to remind me \n",
    "def transform(f) -> Transformed:\n",
    "\n",
    "  def init_f(*args, **kwargs):\n",
    "    frame_stack.append(Frame({}, is_initialising=True))\n",
    "    f(*args, **kwargs)\n",
    "    frame = frame_stack.pop()\n",
    "    return frame.params\n",
    "\n",
    "  def apply_f(params, *args, **kwargs):\n",
    "    frame_stack.append(Frame(params))\n",
    "    outs = f(*args, **kwargs)\n",
    "    frame_stack.pop()\n",
    "    return outs\n",
    "\n",
    "  return Transformed(init_f, apply_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_param(identifier, shape):\n",
    "  if current_frame().is_initialising:\n",
    "    current_frame().params[identifier] = np.random.normal(size=shape)\n",
    "\n",
    "  return current_frame().params[identifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make printing parameters a little more readable\n",
    "def parameter_shapes(params):\n",
    "  return jax.tree_util.tree_map(lambda p: p.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear:\n",
    "  def __init__(self, width):\n",
    "    self._width = width\n",
    "\n",
    "  def __call__(self, x):\n",
    "    w = get_param('w', shape=(x.shape[-1], self._width))\n",
    "    b = get_param('b', shape=(self._width,))\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "  \u001b[0;32mdef\u001b[0m \u001b[0minit_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mframe_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_initialising\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_163207/2668621232.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init, apply = transform(Linear(4))\n",
    "init??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "  \u001b[0;32mdef\u001b[0m \u001b[0mapply_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mframe_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mframe_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_163207/2668621232.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "apply??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mNamedTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Typed version of namedtuple.\n",
      "\n",
      "Usage in Python versions >= 3.6::\n",
      "\n",
      "    class Employee(NamedTuple):\n",
      "        name: str\n",
      "        id: int\n",
      "\n",
      "This is equivalent to::\n",
      "\n",
      "    Employee = collections.namedtuple('Employee', ['name', 'id'])\n",
      "\n",
      "The resulting class has an extra __annotations__ attribute, giving a\n",
      "dict that maps field names to types.  (The field names are also in\n",
      "the _fields attribute, which is part of the namedtuple API.)\n",
      "Alternative equivalent keyword syntax is also accepted::\n",
      "\n",
      "    Employee = NamedTuple('Employee', name=str, id=int)\n",
      "\n",
      "In Python versions <= 3.5 use::\n",
      "\n",
      "    Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.miniconda3/envs/py39/lib/python3.9/typing.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "NamedTuple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Employee(name='George', id=42)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Employee(NamedTuple):\n",
    "    name: str\n",
    "    id: int\n",
    "\n",
    "g = Employee('George', 42)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[2, 3] n=6 x∈[1.000, 1.000] μ=1.000 σ=0. gpu:0 [[1.000, 1.000, 1.000], [1.000, 1.000, 1.000]]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = jnp.ones((2, 3))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': (4,), 'w': (3, 4)}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = init(data)\n",
    "parameter_shapes(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[2, 4] n=8 x∈[-0.827, 3.564] μ=1.658 σ=1.599 gpu:0 [[1.593, 3.564, 2.304, -0.827], [1.593, 3.564, 2.304, -0.827]]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(params, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Frame(params={}, is_initialising=True, module_counts=defaultdict(<function Frame.<lambda>.<locals>.<lambda> at 0x7f6e14c49670>, {}), call_stack=[])]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_stack.append(Frame({}, is_initialising=True))\n",
    "frame_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses # adds some dunder methods like __init__ and __repr__\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclasses.dataclass\n",
    "class Frame:\n",
    "  \"\"\"Tracks what's going on during a call of a transformed function.\"\"\"\n",
    "  params: Dict[str, jnp.ndarray]\n",
    "  is_initialising: bool = False\n",
    "\n",
    "  # Keeps track of how many modules of each class have been created so far.\n",
    "  # Used to assign new modules unique names.\n",
    "  module_counts: Dict[str, int] = dataclasses.field(\n",
    "      default_factory=lambda: collections.defaultdict(lambda: 0))\n",
    "\n",
    "  # Keeps track of the entire path to the current module method call.\n",
    "  # Module methods, when called, will add themselves to this stack.\n",
    "  # Used to give each parameter a unique name corresponding to the\n",
    "  # method scope it is in.\n",
    "  call_stack: list = dataclasses.field(default_factory=list)\n",
    "\n",
    "  def create_param_path(self, identifier) -> str:\n",
    "    \"\"\"Creates a unique path for this param.\"\"\"\n",
    "    return '/'.join(['~'] + self.call_stack + [identifier])\n",
    "\n",
    "  def create_unique_module_name(self, module_name: str) -> str:\n",
    "    \"\"\"Assigns a unique name to the module by appending its number to its name.\"\"\"\n",
    "    number = self.module_counts[module_name]\n",
    "    self.module_counts[module_name] += 1\n",
    "    return f\"{module_name}_{number}\"\n",
    "\n",
    "frame_stack = []\n",
    "def current_frame():\n",
    "  return frame_stack[-1] if len(frame_stack)>0 else []\n",
    "\n",
    "current_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "  def __init__(self):\n",
    "    # Assign a unique (for the current `transform` call)\n",
    "    # name to this instance of the module.\n",
    "    self._unique_name = current_frame().create_unique_module_name(\n",
    "        self.__class__.__name__)\n",
    "\n",
    "def module_method(f):\n",
    "  \"\"\"A decorator for Module methods.\"\"\"\n",
    "\n",
    "  def wrapped(self, *args, **kwargs):\n",
    "    \"\"\"A version of f that lets the frame know it's being called.\"\"\"\n",
    "    # Self is the instance to which this method is attached.\n",
    "    module_name = self._unique_name\n",
    "    call_stack = current_frame().call_stack\n",
    "    call_stack.append(module_name)\n",
    "    call_stack.append(f.__name__)\n",
    "    outs = f(self, *args, **kwargs)\n",
    "    assert call_stack.pop() == f.__name__\n",
    "    assert call_stack.pop() == module_name\n",
    "    return outs\n",
    "  return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(f) -> Transformed:\n",
    "\n",
    "  def init_f(*args, **kwargs):\n",
    "    frame_stack.append(Frame({}, is_initialising=True))\n",
    "    f(*args, **kwargs)\n",
    "    frame = frame_stack.pop()\n",
    "    return frame.params\n",
    "\n",
    "  def apply_f(params, *args, **kwargs):\n",
    "    frame_stack.append(Frame(params))\n",
    "    outs = f(*args, **kwargs)\n",
    "    frame_stack.pop()\n",
    "    return outs\n",
    "\n",
    "  return Transformed(init_f, apply_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_param(identifier, shape):\n",
    "  frame = current_frame()\n",
    "  param_path = frame.create_param_path(identifier)\n",
    "\n",
    "  if frame.is_initialising:\n",
    "    frame.params[param_path] = np.random.normal(size=shape)\n",
    "\n",
    "  return frame.params[param_path]\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "\n",
    "  def __init__(self, width):\n",
    "    super().__init__()\n",
    "    self._width = width\n",
    "    \n",
    "  @module_method  \n",
    "  def __call__(self, x):\n",
    "    w = get_param('w', shape=(x.shape[-1], self._width))\n",
    "    b = get_param('b', shape=(self._width,))\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'~/Linear_0/__call__/b': (4,), '~/Linear_0/__call__/w': (3, 4)}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init, apply = transform(lambda x: Linear(4)(x))\n",
    "\n",
    "params = init(data)\n",
    "parameter_shapes(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[2, 4] n=8 x∈[-1.179, 3.948] μ=0.996 σ=2.137 gpu:0 [[-1.179, 2.111, 3.948, -0.894], [-1.179, 2.111, 3.948, -0.894]]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(params, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "\n",
    "  def __init__(self, widths):\n",
    "    super().__init__()\n",
    "    self._widths = widths\n",
    "\n",
    "  @module_method\n",
    "  def __call__(self, x):\n",
    "    for w in self._widths:\n",
    "      out = Linear(w)(x)\n",
    "      x = jax.nn.sigmoid(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'~/MLP_0/__call__/Linear_0/__call__/b': (3,),\n",
       " '~/MLP_0/__call__/Linear_0/__call__/w': (3, 3),\n",
       " '~/MLP_0/__call__/Linear_1/__call__/b': (5,),\n",
       " '~/MLP_0/__call__/Linear_1/__call__/w': (3, 5)}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init, apply = transform(lambda x: MLP([3, 5])(x))\n",
    "parameter_shapes(init(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop (Haiku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'~/MLP_0/__call__/Linear_0/__call__/b': (128,),\n",
       " '~/MLP_0/__call__/Linear_0/__call__/w': (1, 128),\n",
       " '~/MLP_0/__call__/Linear_1/__call__/b': (50,),\n",
       " '~/MLP_0/__call__/Linear_1/__call__/w': (128, 50),\n",
       " '~/MLP_0/__call__/Linear_2/__call__/b': (1,),\n",
       " '~/MLP_0/__call__/Linear_2/__call__/w': (50, 1)}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data: a quadratic curve.\n",
    "xs = np.linspace(-2., 2., num=128)[:, None]  # Generate array of shape (128, 1).\n",
    "ys = xs ** 2\n",
    "\n",
    "# Model\n",
    "def mlp(x):\n",
    "  return MLP([128, 50, 1])(x)\n",
    "\n",
    "init, forward = transform(mlp)\n",
    "params = init(xs)\n",
    "parameter_shapes(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and update function\n",
    "def loss_fn(params, x, y):\n",
    "  return jnp.mean((forward(params, x) - y) ** 2)\n",
    "\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "@jax.jit\n",
    "def update(params, x, y):\n",
    "  grads = jax.grad(loss_fn)(params, x, y)\n",
    "  return jax.tree_util.tree_map(\n",
    "      lambda p, g: p - LEARNING_RATE * g, params, grads\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5000):\n",
    "  params = update(params, xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(xs, ys, label='Data')\n",
    "plt.scatter(xs, forward(params, xs), label='Model prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict model, inputs\n",
    "inputs = x_train\n",
    "z = lin(w1,b1)(inputs)\n",
    "z = relu(z)\n",
    "z = lin(w2,b2)(z)\n",
    "\n",
    "# model = lin(w1,b1) | relu | lin(w2,b2)  \n",
    "# predict(model, inputs) = model(inputs), model -> inputs -> outputs\n",
    "# evaluate (model, loss, inputs, targets) -> msg\n",
    "# update: msg -> model -> model\n",
    "\n",
    "# @jit\n",
    "# def update(params, x, y): params\n",
    "#   grads = grad(loss)(params, x, y)\n",
    "#   return [(w - step_size * dw, b - step_size * db)\n",
    "#           for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model, loss, inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,b1 = initLinear(random.PRNGKey(42), 784,50)\n",
    "w1,b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2,b2 = initLinear(random.PRNGKey(43), 50,1)\n",
    "w2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fastcore.transform.Pipeline([ReLU])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, model(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fastcore.transform.Pipeline([Linear(w1,b1), ReLU, Linear(w2,b2)])\n",
    "x_train, model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(x_train)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(o,t): return (o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1):\n",
    "  w_key, b_key = random.split(key)\n",
    "  return scale * random.normal(w_key, (m, n)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "random_layer_params(784,50, random.PRNGKey(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  sizes = [784, 512, 512, 10]\n",
    "  keys = random.split(random.PRNGKey(42), len(sizes))\n",
    "  keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(m, n) for m, n in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = init_network_params(sizes, random.PRNGKey(42))\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x): return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "  # per-example predictions\n",
    "  activations = image\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = activations@w + b\n",
    "    activations = relu(outputs)\n",
    "  \n",
    "  final_w, final_b = params[-1]\n",
    "  logits = activations@final_w + final_b\n",
    "  return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works on single examples\n",
    "random_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\n",
    "preds = predict(params, random_flattened_image)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_predict = vmap(predict, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\n",
    "batched_preds = batched_predict(params, random_flattened_images)\n",
    "batched_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = random.normal(random.PRNGKey(42), (50,10))\n",
    "b = random.normal(random.PRNGKey(43), (10,10))\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a@b, jnp.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.array([10,20,30])\n",
    "a[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, k, dtype=jnp.float32):\n",
    "  \"\"\"Create a one-hot encoding of y of size k klasses\"\"\"\n",
    "  return jnp.array(y[:, None] == jnp.arange(k), dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, images, targets):\n",
    "  target_class = jnp.argmax(targets, axis=1)\n",
    "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "  return jnp.mean(predicted_class == target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, test_labels = map(partial(one_hot, k=10), (y_train, y_valid))\n",
    "train_images, test_images = x_train, x_valid\n",
    "train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels-train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.mean(train_labels -train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(o, t): return jnp.mean((o-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, images, targets): return mse(batched_predict(params, images), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(params, train_images, train_labels), grad(loss)(params, train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.01\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "  grads = grad(loss)(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8\n",
    "n_targets = 10\n",
    "# batch_size = 128\n",
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  x, y = train_images, train_labels\n",
    "  params = update(params, x, y)\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, train_images, train_labels)\n",
    "  test_acc = accuracy(params, test_images, test_labels)\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.random.normal(key, (m,nh), dtype=jax.numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = jax.random.normal(key, (m,nh), dtype=jax.numpy.float32)\n",
    "b1 = jnp.zeros(nh)\n",
    "w2 = jax.random.normal(key, (nh,1), dtype=jax.numpy.float32)\n",
    "b2 = jnp.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, w1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = lin(x_valid, w1, b1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = relu(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(x_valid)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Of course, `mse` is not a suitable loss function for multi-class classification; we'll use a better loss function soon. We'll use `mse` for now to keep things simple.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(res-y_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get rid of that trailing (,1), in order to use `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(res[:,0]-y_valid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.float32(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_valid = map(jnp.float32, (y_train,y_valid))\n",
    "\n",
    "preds = model(x_train)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): return ((output[:,0]-targ)** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [784, 512, 512, 10]\n",
    "sizes[:-1], sizes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(sizes[:-1], sizes[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state which holds the parameters for the transformed function.\n",
    "# get_param uses this to know where to get params from.\n",
    "current_params = []\n",
    "\n",
    "def transform(f):\n",
    "\n",
    "  def apply_f(params, *args, **kwargs):\n",
    "    current_params.append(params)\n",
    "    outs = f(*args, **kwargs)\n",
    "    current_params.pop()\n",
    "    return outs\n",
    "\n",
    "  return apply_f\n",
    "\n",
    "\n",
    "def get_param(identifier):\n",
    "  return current_params[-1][identifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols,diff\n",
    "x,y = symbols('x y')\n",
    "diff(x**2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(3*x**2+9, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:,0]-targ\n",
    "    loss = (res ** 2).mean()\n",
    "    \n",
    "    # backward pass:\n",
    "    out.g = 2.*diff[:,None] / inp.shape[0]\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing against later\n",
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cheat a little bit and use PyTorch autograd to check our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1 = lin(inp, w12, b12)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w22, b22)\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state which holds the parameters for the transformed function.\n",
    "# get_param uses this to know where to get params from.\n",
    "current_params = []\n",
    "\n",
    "def transform(f):\n",
    "\n",
    "  def apply_f(params, *args, **kwargs):\n",
    "    current_params.append(params)\n",
    "    outs = f(*args, **kwargs)\n",
    "    current_params.pop()\n",
    "    return outs\n",
    "\n",
    "  return apply_f\n",
    "\n",
    "\n",
    "def get_param(identifier):\n",
    "  return current_params[-1][identifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp,self.targ = inp,targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    def forward(self, inp): return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b806adfb64333d0ca5c14ed2dbf613d5d551ec856d702e8a01588c05fb48e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
