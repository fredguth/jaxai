{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import lovely_jax as lj\n",
    "lj.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11176"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pynvml\n",
    "def get_memory_free_MiB(gpu_index):\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(int(gpu_index))\n",
    "    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    return mem_info.free // 1024 ** 2\n",
    "\n",
    "get_memory_free_MiB(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(jnp.array, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[50000, 784] n=39200000 x∈[0., 0.996] μ=0.130 σ=0.307 gpu:0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[50, 10] n=500 x∈[-0.028, 0.027] μ=-4.746e-06 σ=0.010 gpu:0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.normal(random.PRNGKey(42), (50,10))*1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array[784, 50] n=39200 x∈[-3.969, 4.653] μ=0.001 σ=1.008 gpu:0,\n",
       " Array[50] x∈[-2.799, 1.671] μ=-0.163 σ=1.007 gpu:0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1):\n",
    "  w_key, b_key = random.split(key)\n",
    "  return scale * random.normal(w_key, (m, n)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "random_layer_params(784,50, random.PRNGKey(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[4, 2] u32 n=8 x∈[13772463, 3276725750] μ=1.549e+09 σ=1.162e+09 gpu:0 [[2954079971, 2575172198], [1139032965, 1035738655], [3276725750, 1223413151], [173471767, 13772463]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  sizes = [784, 512, 512, 10]\n",
    "  keys = random.split(random.PRNGKey(42), len(sizes))\n",
    "  keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(784, 512), (512, 512), (512, 10)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(m, n) for m, n in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Array[784, 512] n=401408 x∈[-4.272, 5.061] μ=-0.000 σ=0.999 gpu:0,\n",
       "  Array[512] x∈[-2.752, 2.637] μ=0.017 σ=1.006 gpu:0),\n",
       " (Array[512, 512] n=262144 x∈[-4.694, 4.418] μ=0.003 σ=0.999 gpu:0,\n",
       "  Array[512] x∈[-3.008, 2.639] μ=0.027 σ=0.996 gpu:0),\n",
       " (Array[512, 10] n=5120 x∈[-3.469, 4.165] μ=0.017 σ=1.001 gpu:0,\n",
       "  Array[10] x∈[-1.779, 1.051] μ=-0.887 σ=0.832 gpu:0 [-0.804, -1.676, -1.779, -1.359, 1.051, -0.639, -0.035, -1.232, -0.742, -1.655])]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = init_network_params(sizes, random.PRNGKey(42))\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x): return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "  # per-example predictions\n",
    "  activations = image\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = activations@w + b\n",
    "    activations = relu(outputs)\n",
    "  \n",
    "  final_w, final_b = params[-1]\n",
    "  logits = activations@final_w + final_b\n",
    "  return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[10] x∈[-2.107e+04, 0.] μ=-1.238e+04 σ=6.829e+03 gpu:0 [-1.162e+04, -1.724e+04, -2.047e+04, 0., -1.182e+04, -1.465e+04, -1.250e+04, -2.107e+04, -535.592, -1.385e+04]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This works on single examples\n",
    "random_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\n",
    "preds = predict(params, random_flattened_image)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_predict = vmap(predict, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[10, 10] n=100 x∈[-2.639e+04, 0.] μ=-1.145e+04 σ=7.787e+03 gpu:0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\n",
    "batched_preds = batched_predict(params, random_flattened_images)\n",
    "batched_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array[50, 10] n=500 x∈[-2.774, 2.730] μ=-0.000 σ=1.033 gpu:0,\n",
       " Array[10, 10] n=100 x∈[-2.508, 2.865] μ=0.049 σ=0.985 gpu:0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = random.normal(random.PRNGKey(42), (50,10))\n",
    "b = random.normal(random.PRNGKey(43), (10,10))\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array[50, 10] n=500 x∈[-11.963, 9.819] μ=0.010 σ=3.019 gpu:0,\n",
       " Array[50, 10] n=500 x∈[-11.963, 9.819] μ=0.010 σ=3.019 gpu:0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a@b, jnp.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[10] i32 x∈[0, 9] μ=4.500 σ=2.872 gpu:0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[3, 1] i32 x∈[10, 30] μ=20.000 σ=8.165 gpu:0 [[10], [20], [30]]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = jnp.array([10,20,30])\n",
    "a[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, k, dtype=jnp.float32):\n",
    "  \"\"\"Create a one-hot encoding of y of size k klasses\"\"\"\n",
    "  return jnp.array(y[:, None] == jnp.arange(k), dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, images, targets):\n",
    "  target_class = jnp.argmax(targets, axis=1)\n",
    "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "  return jnp.mean(predicted_class == target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array[50000, 784] n=39200000 x∈[0., 0.996] μ=0.130 σ=0.307 gpu:0,\n",
       " Array[50000, 10] n=500000 x∈[0., 1.000] μ=0.100 σ=0.300 gpu:0,\n",
       " Array[10000, 784] n=7840000 x∈[0., 0.996] μ=0.129 σ=0.305 gpu:0,\n",
       " Array[10000, 10] n=100000 x∈[0., 1.000] μ=0.100 σ=0.300 gpu:0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels, test_labels = map(partial(one_hot, k=10), (y_train, y_valid))\n",
    "train_images, test_images = x_train, x_valid\n",
    "train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[50000, 10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels-train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array gpu:0 0.100"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(train_labels -train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(o, t): return jnp.mean((o-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, images, targets): return mse(batched_predict(params, images), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array gpu:0 3.334e+07,\n",
       " [(Array[784, 512] n=401408 x∈[-1.781e+05, 2.268e+05] μ=1.916e+03 σ=1.738e+04 gpu:0,\n",
       "   Array[512] x∈[-2.839e+05, 3.672e+05] μ=1.335e+04 σ=7.398e+04 gpu:0),\n",
       "  (Array[512, 512] n=262144 x∈[-5.188e+05, 5.030e+05] μ=2.646e+03 σ=2.896e+04 gpu:0,\n",
       "   Array[512] x∈[-2.165e+04, 2.077e+04] μ=675.916 σ=5.403e+03 gpu:0),\n",
       "  (Array[512, 10] n=5120 x∈[-5.433e+05, 2.726e+06] μ=0.000 σ=2.679e+05 gpu:0,\n",
       "   Array[10] x∈[-1.723e+03, 8.635e+03] μ=7.324e-05 σ=2.927e+03 gpu:0 [-967.755, -919.111, -1.242e+03, 412.967, -743.843, -922.010, -1.197e+03, -1.334e+03, 8.635e+03, -1.723e+03])])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(params, train_images, train_labels), grad(loss)(params, train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.01\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "  grads = grad(loss)(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 2.34 sec\n",
      "Training set accuracy 0.09975999593734741\n",
      "Test set accuracy 0.09609999507665634\n",
      "Epoch 1 in 0.00 sec\n",
      "Training set accuracy 0.09683999419212341\n",
      "Test set accuracy 0.10089999437332153\n",
      "Epoch 2 in 0.00 sec\n",
      "Training set accuracy 0.09975999593734741\n",
      "Test set accuracy 0.09609999507665634\n",
      "Epoch 3 in 0.00 sec\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "Epoch 4 in 0.00 sec\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "Epoch 5 in 0.00 sec\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "Epoch 6 in 0.00 sec\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n",
      "Epoch 7 in 0.00 sec\n",
      "Training set accuracy 0.09863999485969543\n",
      "Test set accuracy 0.09910000115633011\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "n_targets = 10\n",
    "# batch_size = 128\n",
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  x, y = train_images, train_labels\n",
    "  params = update(params, x, y)\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, train_images, train_labels)\n",
    "  test_acc = accuracy(params, test_images, test_labels)\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, Array i32 gpu:0 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[784, 50] n=39200 x∈[-4.057, 4.385] μ=0.004 σ=0.999 gpu:0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.random.normal(key, (m,nh), dtype=jax.numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = jax.random.normal(key, (m,nh), dtype=jax.numpy.float32)\n",
    "b1 = jnp.zeros(nh)\n",
    "w2 = jax.random.normal(key, (nh,1), dtype=jax.numpy.float32)\n",
    "b2 = jnp.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array[10000, 784] n=7840000 x∈[0., 0.996] μ=0.129 σ=0.305 gpu:0,\n",
       " Array[784, 50] n=39200 x∈[-4.057, 4.385] μ=0.004 σ=0.999 gpu:0,\n",
       " Array[50] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid, w1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[10000, 50] n=500000 x∈[-39.335, 46.460] μ=1.220 σ=9.081 gpu:0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(x_valid, w1, b1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[10000, 50] n=500000 x∈[0., 46.460] μ=4.220 σ=5.810 gpu:0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[10000, 1] x∈[-154.850, 80.024] μ=-31.277 σ=34.980 gpu:0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(x_valid)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Of course, `mse` is not a suitable loss function for multi-class classification; we'll use a better loss function soon. We'll use `mse` for now to keep things simple.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1), (10000,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res-y_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get rid of that trailing (,1), in order to use `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res[:,0]-y_valid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.float32(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[50000, 1] x∈[-186.163, 77.793] μ=-31.938 σ=36.011 gpu:0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train,y_valid = map(jnp.float32, (y_train,y_valid))\n",
    "\n",
    "preds = model(x_train)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): return ((output[:,0]-targ)** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array gpu:0 2.633e+03"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([784, 512, 512], [512, 512, 10])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = [784, 512, 512, 10]\n",
    "sizes[:-1], sizes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(784, 512), (512, 512), (512, 10)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(sizes[:-1], sizes[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state which holds the parameters for the transformed function.\n",
    "# get_param uses this to know where to get params from.\n",
    "current_params = []\n",
    "\n",
    "def transform(f):\n",
    "\n",
    "  def apply_f(params, *args, **kwargs):\n",
    "    current_params.append(params)\n",
    "    outs = f(*args, **kwargs)\n",
    "    current_params.pop()\n",
    "    return outs\n",
    "\n",
    "  return apply_f\n",
    "\n",
    "\n",
    "def get_param(identifier):\n",
    "  return current_params[-1][identifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x$"
      ],
      "text/plain": [
       "2*x"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols,diff\n",
    "x,y = symbols('x y')\n",
    "diff(x**2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6 x$"
      ],
      "text/plain": [
       "6*x"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(3*x**2+9, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:,0]-targ\n",
    "    loss = (res ** 2).mean()\n",
    "    \n",
    "    # backward pass:\n",
    "    out.g = 2.*diff[:,None] / inp.shape[0]\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (1,) and (50,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m forward_and_backward(x_train, y_train)\n",
      "Cell \u001b[0;32mIn [63], line 11\u001b[0m, in \u001b[0;36mforward_and_backward\u001b[0;34m(inp, targ)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# backward pass:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m out\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m \u001b[39m2.\u001b[39m\u001b[39m*\u001b[39mdiff[:,\u001b[39mNone\u001b[39;00m] \u001b[39m/\u001b[39m inp\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m lin_grad(l2, out, w2, b2)\n\u001b[1;32m     12\u001b[0m l1\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m (l1\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat() \u001b[39m*\u001b[39m l2\u001b[39m.\u001b[39mg\n\u001b[1;32m     13\u001b[0m lin_grad(inp, l1, w1, b1)\n",
      "Cell \u001b[0;32mIn [62], line 3\u001b[0m, in \u001b[0;36mlin_grad\u001b[0;34m(inp, out, w, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlin_grad\u001b[39m(inp, out, w, b):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# grad of matmul with respect to input\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     inp\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39;49mg \u001b[39m@\u001b[39;49m w\n\u001b[1;32m      4\u001b[0m     w\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m (inp\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m out\u001b[39m.\u001b[39mg\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m     b\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mg\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.miniconda3/envs/py39/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:4948\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4946\u001b[0m args \u001b[39m=\u001b[39m (other, \u001b[39mself\u001b[39m) \u001b[39mif\u001b[39;00m swap \u001b[39melse\u001b[39;00m (\u001b[39mself\u001b[39m, other)\n\u001b[1;32m   4947\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m-> 4948\u001b[0m   \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   4949\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m   4950\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsupported operand type(s) for \u001b[39m\u001b[39m{\u001b[39;00mopchar\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4951\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.miniconda3/envs/py39/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:2987\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   2985\u001b[0m a \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39msqueeze(a, \u001b[39mtuple\u001b[39m(a_squeeze))\n\u001b[1;32m   2986\u001b[0m b \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39msqueeze(b, \u001b[39mtuple\u001b[39m(b_squeeze))\n\u001b[0;32m-> 2987\u001b[0m out \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39;49mdot_general(\n\u001b[1;32m   2988\u001b[0m   a, b, (((ndim(a) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m,), (ndim(b) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m b_is_mat,)), (a_batch, b_batch)),\n\u001b[1;32m   2989\u001b[0m   precision\u001b[39m=\u001b[39;49mprecision)\n\u001b[1;32m   2990\u001b[0m \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mtranspose(out, perm)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.miniconda3/envs/py39/lib/python3.9/site-packages/jax/_src/lax/lax.py:2530\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   2527\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m core\u001b[39m.\u001b[39msymbolic_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[1;32m   2528\u001b[0m   msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2529\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mshape, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2530\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[1;32m   2532\u001b[0m \u001b[39mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[39m.\u001b[39mshape, rhs\u001b[39m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (1,) and (50,)."
     ]
    }
   ],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing against later\n",
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cheat a little bit and use PyTorch autograd to check our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1 = lin(inp, w12, b12)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w22, b22)\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state which holds the parameters for the transformed function.\n",
    "# get_param uses this to know where to get params from.\n",
    "current_params = []\n",
    "\n",
    "def transform(f):\n",
    "\n",
    "  def apply_f(params, *args, **kwargs):\n",
    "    current_params.append(params)\n",
    "    outs = f(*args, **kwargs)\n",
    "    current_params.pop()\n",
    "    return outs\n",
    "\n",
    "  return apply_f\n",
    "\n",
    "\n",
    "def get_param(identifier):\n",
    "  return current_params[-1][identifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp,self.targ = inp,targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    def forward(self, inp): return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n",
       "          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n",
       "        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n",
       "        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b806adfb64333d0ca5c14ed2dbf613d5d551ec856d702e8a01588c05fb48e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
