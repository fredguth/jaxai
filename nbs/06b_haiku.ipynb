{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAIKU MNIST example\n",
    "https://github.com/deepmind/dm-haiku/blob/main/examples/mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_MEM_FRACTION=0.8\n"
     ]
    }
   ],
   "source": [
    "# Sets how much GPU memory JAX preallocate\n",
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, NamedTuple\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "import lovely_jax as lj\n",
    "lj.monkey_patch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(NamedTuple):\n",
    "  image: np.ndarray  # [B, H, W, 1]\n",
    "  label: np.ndarray  # [B]\n",
    "  \n",
    "def load_dataset(\n",
    "    split: str,\n",
    "    *,\n",
    "    shuffle: bool,\n",
    "    batch_size: int,\n",
    ") -> Iterator[Batch]:\n",
    "  \"\"\"Loads the MNIST dataset.\"\"\"\n",
    "  ds = tfds.load(\"mnist:3.*.*\", split=split).cache().repeat()\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(10 * batch_size, seed=0)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.map(lambda x: Batch(**x))\n",
    "  return iter(tfds.as_numpy(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 15:23:57.343196: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-01-13 15:23:57.345746: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(__main__.Batch, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " bs = 1_000\n",
    " train = load_dataset(\"train\", shuffle=True, batch_size=bs)\n",
    " batch = next(train)\n",
    " type(batch), len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "def net_fn(images: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n",
    "  x = images.astype(jnp.float32) / 255.\n",
    "  mlp = hk.Sequential([\n",
    "      hk.Flatten(),\n",
    "      hk.Linear(300), jax.nn.relu,\n",
    "      hk.Linear(100), jax.nn.relu,\n",
    "      hk.Linear(NUM_CLASSES),\n",
    "  ])\n",
    "  return mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = hk.without_apply_rng(hk.transform(net_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': {'w': Array[784, 300] n=235200 x∈[-0.071, 0.071] μ=-5.580e-05 σ=0.031 gpu:0,\n",
       "  'b': Array[300] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0},\n",
       " 'linear_1': {'w': Array[300, 100] n=30000 x∈[-0.115, 0.115] μ=0.000 σ=0.051 gpu:0,\n",
       "  'b': Array[100] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0},\n",
       " 'linear_2': {'w': Array[100, 10] n=1000 x∈[-0.196, 0.197] μ=-0.001 σ=0.091 gpu:0,\n",
       "  'b': Array[10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params = network.init(jax.random.PRNGKey(seed=0), next(train).image)\n",
    "initial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[1000, 10] n=10000 x∈[-0.449, 0.470] μ=-0.006 σ=0.121 gpu:0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = network.apply(initial_params, batch.image)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array gpu:0 0.112"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def evaluate(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "    \"\"\"Evaluation metric (classification accuracy).\"\"\"\n",
    "    logits = network.apply(params, batch.image)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    return jnp.mean(predictions == batch.label)\n",
    "\n",
    "evaluate(initial_params, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params: hk.Params, batch: Batch, wd=1e-4) -> jnp.ndarray:\n",
    "    \"\"\"Cross-entropy classification loss, regularised by L2 weight decay.\"\"\"\n",
    "    batch_size, *_ = batch.image.shape\n",
    "    logits = network.apply(params, batch.image)\n",
    "    labels = jax.nn.one_hot(batch.label, NUM_CLASSES)\n",
    "\n",
    "    l2_regulariser = 0.5 * sum(\n",
    "        jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "    log_likelihood = jnp.sum(labels * jax.nn.log_softmax(logits))\n",
    "\n",
    "    return -log_likelihood / batch_size + wd * l2_regulariser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array gpu:0 2.326"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(initial_params, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingState(params={'linear': {'w': Array[784, 300] n=235200 x∈[-0.071, 0.071] μ=-5.580e-05 σ=0.031 gpu:0, 'b': Array[300] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'linear_1': {'w': Array[300, 100] n=30000 x∈[-0.115, 0.115] μ=0.000 σ=0.051 gpu:0, 'b': Array[100] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'linear_2': {'w': Array[100, 10] n=1000 x∈[-0.196, 0.197] μ=-0.001 σ=0.091 gpu:0, 'b': Array[10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]}}, avg_params={'linear': {'w': Array[784, 300] n=235200 x∈[-0.071, 0.071] μ=-5.580e-05 σ=0.031 gpu:0, 'b': Array[300] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'linear_1': {'w': Array[300, 100] n=30000 x∈[-0.115, 0.115] μ=0.000 σ=0.051 gpu:0, 'b': Array[100] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'linear_2': {'w': Array[100, 10] n=1000 x∈[-0.196, 0.197] μ=-0.001 σ=0.091 gpu:0, 'b': Array[10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]}}, opt_state=(ScaleByAdamState(count=Array i32 gpu:0 0, mu={'linear': {'b': Array[300] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0, 'w': Array[784, 300] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'linear_1': {'b': Array[100] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0, 'w': Array[300, 100] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'linear_2': {'b': Array[10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[100, 10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}}, nu={'linear': {'b': Array[300] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0, 'w': Array[784, 300] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'linear_1': {'b': Array[100] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0, 'w': Array[300, 100] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'linear_2': {'b': Array[10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[100, 10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}}), EmptyState()))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrainingState(NamedTuple):\n",
    "  params: hk.Params\n",
    "  avg_params: hk.Params\n",
    "  opt_state: optax.OptState\n",
    "\n",
    "optimiser = optax.adam(1e-3)\n",
    "initial_opt_state = optimiser.init(initial_params)\n",
    "state = TrainingState(initial_params, initial_params, initial_opt_state)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': {'b': Array[300] x∈[-0.011, 0.004] μ=-0.001 σ=0.002 gpu:0,\n",
       "  'w': Array[784, 300] n=235200 x∈[-0.010, 0.006] μ=-0.000 σ=0.001 gpu:0},\n",
       " 'linear_1': {'b': Array[100] x∈[-0.016, 0.006] μ=-0.004 σ=0.005 gpu:0,\n",
       "  'w': Array[300, 100] n=30000 x∈[-0.012, 0.008] μ=-0.000 σ=0.001 gpu:0},\n",
       " 'linear_2': {'b': Array[10] x∈[-0.023, 0.026] μ=1.593e-08 σ=0.015 gpu:0 [0.026, -0.023, 0.007, 0.011, 0.005, -0.023, 0.009, -0.015, -0.002, 0.006],\n",
       "  'w': Array[100, 10] n=1000 x∈[-0.026, 0.016] μ=-1.259e-07 σ=0.005 gpu:0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = jax.grad(loss)(state.params, batch)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'linear': {'b': Array[300] x∈[-0.001, 0.001] μ=0.000 σ=0.001 gpu:0,\n",
       "   'w': Array[784, 300] n=235200 x∈[-0.001, 0.001] μ=7.347e-05 σ=0.001 gpu:0},\n",
       "  'linear_1': {'b': Array[100] x∈[-0.001, 0.001] μ=0.000 σ=0.001 gpu:0,\n",
       "   'w': Array[300, 100] n=30000 x∈[-0.001, 0.001] μ=0.000 σ=0.001 gpu:0},\n",
       "  'linear_2': {'b': Array[10] x∈[-0.001, 0.001] μ=-8.737e-05 σ=0.001 gpu:0 [-0.001, 0.001, -0.001, -0.001, -0.001, 0.001, -0.001, 0.001, 0.001, -0.000],\n",
       "   'w': Array[100, 10] n=1000 x∈[-0.001, 0.001] μ=-0.000 σ=0.001 gpu:0}},\n",
       " (ScaleByAdamState(count=Array i32 gpu:0 3, mu={'linear': {'b': Array[300] x∈[-0.002, 0.001] μ=-0.000 σ=0.000 gpu:0, 'w': Array[784, 300] n=235200 x∈[-0.003, 0.002] μ=-2.849e-05 σ=0.000 gpu:0}, 'linear_1': {'b': Array[100] x∈[-0.003, 0.002] μ=-0.001 σ=0.001 gpu:0, 'w': Array[300, 100] n=30000 x∈[-0.003, 0.002] μ=-7.652e-05 σ=0.000 gpu:0}, 'linear_2': {'b': Array[10] x∈[-0.005, 0.005] μ=3.475e-09 σ=0.003 gpu:0 [0.005, -0.005, 0.001, 0.002, 0.001, -0.004, 0.004, -0.004, -0.001, 0.000], 'w': Array[100, 10] n=1000 x∈[-0.006, 0.004] μ=-3.113e-08 σ=0.001 gpu:0}}, nu={'linear': {'b': Array[300] x∈[7.731e-12, 2.533e-07] μ=1.447e-08 σ=2.348e-08 gpu:0, 'w': Array[784, 300] n=235200 x∈[7.156e-21, 2.646e-07] μ=1.944e-09 σ=6.477e-09 gpu:0}, 'linear_1': {'b': Array[100] x∈[5.974e-11, 4.949e-07] μ=7.992e-08 σ=1.050e-07 gpu:0, 'w': Array[300, 100] n=30000 x∈[3.875e-19, 3.373e-07] μ=5.185e-09 σ=1.389e-08 gpu:0}, 'linear_2': {'b': Array[10] x∈[6.836e-08, 9.764e-07] μ=5.074e-07 σ=3.809e-07 gpu:0 [9.343e-07, 8.735e-07, 1.036e-07, 2.134e-07, 9.706e-08, 9.764e-07, 9.710e-07, 6.090e-07, 6.836e-08, 2.273e-07], 'w': Array[100, 10] n=1000 x∈[7.263e-13, 1.376e-06] μ=4.645e-08 σ=9.244e-08 gpu:0}}),\n",
       "  EmptyState()))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates, opt_state = optimiser.update(grads, state.opt_state)\n",
    "updates, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(state: TrainingState, batch: Batch) -> TrainingState:\n",
    "    \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
    "    grads = jax.grad(loss)(state.params, batch)\n",
    "    updates, opt_state = optimiser.update(grads, state.opt_state)\n",
    "    params = optax.apply_updates(state.params, updates)\n",
    "    # Compute avg_params, the exponential moving average of the \"live\" params.\n",
    "    # We use this only for evaluation (cf. https://doi.org/10.1137/0330046).\n",
    "    avg_params = optax.incremental_update(\n",
    "        params, state.avg_params, step_size=0.001) #lambda new, old: step_size * new + (1.0 - step_size) * old\n",
    "    return TrainingState(params, avg_params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array gpu:0 2.117"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train)\n",
    "state = update(state,batch)\n",
    "loss(state.params, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ArrayTree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ArrayTree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mupdates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ArrayTree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ArrayTree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ArrayTree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ArrayTree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpdates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m\"\"\"Applies an update to the corresponding parameters.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  This is a utility functions that applies an update to a set of parameters, and\u001b[0m\n",
      "\u001b[0;34m  then returns the updated parameters to the caller. As an example, the update\u001b[0m\n",
      "\u001b[0;34m  may be a gradient transformed by a sequence of`GradientTransformations`. This\u001b[0m\n",
      "\u001b[0;34m  function is exposed for convenience, but it just adds updates and parameters;\u001b[0m\n",
      "\u001b[0;34m  you may also apply updates to parameters manually, using `tree_map`\u001b[0m\n",
      "\u001b[0;34m  (e.g. if you want to manipulate updates in custom ways before applying them).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  Args:\u001b[0m\n",
      "\u001b[0;34m    params: a tree of parameters.\u001b[0m\n",
      "\u001b[0;34m    updates: a tree of updates, the tree structure and the shape of the leaf\u001b[0m\n",
      "\u001b[0;34m    nodes must match that of `params`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  Returns:\u001b[0m\n",
      "\u001b[0;34m    Updated parameters, with same structure, shape and type as `params`.\u001b[0m\n",
      "\u001b[0;34m  \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mreturn\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.miniconda3/envs/py39/lib/python3.9/site-packages/optax/_src/update.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <generator object _eager_dataset_iterator at 0x7fd5383ae120>,\n",
       " 'test': <generator object _eager_dataset_iterator at 0x7fd5383aedd0>}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_datasets = {split: load_dataset(split, shuffle=False, batch_size=10_000) for split in (\"train\", \"test\")}\n",
    "eval_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step': 0, 'split': 'train', 'accuracy': '0.123'}\n",
      "{'step': 0, 'split': 'test', 'accuracy': '0.121'}\n",
      "{'step': 100, 'split': 'train', 'accuracy': '0.436'}\n",
      "{'step': 100, 'split': 'test', 'accuracy': '0.440'}\n",
      "{'step': 200, 'split': 'train', 'accuracy': '0.635'}\n",
      "{'step': 200, 'split': 'test', 'accuracy': '0.645'}\n",
      "{'step': 300, 'split': 'train', 'accuracy': '0.781'}\n",
      "{'step': 300, 'split': 'test', 'accuracy': '0.792'}\n",
      "{'step': 400, 'split': 'train', 'accuracy': '0.866'}\n",
      "{'step': 400, 'split': 'test', 'accuracy': '0.864'}\n",
      "{'step': 500, 'split': 'train', 'accuracy': '0.902'}\n",
      "{'step': 500, 'split': 'test', 'accuracy': '0.904'}\n",
      "{'step': 600, 'split': 'train', 'accuracy': '0.930'}\n",
      "{'step': 600, 'split': 'test', 'accuracy': '0.928'}\n",
      "{'step': 700, 'split': 'train', 'accuracy': '0.947'}\n",
      "{'step': 700, 'split': 'test', 'accuracy': '0.942'}\n",
      "{'step': 800, 'split': 'train', 'accuracy': '0.955'}\n",
      "{'step': 800, 'split': 'test', 'accuracy': '0.952'}\n",
      "{'step': 900, 'split': 'train', 'accuracy': '0.966'}\n",
      "{'step': 900, 'split': 'test', 'accuracy': '0.958'}\n",
      "{'step': 1000, 'split': 'train', 'accuracy': '0.972'}\n",
      "{'step': 1000, 'split': 'test', 'accuracy': '0.963'}\n",
      "{'step': 1100, 'split': 'train', 'accuracy': '0.978'}\n",
      "{'step': 1100, 'split': 'test', 'accuracy': '0.968'}\n",
      "{'step': 1200, 'split': 'train', 'accuracy': '0.979'}\n",
      "{'step': 1200, 'split': 'test', 'accuracy': '0.970'}\n",
      "{'step': 1300, 'split': 'train', 'accuracy': '0.987'}\n",
      "{'step': 1300, 'split': 'test', 'accuracy': '0.973'}\n",
      "{'step': 1400, 'split': 'train', 'accuracy': '0.986'}\n",
      "{'step': 1400, 'split': 'test', 'accuracy': '0.975'}\n",
      "{'step': 1500, 'split': 'train', 'accuracy': '0.989'}\n",
      "{'step': 1500, 'split': 'test', 'accuracy': '0.975'}\n",
      "{'step': 1600, 'split': 'train', 'accuracy': '0.991'}\n",
      "{'step': 1600, 'split': 'test', 'accuracy': '0.977'}\n",
      "{'step': 1700, 'split': 'train', 'accuracy': '0.993'}\n",
      "{'step': 1700, 'split': 'test', 'accuracy': '0.978'}\n",
      "{'step': 1800, 'split': 'train', 'accuracy': '0.995'}\n",
      "{'step': 1800, 'split': 'test', 'accuracy': '0.979'}\n",
      "{'step': 1900, 'split': 'train', 'accuracy': '0.996'}\n",
      "{'step': 1900, 'split': 'test', 'accuracy': '0.980'}\n",
      "{'step': 2000, 'split': 'train', 'accuracy': '0.997'}\n",
      "{'step': 2000, 'split': 'test', 'accuracy': '0.980'}\n",
      "{'step': 2100, 'split': 'train', 'accuracy': '0.998'}\n",
      "{'step': 2100, 'split': 'test', 'accuracy': '0.981'}\n",
      "{'step': 2200, 'split': 'train', 'accuracy': '0.998'}\n",
      "{'step': 2200, 'split': 'test', 'accuracy': '0.981'}\n",
      "{'step': 2300, 'split': 'train', 'accuracy': '0.998'}\n",
      "{'step': 2300, 'split': 'test', 'accuracy': '0.981'}\n",
      "{'step': 2400, 'split': 'train', 'accuracy': '0.998'}\n",
      "{'step': 2400, 'split': 'test', 'accuracy': '0.981'}\n",
      "{'step': 2500, 'split': 'train', 'accuracy': '0.999'}\n",
      "{'step': 2500, 'split': 'test', 'accuracy': '0.981'}\n",
      "{'step': 2600, 'split': 'train', 'accuracy': '0.999'}\n",
      "{'step': 2600, 'split': 'test', 'accuracy': '0.981'}\n",
      "{'step': 2700, 'split': 'train', 'accuracy': '1.000'}\n",
      "{'step': 2700, 'split': 'test', 'accuracy': '0.981'}\n",
      "{'step': 2800, 'split': 'train', 'accuracy': '0.999'}\n",
      "{'step': 2800, 'split': 'test', 'accuracy': '0.981'}\n",
      "{'step': 2900, 'split': 'train', 'accuracy': '0.999'}\n",
      "{'step': 2900, 'split': 'test', 'accuracy': '0.981'}\n",
      "{'step': 3000, 'split': 'train', 'accuracy': '1.000'}\n",
      "{'step': 3000, 'split': 'test', 'accuracy': '0.981'}\n"
     ]
    }
   ],
   "source": [
    "for step in range(3001):\n",
    "    if step % 100 == 0:\n",
    "      # Periodically evaluate classification accuracy on train & test sets.\n",
    "      # Note that each evaluation is only on a (large) batch.\n",
    "      for split, dataset in eval_datasets.items():\n",
    "        accuracy = np.array(evaluate(state.avg_params, next(dataset))).item()\n",
    "        print({\"step\": step, \"split\": split, \"accuracy\": f\"{accuracy:.3f}\"})\n",
    "\n",
    "    # Do SGD on a batch of training examples.\n",
    "    state = update(state, next(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = hk.without_apply_rng(hk.transform(net_fn))\n",
    "optimiser = optax.adam(1e-3)\n",
    "\n",
    "def loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "    \"\"\"Cross-entropy classification loss, regularised by L2 weight decay.\"\"\"\n",
    "    batch_size, *_ = batch.image.shape\n",
    "    logits = network.apply(params, batch.image)\n",
    "    labels = jax.nn.one_hot(batch.label, NUM_CLASSES)\n",
    "\n",
    "    l2_regulariser = 0.5 * sum(\n",
    "        jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
    "    log_likelihood = jnp.sum(labels * jax.nn.log_softmax(logits))\n",
    "\n",
    "    return -log_likelihood / batch_size + 1e-4 * l2_regulariser\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "    \"\"\"Evaluation metric (classification accuracy).\"\"\"\n",
    "    logits = network.apply(params, batch.image)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    return jnp.mean(predictions == batch.label)\n",
    "\n",
    "@jax.jit\n",
    "def update(state: TrainingState, batch: Batch) -> TrainingState:\n",
    "    \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
    "    grads = jax.grad(loss)(state.params, batch)\n",
    "    updates, opt_state = optimiser.update(grads, state.opt_state)\n",
    "    params = optax.apply_updates(state.params, updates)\n",
    "    # Compute avg_params, the exponential moving average of the \"live\" params.\n",
    "    # We use this only for evaluation (cf. https://doi.org/10.1137/0330046).\n",
    "    avg_params = optax.incremental_update(\n",
    "        params, state.avg_params, step_size=0.001)\n",
    "    return TrainingState(params, avg_params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'split': 'train', 'accuracy': '1.000'}\n",
      "{'epoch': 0, 'split': 'test', 'accuracy': '0.983'}\n",
      "{'epoch': 1, 'split': 'train', 'accuracy': '1.000'}\n",
      "{'epoch': 1, 'split': 'test', 'accuracy': '0.983'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training & evaluation loop.\n",
    "n_epochs = 2\n",
    "for epoch in range(n_epochs):\n",
    "    for _, batch in enumerate(train): state = update(state, batch)\n",
    "    \n",
    "    for split, dataset in eval_datasets.items():\n",
    "        accuracy = np.array(evaluate(state.avg_params, next(dataset))).item()\n",
    "        print({\"epoch\": epoch, \"split\": split, \"accuracy\": f\"{accuracy:.3f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self,dls): fc.store_attr()\n",
    "    def one_batch(self):\n",
    "        self.xb, self.yb = self.batch\n",
    "    def one_epoch(self, is_training):\n",
    "        dl = self.dls.train if is_training else self.dls.valid\n",
    "        for self.num, self.batch in enumerate(dl): self.one_batch()\n",
    "\n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(True)\n",
    "            self.one_epoch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b806adfb64333d0ca5c14ed2dbf613d5d551ec856d702e8a01588c05fb48e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
