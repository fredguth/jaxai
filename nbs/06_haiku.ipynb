{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets how much GPU memory JAX preallocate\n",
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, NamedTuple, Callable\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import lovely_jax as lj\n",
    "lj.monkey_patch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "import torchvision.transforms.functional as TF\n",
    "from operator import itemgetter\n",
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_dict(ds):\n",
    "    get = itemgetter(*ds.features)\n",
    "    def _f(b): return get(default_collate(b))\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaders:\n",
    "    def __init__(self, *dls): self.train,self.valid = dls[:2]\n",
    "\n",
    "    @classmethod\n",
    "    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n",
    "        return cls(*[DataLoader(ds, batch_size, collate_fn=collate_dict(ds), **kwargs) for ds in dd.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inplace(f):\n",
    "    def _f(b):\n",
    "        f(b)\n",
    "        return b\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = dsd.with_transform(transformi)\n",
    "bs = 1024*8\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=6)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, batch = list(enumerate(dt))[0]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(NamedTuple):\n",
    "  image: np.ndarray  # [B, H, W, 1]\n",
    "  label: np.ndarray  # [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = map(jnp.array, batch)\n",
    "batch = Batch(xb,yb)\n",
    "batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x:jnp.array) ->jnp.ndarray: return hk.nets.MLP(output_sizes=[50,10])(x)\n",
    "model = hk.without_apply_rng(hk.transform(forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "initial_params = model.init(key, xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.apply(initial_params, batch.image)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.typedispatch\n",
    "@jax.jit\n",
    "def evaluate(params:hk.Params, batch:Batch) -> jnp.ndarray:\n",
    "    logits = model.apply(params, batch.image)\n",
    "    preds = jnp.argmax (logits, axis=-1)\n",
    "    return jnp.mean(preds == batch.label)\n",
    "\n",
    "evaluate(initial_params, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = optax.softmax_cross_entropy_with_integer_labels(model.apply(initial_params, batch.image), batch.label)\n",
    "jnp.sum(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "def loss(params:hk.Params, batch: Batch)-> jnp.ndarray:\n",
    "    bs, *_ = batch.image.shape\n",
    "    preds = model.apply(params, batch.image)\n",
    "    return jnp.sum(optax.softmax_cross_entropy_with_integer_labels(preds, batch.label)/bs)\n",
    "\n",
    "loss(initial_params, batch), batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingState(NamedTuple):\n",
    "  params: hk.Params\n",
    "  opt_state: optax.OptState\n",
    "  \n",
    "# Optimiser\n",
    "lr = 1e-3\n",
    "opt = optax.adam(lr)\n",
    "initial_opt_state = opt.init(initial_params)\n",
    "state = TrainingState(initial_params, initial_opt_state)\n",
    "opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(state: TrainingState, batch: Batch) -> TrainingState:\n",
    "    grads = jax.grad(loss)(state.params, batch)\n",
    "    updates, opt_state = opt.update(grads, state.opt_state)\n",
    "    params = optax.apply_updates(state.params, updates)\n",
    "    return TrainingState(params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.params, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = update(state, batch)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingStats(NamedTuple):\n",
    "  accuracy: list\n",
    "  losses: list\n",
    "  ns: list\n",
    "\n",
    "stats = TrainingStats([],[],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(state: TrainingState, batch: Batch) -> TrainingState:\n",
    "    l, grad = jax.value_and_grad(loss)(state.params, batch)\n",
    "    updates, opt_state = opt.update(grad, state.opt_state)\n",
    "    params = optax.apply_updates(state.params, updates)\n",
    "    return TrainingState(params, opt_state), l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.typedispatch\n",
    "@jax.jit\n",
    "def evaluate(logits: jnp.ndarray, batch:Batch) -> jnp.ndarray:\n",
    "    preds = jnp.argmax (logits, axis=-1)\n",
    "    return jnp.mean(preds == batch.label)\n",
    "\n",
    "@fc.typedispatch\n",
    "@jax.jit\n",
    "def evaluate(params:hk.Params, batch:Batch) -> jnp.ndarray:\n",
    "    logits = model.apply(params, batch.image)\n",
    "    preds = jnp.argmax (logits, axis=-1)\n",
    "    return jnp.mean(preds == batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(dls.valid))\n",
    "b = Batch(jnp.array(xb), jnp.array(yb))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(state.params, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TrainingState(initial_params, initial_opt_state)\n",
    "n_epochs = 2\n",
    "for epoch in range(n_epochs):\n",
    "    for _, batch in enumerate(dls.train): \n",
    "        xb, yb = batch\n",
    "        state, _ = update(state, Batch(jnp.array(xb), jnp.array(yb)))\n",
    "    xb, yb = next(iter(dls.valid))\n",
    "    accuracy = evaluate(state.params, Batch(jnp.array(xb), jnp.array(yb)))\n",
    "    print({\"epoch\": epoch, \"accuracy\": f\"{accuracy:.3f}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subkey = jax.random.PRNGKey(42)\n",
    "xb, yb = next(iter(dls.train))\n",
    "initial_params = model.init(subkey, xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TrainingState(initial_params, initial_opt_state)\n",
    "stats = TrainingStats([],[],[])\n",
    "class Learner:\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    def __init__(self, \n",
    "            model:hk.transform, \n",
    "            dls:DataLoaders, \n",
    "            loss_func:Callable[..., jnp.ndarray], \n",
    "            lr: float, \n",
    "            opt_func:Callable=optax.sgd, \n",
    "            state=None, \n",
    "            stats=None ): \n",
    "        fc.store_attr()        \n",
    "        if state is None:\n",
    "            self.key, subkey = jax.random.split(self.key)\n",
    "            xb, _ = next(iter(dls.train))\n",
    "            initial_params = model.init(subkey, xb)\n",
    "            initial_opt_state = opt.init(initial_params)\n",
    "            # self.state = TrainingState(initial_params, initial_opt_state)\n",
    "        if stats is None:\n",
    "            stats = TrainingStats([],[],[])\n",
    "\n",
    "# learn = Learner(model, dls, loss, 1e-2, opt, state, stats)\n",
    "learn = Learner(model, dls, loss, 1e-2, opt, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_batch(batch):\n",
    "    if state.is_training:\n",
    "        state, loss = update(state, batch)\n",
    "    with jax.default_device(jax.devices(\"cpu\")[0]): calc_stats(batch, loss)\n",
    "\n",
    "def calc_stats(batch, loss):\n",
    "    logits = model.apply(state.params, batch.image)\n",
    "    acc = evaluate(initial_params, batch)\n",
    "    stats.accuracy.append(acc)\n",
    "    n = len(batch.label)\n",
    "    stats.losses.append(loss*n)\n",
    "    stats.ns.append(n)\n",
    "\n",
    "def one_epoch():\n",
    "    dl = dls.train if state.is_training else dls.valid\n",
    "    for num, batch in enumerate(dl): \n",
    "        one_batch(map(jnp.array, batch))\n",
    "    n = sum(stats.ns)\n",
    "    print(state.epoch, state.is_training, sum(stats.losses)/n, sum(stats.accs)/n)\n",
    "\n",
    "def fit(n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        state.epoch = epoch\n",
    "        state.is_training = True\n",
    "        one_epoch()\n",
    "        state.is_training = False\n",
    "        one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TrainingState(initial_params, initial_opt_state, False, 0)\n",
    "stats = TrainingStats([],[],[])\n",
    "fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.xb,self.yb = to_device(self.batch)\n",
    "        self.preds = self.model(self.xb)\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        with torch.no_grad(): self.calc_stats()\n",
    "\n",
    "    def calc_stats(self):\n",
    "        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()\n",
    "        self.accs.append(acc)\n",
    "        n = len(self.xb)\n",
    "        self.losses.append(self.loss*n)\n",
    "        self.ns.append(n)\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.training = train\n",
    "        dl = self.dls.train if train else self.dls.valid\n",
    "        for self.num,self.batch in enumerate(dl): self.one_batch()\n",
    "        n = sum(self.ns)\n",
    "        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.accs,self.losses,self.ns = [],[],[]\n",
    "        self.model.to(def_device)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        self.n_epochs = n_epochs\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(True)\n",
    "            with torch.no_grad(): self.one_epoch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dls.train\n",
    "n_epochs = 2\n",
    "for epoch in n_epochs:\n",
    "    for num, batch in enumerate(dl):\n",
    "        xb, yb = map(jnp.array, batch)\n",
    "        loss, grads = value_and_grad(lambda p,x,y: loss_func(model(loss_func(model.apply))(params, xb, yb)\n",
    "    p   arams = jax.tree_map(UpdateWeights, params, param_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.typedispatch\n",
    "def fn(x:int): return x+1\n",
    "\n",
    "@fc.typedispatch\n",
    "def fn(x:float): return x+2\n",
    "\n",
    "fn(1), fn(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.typedispatch\n",
    "def fn(x:int): return 1, x+5\n",
    "\n",
    "g,h = fn(1)\n",
    "g,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = fn(1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model\n",
    "def forward(x:jnp.array) -> jnp.ndarray: return hk.nets.MLP(output_sizes=[50,10])(x)\n",
    "model = hk.without_apply_rng(hk.transform(forward))\n",
    "\n",
    "# Optimiser\n",
    "lr = 1e-3\n",
    "opt = optax.adam(lr)\n",
    "\n",
    "# Loss\n",
    "def loss(params:hk.Params, batch)-> jnp.ndarray:\n",
    "    xb,yb = batch\n",
    "    bs, *_ = xb.shape\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(model.apply(params, xb), yb)\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params, batch) -> jnp.ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jrnd.PRNGKey(42)\n",
    "params = model.init(key, xb)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.apply(params, None, xb)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb # integers, not one_hot_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = optax.softmax_cross_entropy_with_integer_labels\n",
    "# if yb was one_hot_encoded, could use `optax.softmax_cross_entropy``\n",
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, x,y):\n",
    "    preds = model.apply(params, None, x)\n",
    "    return loss_func(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(params, xb,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.transform\n",
    "def loss_fn(batch) ->jnp.ndarray:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.DeviceArray??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, lr\n",
    "lr = 0.02\n",
    "@jax.jit\n",
    "def update(params,x,y):\n",
    "    v, g = value_and_grad(loss)(params, x,y)\n",
    "    # return jax.tree_map((lambda w,g,r: w-g*r), params, g, lr)\n",
    "    return v,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(params, xb,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v,g = update(params, xb[0],yb[0])\n",
    "v,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, x,y):\n",
    "    preds = vmap(model.apply, in_axes=(None, None, 0))(params, None, x)\n",
    "    return loss_func(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v,g = update(params, xb,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = update(params, xb,yb)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dls.train\n",
    "n_epochs = 2\n",
    "for epoch in n_epochs:\n",
    "    for num, batch in enumerate(dl):\n",
    "        xb, yb = map(jnp.array, batch)\n",
    "        loss, grads = value_and_grad(lambda p,x,y: loss_func(model(loss_func(model.apply))(params, xb, yb)\n",
    "    p   arams = jax.tree_map(UpdateWeights, params, param_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(model, params, loss_func, x, y):\n",
    "    def loss(params, x,y): return loss_func(model(params))\n",
    "  grads = grad(loss)(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lambda a,b: a+b)(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, params, dl, n_epochs: int):\n",
    "    updated_params = params\n",
    "    for epoch in n_epochs:\n",
    "        for num, batch in enumerate(dl):\n",
    "            #one_batch\n",
    "            \n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit(model, params, dls, n_epochs: int):\n",
    "    \n",
    "#     def one_batch(batch:jnp.array, is_training:bool):\n",
    "#         # convert torch tensors to jnp.arrays\n",
    "#         xb,yb = batch\n",
    "#         preds = model.apply(params, None, xb)\n",
    "#         if is_training:\n",
    "#             loss, grads = value_and_grad(loss_func)(preds,yb)\n",
    "            \n",
    "#         else:\n",
    "#             loss = loss_func(preds, yb)\n",
    "        \n",
    "\n",
    "#         loss = loss_func(preds, yb)\n",
    "#     def one_epoch(is_training: bool):\n",
    "#         dl = dls.train if is_training else dls.valid\n",
    "#         for num, batch in enumerate(dl): one_batch(map(jnp.array, batch), is_training)\n",
    "        \n",
    "#     for epoch in range(n_epochs):\n",
    "#         one_epoch(is_training=True)\n",
    "#         one_epoch(is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self,dls): fc.store_attr()\n",
    "    def one_batch(self):\n",
    "        self.xb, self.yb = self.batch\n",
    "    def one_epoch(self, is_training):\n",
    "        dl = self.dls.train if is_training else self.dls.valid\n",
    "        for self.num, self.batch in enumerate(dl): self.one_batch()\n",
    "\n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(True)\n",
    "            self.one_epoch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b806adfb64333d0ca5c14ed2dbf613d5d551ec856d702e8a01588c05fb48e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
